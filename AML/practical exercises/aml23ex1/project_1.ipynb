{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_8312\\2212644858.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmake_moons\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.datasets import make_moons\n",
    "from torch import Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'Running on device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization functions\n",
    "def plot_density(model, loader=[], batch_size=100, mesh_size=5., device=\"cpu\"):\n",
    "    \"\"\"Plot the density of a normalizing flow model. If loader not empty, it plots also its data samples.\n",
    "\n",
    "    Args:\n",
    "        model: normalizing flow model. Flow or StackedFlows\n",
    "        loader: loader containing data to plot. DataLoader\n",
    "        bacth_size: discretization factor for the mesh. int\n",
    "        mesh_size: range for the 2D mesh. float\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        xx, yy = np.meshgrid(np.linspace(- mesh_size, mesh_size, num=batch_size), np.linspace(- mesh_size, mesh_size, num=batch_size))\n",
    "        coords = np.stack((xx, yy), axis=2)\n",
    "        coords_resh = coords.reshape([-1, 2])\n",
    "        log_prob = np.zeros((batch_size**2))\n",
    "        for i in range(0, batch_size**2, batch_size):\n",
    "            data = torch.from_numpy(coords_resh[i:i+batch_size, :]).float().to(device)\n",
    "            log_prob[i:i+batch_size] = model.log_prob(data.to(device)).cpu().detach().numpy()\n",
    "\n",
    "        plt.scatter(coords_resh[:,0], coords_resh[:,1], c=np.exp(log_prob))\n",
    "        plt.colorbar()\n",
    "        for X in loader:\n",
    "            plt.scatter(X[:,0], X[:,1], marker='x', c='orange', alpha=.05)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "def plot_samples(model, num_samples=500, mesh_size=5.):\n",
    "    \"\"\"Plot samples from a normalizing flow model. Colors are selected according to the densities at the samples.\n",
    "\n",
    "    Args:\n",
    "        model: normalizing flow model. Flow or StackedFlows\n",
    "        num_samples: number of samples to plot. int\n",
    "        mesh_size: range for the 2D mesh. float\n",
    "    \"\"\"\n",
    "    x, log_prob = model.rsample(batch_size=num_samples)\n",
    "    x = x.cpu().detach().numpy()\n",
    "    log_prob = log_prob.cpu().detach().numpy()\n",
    "    plt.scatter(x[:,0], x[:,1], c=np.exp(log_prob))\n",
    "    plt.xlim(-mesh_size, mesh_size)\n",
    "    plt.ylim(-mesh_size, mesh_size)\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 1: Normalizing Flows (10 pt)\n",
    "In this notebook we will implement two normalizing flow (NF) layers, stack tranformations, and train the model with maximum likelihood on 3 datasets.\n",
    "\n",
    "## Your task\n",
    "Complete the missing code. Make sure that all the functions follow the provided specification, i.e. the output of the function exactly matches the description in the docstring. \n",
    "\n",
    "Do not add or modify any code outside of the following comment blocks\n",
    "```\n",
    "##########################################################\n",
    "# YOUR CODE HERE\n",
    ".....\n",
    "##########################################################\n",
    "```\n",
    "After you fill in all the missing code, restart the kernel and re-run all the cells in the notebook.\n",
    "\n",
    "The following things are **NOT** allowed:\n",
    "- Using additional `import` statements\n",
    "- Using `torch.autograd.functional.jacobian`\n",
    "- Using `torch.det`\n",
    "- Using `torch.distributions`\n",
    "- Copying / reusing code from other sources (e.g. code by other students)\n",
    "\n",
    "If you plagiarize even for a single project task, you won't be eligible for the bonus this semester."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Normalizing Flow "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All transformations (i.e. normalizing flow layers) inherit from the class `nf.Flow` that is included in `nf_utils.py`. The class `nf.Flow` has the following methods:\n",
    "- `forward`: Apply the transformation and compute the Jacobian determinant (Slide 31)\n",
    "- `inverse`: Apply the inverse transformation (if it exists in closed form) and compute the Jacobian determinant of the *inverse* transformation (Slide 27)\n",
    "\n",
    "Additonally, by calling `Flow.get_inverse()` we \"reverse the direction\" of the transformation. That is, the foward transformation becomes the inverse, and vice versa..\n",
    "\n",
    "In this section, we will implement two NF transformations:\n",
    "- Affine transformation\n",
    "- Radial transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Affine tranformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An affine tranformation scales each dimension independently and shifts the inputs by a constant offset. The tranformation is defined as follows:\n",
    "\n",
    "$$f(\\mathbf{z})= \\exp(\\mathbf{a}) \\odot \\mathbf{z} + \\mathbf{b}$$\n",
    "\n",
    "where parameters $\\mathbf{a} \\in \\mathbb{R}^{D}$ and $\\mathbf{b} \\in \\mathbb{R}^{D}$. \n",
    "We apply $\\exp$ elementwise to $\\mathbf{a}$ to obtain positive scales for each dimension. \n",
    "\n",
    "Note that we can compute the inverse of this transformation analytically."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 1: Affine - forward (1 pt)\n",
    "Implement the `forward` method in the class `Affine`.\n",
    "\n",
    "\n",
    "#### Task 2: Affine - inverse (1 pt)\n",
    "Implement the `inverse` method in the class `Affine`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flows.affine import Affine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Radial transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A radial flow is a simple but expressive transformation. It has been introduced in this [paper](https://arxiv.org/pdf/1505.05770.pdf) by Rezende and Mohamed. The transformation is defined as:\n",
    "\n",
    "$$f(\\mathbf{z}) =  \\mathbf{z} + \\beta h(\\alpha, r)(\\mathbf{z} - \\mathbf{z_0})$$\n",
    "\n",
    "where $r= \\|\\mathbf{z} - \\mathbf{z_0}\\|_2$, $h(\\alpha, r) = \\frac{1}{\\alpha + r}$ and parameters $\\mathbf{z_0} \\in \\mathbb{R}^{D}$, $\\alpha \\in \\mathbb{R}_+$ and $\\beta \\in \\mathbb{R}$. The parameters need to satisfy the constraints $\\alpha > 0$ and $\\beta \\geq -\\alpha$ for the transformation to be invertible. \n",
    "\n",
    "To enfore the above constraints in practice, we can do the following.\n",
    "\n",
    "- Define a learnable parameter $\\tilde{\\alpha} \\in \\mathbb{R}$ and obtain $\\alpha$ for the transformation as $\\alpha = \\textrm{softplus}(\\tilde{\\alpha})$.\n",
    "- Define a learnable parameter $\\tilde{\\beta} \\in \\mathbb{R}$ and obtain $\\beta$ for the transformation as $\\beta =-\\alpha + \\textrm{softplus}(\\tilde{\\beta}) > -\\alpha$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 3: Radial - forward (3 pt)\n",
    "Implement the `forward` method in the class `Radial`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flows.radial import Radial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stacking tranformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To define a more expressive NF model, we need to stack multiple transformations. It's important that every stacked transformation can be computed in the `forward` direction if we want to perform sampling (slide 31). Similarly, we need to be able to compute the `inverse` direction for each transformation if we want to learn the model via MLE (slide 27)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 4: stacking tranformations - log_prob (2 pt)\n",
    "Implement the method `log_prob` in class `StackedFlows`. This method should compute tthe log density for each sample."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 5: stacking tranformations - rsample (2 pt)\n",
    "Implement the method `rsample` in class `StackedFlows`. This method should draw a sample from the base distribution, pass it through all the transformations and compute its log density."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flows.stacked_flow import StackedFlows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Maximum-likelihood training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train normalizing flows by maximizing the log-likelihood (Slide 28) of the observed data points $\\boldsymbol{x}^{(i)}$ w.r.t. the flow parameters $\\varphi$ i.e.:\n",
    "\n",
    "$$\\max_\\varphi \\frac{1}{|\\mathcal{D}|} \\sum_{\\boldsymbol{x}^{(i)} \\in \\mathcal{D}} \\log p(\\boldsymbol{x}^{(i)})$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 6: training - maximum likelihood (1 pt)\n",
    "Complete the functions likelihood such that it trains the model with maximum likelihood. \n",
    "\n",
    "The variable loss should be a scalar equal to the the mean loss for the data in the current batch. Note that here we expect to minimize the negaitve log-likelihood instead of maximizing the log-likelihood.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flows.loss import likelihood\n",
    "\n",
    "def train(model, dataset, batch_size=100, max_epochs=1000, frequency=250):\n",
    "    \"\"\"Train a normalizing flow model with maximum likelihood.\n",
    "\n",
    "    Args:\n",
    "        model: normalizing flow model. Flow or StackedFlows\n",
    "        dataset: dataset containing data to fit. Dataset\n",
    "        batch_size: number of samples per batch. int\n",
    "        max_epochs: number of training epochs. int\n",
    "        frequency: frequency for plotting density visualization. int\n",
    "        \n",
    "    Return:\n",
    "        model: trained model. Flow or StackedFlows\n",
    "        losses: loss evolution during training. list of floats\n",
    "    \"\"\"\n",
    "    # Load dataset\n",
    "    train_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size)\n",
    "    \n",
    "    # Train model\n",
    "    losses = []\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-2, weight_decay=1e-4)\n",
    "    for epoch in range(max_epochs + 1):\n",
    "        total_loss = 0\n",
    "        for batch_index, (X_train) in enumerate(train_loader):\n",
    "            loss = likelihood(X_train, model, device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        total_loss /= len(train_loader)\n",
    "        losses.append(total_loss)\n",
    "        \n",
    "        if epoch % frequency == 0:\n",
    "            print(f\"Epoch {epoch} -> loss: {total_loss:.2f}\")\n",
    "            plot_density(model, train_loader, device=device)\n",
    "    \n",
    "    return model, losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now fit our models on the following toy datasets:\n",
    "- A single Gaussian with non-zero mean\n",
    "- Three gaussians\n",
    "- Two moons\n",
    "\n",
    "For each dataset, we train an affine and a radial transformation with a Gaussian base distribution. The affine tranformation should only be able to scale and shift the base distribution. The radial tranformation is capable of more complex transformations.\n",
    "\n",
    "Plots show:\n",
    "- Evolution of density estimation during training.\n",
    "- The loss curve during training. \n",
    "- The density learned by the model after training.\n",
    "- Samples from the model after training (if possible).\n",
    "\n",
    "If everything is implement correctly, you should see significant changes in the loss value after the first 100 epochs for all datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset 1: shifted Gaussian\n",
    "The first dataset composed of one Gaussian with a non zero mean. All flows should manage to fit this density."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CircleGaussiansDataset(Dataset):\n",
    "    \"\"\"Create a 2D dataset with Gaussians on a circle.\n",
    "\n",
    "    Args:\n",
    "        n_gaussians: number of Gaussians. int\n",
    "        n_samples: number of sample per Gaussian. int\n",
    "        radius: radius of the circle where the Gaussian means lie. float\n",
    "        varaince: varaince of the gaussians. float\n",
    "        seed: random seed: int\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_gaussians: int = 6,\n",
    "        n_samples: int = 100,\n",
    "        radius: float = 3.,\n",
    "        variance: float = .3,\n",
    "        seed: int = 0\n",
    "    ):\n",
    "        self.n_gaussians = n_gaussians\n",
    "        self.n_samples = n_samples\n",
    "        self.radius = radius\n",
    "        self.variance = variance\n",
    "\n",
    "        np.random.seed(seed)\n",
    "        radial_pos = np.linspace(0, np.pi*2, num=n_gaussians, endpoint=False)\n",
    "        mean_pos = radius * np.column_stack((np.sin(radial_pos), np.cos(radial_pos)))\n",
    "        samples = []\n",
    "        for _, mean in enumerate(mean_pos):\n",
    "            sampled_points = mean[:,None] + (np.random.normal(loc=0, scale=variance, size=n_samples), np.random.normal(loc=0, scale=variance, size=n_samples ))\n",
    "            samples.append(sampled_points)\n",
    "        p = np.random.permutation(self.n_gaussians * self.n_samples)\n",
    "        self.X = np.transpose(samples, (0, 2, 1)).reshape([-1,2])[p]\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.n_gaussians * self.n_samples\n",
    "\n",
    "    def __getitem__(self, item: int) -> Tensor:\n",
    "        x = torch.from_numpy(self.X[item]).type(torch.FloatTensor)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_1 = CircleGaussiansDataset(n_gaussians=1, n_samples=500)\n",
    "plt.figure(figsize=(4, 4))\n",
    "plt.scatter(dataset_1.X[:,0], dataset_1.X[:,1], alpha=.05, marker='x', c='C1')\n",
    "plt.xlim(-5, 5)\n",
    "plt.ylim(-5, 5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Affine flow "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good final training loss is < 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "transforms = [Affine()]\n",
    "model = StackedFlows(transforms, base_dist='Normal', device=device).to(device)\n",
    "model, losses = train(model, dataset_1, max_epochs=201)\n",
    "\n",
    "# Plots\n",
    "plt.plot(losses)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()\n",
    "plot_density(model, [], device=device)\n",
    "plot_samples(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Radial flow (4 layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "transforms = [Radial().get_inverse().to(device) for _ in range(4)]\n",
    "model = StackedFlows(transforms, base_dist='Normal', device=device).to(device)\n",
    "model, losses = train(model, dataset_1, max_epochs=501)\n",
    "\n",
    "# Plots\n",
    "plt.plot(losses)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()\n",
    "plot_density(model, [], device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset 2: 3 Gaussians\n",
    "The second dataset is composed of 3 gaussians with means on a circle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_2 = CircleGaussiansDataset(n_gaussians=3, n_samples=400, variance=.4)\n",
    "plt.figure(figsize=(4, 4))\n",
    "plt.scatter(dataset_2.X[:,0], dataset_2.X[:,1], alpha=.05, marker='x', c='C1')\n",
    "plt.xlim(-5, 5)\n",
    "plt.ylim(-5, 5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Affine flow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Affine flow should fail here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "transforms = [Affine().to(device)]\n",
    "model = StackedFlows(transforms, base_dist='Normal', device=device).to(device)\n",
    "model, losses = train(model, dataset_2, max_epochs=201)\n",
    "\n",
    "# Plots\n",
    "plt.plot(losses, marker='*')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()\n",
    "plot_density(model, [], device=device)\n",
    "plot_samples(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Radial flow (16 layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using 16 layers of radial flow should lead to a good reasonable fit of the data after 500 epochs. Traning with more layers and for more epochs would improve the density estimation but would take more time. You might have to run the training multiple times to learn the three Gaussians (sometimes only two Gaussians are learned by the flow).\n",
    "\n",
    "Good loss is < 2.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "transforms = [Radial().get_inverse() for _ in range(16)]\n",
    "model = StackedFlows(transforms, base_dist='Normal', device=device).to(device)\n",
    "model, losses = train(model, dataset_2, max_epochs=501, frequency=100)\n",
    "\n",
    "# Plots\n",
    "plt.plot(losses)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()\n",
    "plot_density(model, [], device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset 3: 2 Moons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The third dataset is composed of 2 moons. Affine flow should fail again. With more layers, Radial flow should work. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MoonsDataset(Dataset):\n",
    "    \"\"\"Create a 2D dataset with spirals.\n",
    "\n",
    "    Args:\n",
    "        n_samples: number of sample per spiral. int\n",
    "        seed: random seed: int\n",
    "    \"\"\"\n",
    "    def __init__(self, n_samples: int = 1200, seed: int = 0):\n",
    "\n",
    "        self.n_samples = n_samples\n",
    "\n",
    "        np.random.seed(seed)\n",
    "        self.X, _ = make_moons(n_samples=n_samples, shuffle=True, noise=.05, random_state=None)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.n_samples\n",
    "\n",
    "    def __getitem__(self, item: int) -> Tensor:\n",
    "        x = torch.from_numpy(self.X[item]).type(torch.FloatTensor)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_3 = MoonsDataset()\n",
    "plt.scatter(dataset_3.X[:,0], dataset_3.X[:,1], alpha=.05, marker='x', c='orange')\n",
    "plt.xlim(-2.5, 3)\n",
    "plt.ylim(-2.5, 3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Affine flow "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "transforms = [Affine().to(device)]\n",
    "model = StackedFlows(transforms, base_dist='Normal', device=device).to(device)\n",
    "model, losses = train(model, dataset_3, max_epochs=500)\n",
    "\n",
    "# Plots\n",
    "plt.plot(losses, marker='*')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()\n",
    "plot_density(model, [], device=device)\n",
    "plot_samples(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Affine flow should fail here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Radial flow (16 layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using 16 layers of radial flow should lead to a good reasonable fit of the data after 500 epochs. Traning with more layers and for more epochs would improve the density estimation but would take more time.\n",
    "\n",
    "Good loss is < 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms = [Radial().get_inverse().to(device) for _ in range(16)]\n",
    "model = StackedFlows(transforms, base_dist='Normal', device=device).to(device)\n",
    "model, losses = train(model, dataset_3, max_epochs=501, frequency=100)\n",
    "\n",
    "# Plots\n",
    "plt.plot(losses, marker='*')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()\n",
    "plot_density(model, [], mesh_size=3, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diffusion_tpp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "457a493feb279d2a8f7e805e1fb95d405b20bc23f0c027dbdc5dd17843557a95"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
