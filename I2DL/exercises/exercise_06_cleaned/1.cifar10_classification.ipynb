{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cifar10 Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Until now, we have implemented several pieces of a deep learning pipeline and even trained a two-layer neural network, but all the hyperparameters were already set to some values yielding resonable results. In real-life problems, however, much of the work in a deep learning project will be geared towards finding the best hyperparameters for a certain problem. In this notebook we will explore some good practices for network debugging and hyperparameters search, as well as extending our previously binary classification neural network to a multi-class one.\n",
    "\n",
    "Let's go!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some lengthy setup.\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from exercise_code.networks.layer import (\n",
    "    Sigmoid, \n",
    "    Relu, \n",
    "    LeakyRelu, \n",
    "    Tanh,\n",
    ")\n",
    "from exercise_code.data import (\n",
    "    DataLoader,\n",
    "    ImageFolderDataset,\n",
    "    RescaleTransform,\n",
    "    NormalizeTransform,\n",
    "    FlattenTransform,\n",
    "    ComposeTransform,\n",
    ")\n",
    "from exercise_code.data.image_folder_dataset import RandomHorizontalFlip\n",
    "from exercise_code.networks import (\n",
    "    ClassificationNet,\n",
    "    BCE,\n",
    "    CrossEntropyFromLogits\n",
    ")\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Quick recap (and some new things)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Until now, in the previous exercises, we focused on building and understanding all the necessary modules for training a simple model. We followed the Pytorch implementations closely, as this is the framework we will use later and we want you to have a smoother transition to its APIs. \n",
    "\n",
    "In the figure below you can see the main components in Pytorch. Before starting the actual exercise, we begin with a quick recap of **our implementation** of these components. \n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "    <h3>Task: Check Code</h3>\n",
    "    <p>Everything is already implemented for this part, but we <b>strongly</b> encourage you to check out the respective source files in order to have a better understanding. </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn-images-1.medium.com/max/800/1*uZrS4KjAuSJQIJPgOiaJUg.png\" style=\"width: 500px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Dataset and Dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data preparation represents an important part of deep learning projects. The data comes from different sources and in different formats and is prepared differently from application to application. One part, however, is clear: because entire datasets are usually too large for us to handle at once, we train our models on smaller batches of data. \n",
    "\n",
    "The goal of the ```Dataset``` class is to encapsulate all the 'dirty' data processing: loading and cleaning the data, storing features (or names of files where features can be found) and labels, as well as providing the means for accessing individual (transformed) items of the data using the ```__getitem__()``` function and an index. You already implemented an ```ImageFolderDataset``` (in ```exercise_code/data/image_folder_dataset.py```) class in Exercise 3. We we will reuse this class here.\n",
    "\n",
    "For processing the data, you implemented several transforms in Exercise 3 (```RescaleTransform```, ```NormalizeTransform```, ```ComposeTransform```). In this exercise we are working with images, which are multidimensional arrays, but we are using simple feedforward neural network which takes a one dimensional array as an input, so it is necessary to reshape the images before feeding them into the model. \n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "    <h3>Task: Check Code</h3>\n",
    "    <p>Please check the implementation of the reshape operation in the <code>FlattenTransform</code> class, which can be found in <code>../exercise_06/exercise_code/data/image_folder_dataset.py</code>. </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_url = \"https://i2dl.vc.in.tum.de/static/data/cifar10.zip\"\n",
    "i2dl_exercises_path = os.path.dirname(os.path.abspath(os.getcwd()))\n",
    "cifar_root = os.path.join(i2dl_exercises_path, \"datasets\", \"cifar10\")\n",
    "\n",
    "# Use the Cifar10 mean and standard deviation computed in Exercise 3.\n",
    "cifar_mean = np.array([0.49191375, 0.48235852, 0.44673872])\n",
    "cifar_std  = np.array([0.24706447, 0.24346213, 0.26147554])\n",
    "\n",
    "# Define all the transforms we will apply on the images when \n",
    "# retrieving them.\n",
    "rescale_transform = RescaleTransform()\n",
    "normalize_transform = NormalizeTransform(\n",
    "    mean=cifar_mean,\n",
    "    std=cifar_std\n",
    ")\n",
    "flatten_transform = FlattenTransform()\n",
    "compose_transform = ComposeTransform([rescale_transform, \n",
    "                                      normalize_transform,\n",
    "                                      flatten_transform])\n",
    "\n",
    "# Create a train, validation and test dataset.\n",
    "datasets = {}\n",
    "for mode in ['train', 'val', 'test']:\n",
    "    crt_dataset = ImageFolderDataset(\n",
    "        mode=mode,\n",
    "        root=cifar_root, \n",
    "        download_url=download_url,\n",
    "        transform=compose_transform,\n",
    "        split={'train': 0.6, 'val': 0.2, 'test': 0.2}\n",
    "    )\n",
    "    datasets[mode] = crt_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, based on this ```Dataset``` object, we can construct a ```Dataloader``` object which samples a random mini-batch of data at once. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataloader for each split.\n",
    "dataloaders = {}\n",
    "for mode in ['train', 'val', 'test']:\n",
    "    crt_dataloader = DataLoader(\n",
    "        dataset=datasets[mode],\n",
    "        batch_size=256,\n",
    "        shuffle=True,\n",
    "        drop_last=True,\n",
    "    )\n",
    "    dataloaders[mode] = crt_dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the ```Dataloader``` has the ```__iter__()``` method, we can simply iterate through the batches it produces, like this:\n",
    "\n",
    "```python\n",
    "for batch in dataloader['train']:\n",
    "    do_something(batch)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Data Augmentation\n",
    "\n",
    "After the above preprocessing steps, our data is in a good shape and ready to be fed into our network. As explained in the chapter above, we used the transformation functions `RescaleTransform`, `NormalizeTransform` and `FlattenTransform` to achieve this shape. These are the general steps that you need to perform on the data before we can even start the training. Of course, all these steps have to be applied to all three splits of our dataset (train, val and test split). So in other words, preprocessing involves preparing the data before they are used in training and inference. \n",
    "\n",
    "Besides these basic transformations, there are many other transformation methods that you can apply to the images. For example, you can <b>flip the images horizontally</b> or <b>blur the image</b> and use these new images to enlarge your dataset. This idea is called Data Augmentation and it involves methods that alter the training images to generate a synthetic dataset that is larger than your original dataset and will hopefully improve the performance of your model. The purpose here is different than in the data preprocessing steps and there is one big difference between data augmentation and data preprocessing: The transformation methods to enlarge your dataset should only be applied to the training data. The validation and test data are not affected by these methods.\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "    <h3>Task: Check Code</h3>\n",
    "    <p>The choice of transformation methods to use for data augmentation can be seen as a hyperparameter of your model and you can try to include these to enlarge your training data and obtain better results for your model. In <code>exercise_code/data/image_folder_dataset.py</code> we implemented the function <code>RandomHorizontalFlip</code> for you, which is randomly flipping an image. Check out the implementation.</p>\n",
    "    <p> Later, we will apply some hyperparameter tuning and in order to improve your model's accuracy, you could try to include some data augmentation methods. Fell free to play around and maybe also implement some other methods as for example Gaussian Blur or Rotation. </p>       \n",
    "</div>\n",
    "\n",
    "Let us quickly check out the `RandomHorizontalFlip` method with an image of the Cifar10 dataset in the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 144x144 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Load the data in a dataset without any transformation \n",
    "dataset = ImageFolderDataset(\n",
    "        mode=mode,\n",
    "        root=cifar_root, \n",
    "        download_url=download_url,\n",
    "        split={'train': 0.6, 'val': 0.2, 'test': 0.2},\n",
    "    )\n",
    "\n",
    "#Retrieve an image from the dataset and flip it\n",
    "image = dataset[1]['image']\n",
    "transform = RandomHorizontalFlip(1)\n",
    "image_flipped = transform(image)\n",
    "\n",
    "#Show the two images\n",
    "plt.figure(figsize = (2,2))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(image.astype('uint8'))\n",
    "plt.axis('off')\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(image_flipped.astype('uint8'))\n",
    "plt.axis('off')\n",
    "plt.title(\"Left: Original Image, Right: Flipped image\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Layers\n",
    "\n",
    "Now, that the data is prepared, we can discuss the model in which we are feeding the data. In our case the model will be a neural network. \n",
    "\n",
    "In Exercise 5, you implemented a simple 2-layer neural network that had a hidden size as a parameter:\n",
    "\n",
    "$$ \n",
    "{\\hat{y}} = \\sigma(\\sigma({x W_1} + {b_1}) {W_2} + {b_2}) \n",
    "$$\n",
    "\n",
    "where $ \\sigma({x}) $ was the sigmoid function, $ {x} $ was the input, $ {W_1}, {W_2} $ the weight matrices and $ {b_1}, {b_2}$ the biases for the two layers.\n",
    "\n",
    "This is how we used this network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = datasets['train'][0]['image'].shape[0]\n",
    "model = ClassificationNet(input_size=input_size, \n",
    "                          hidden_size=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we updated the ```ClassificationNet``` from the previous exercise, so that now you can customize more: the number of outputs, the choice of activation function, the hidden size etc. We encourage you to check out the implementation in ```exercise_code/networks/classification_net.py``` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_layer = 2\n",
    "reg = 0.1\n",
    "\n",
    "model = ClassificationNet(activation=Sigmoid(), \n",
    "                          num_layer=num_layer, \n",
    "                          reg=reg,\n",
    "                          num_classes=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, the forward and backward passes through the model were simply:\n",
    "\n",
    "```python\n",
    "\n",
    "# X is a batch of training features \n",
    "# X.shape = (batch_size, features_size)\n",
    "y_out = model.forward(X)\n",
    "\n",
    "# dout is the gradient of the loss function w.r.t the output of the network.\n",
    "# dout.shape = (batch_size, )\n",
    "model.backward(dout)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just as the learning rate or the number of iterations we want to train for, the number of hidden layers and the number of units in each hidden layer are also hyperparameters. In this notebook you will play with networks of different sizes and will see the impact that the network capacity has.\n",
    "\n",
    "Before we move on to the loss functions, we want to have a look at the activation functions. The choice of an activation function can have a huge impact on the performance of the network that you are designing. So far, you have implemented the `Sigmoid` and the `Relu` activation function in Exercise 5. \n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "    <h3>Task: Check Code</h3>\n",
    "    <p>Take a look at the <code>Sigmoid</code> and the <code>Relu</code> class in <code>exercise_code/networks/layer.py</code> and the implementation of the respective forward and backward pass. Make sure to understand why we use <b>element-wise product</b> instead of dot product in the backward pass of the <code>Sigmoid</code> class to compute the gradient $dx$. That will be helpful for your later implementation of other activation functions.</p>\n",
    "    <p> <b>Note:</b> The <code>cache</code> variable is used to store information from the forward pass and then pass this information in the backward pass to make use of it there. The implementation of both classes show that this variable can be used differently - depending on what information is needed in the backward pass. </p>\n",
    "</div>\n",
    "\n",
    "Now, we want to have a look at two other, very common activation functions that you have already met in the lecture: Leaky ReLU activation function and Tanh activation function. \n",
    "\n",
    "**Leaky Relus** are one attempt to fix the “dying ReLU” problem. Instead of the function being zero when $x<0$, a leaky ReLU has a small negative slope (for example, 0.01). That is, the function computes $f(x) = \\mathbb{1}(x < 0) (\\alpha x) + \\mathbb{1}(x>=0) (x)$ where $\\alpha$ is a small constant. Some people report success with this form of activation function, but the results are not always consistent.\n",
    "\n",
    "The **tanh non-linearity** squashes a real-valued number to the range [-1, 1]. Like the sigmoid neuron, its activations saturate, but unlike the sigmoid neuron its output is zero-centered. Therefore, in practice the tanh non-linearity is always preferred to the sigmoid non-linearity. Also note that the tanh neuron is simply a scaled sigmoid neuron, in particular the following holds: $\\tanh(x) = 2 \\cdot \\sigma(2x) -1$.\n",
    "\n",
    "<img class=left src=https://pytorch.org/docs/stable/_images/LeakyReLU.png alt=\"Figure3\" width=\"350\" align='left'/> \n",
    "<img class=right src=https://pytorch.org/docs/stable/_images/Tanh.png alt=\"Figure4\" width=\"350\"/>\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "    <h3>Task: Implement Activation Layers</h3>\n",
    "    <p> Now, it is your turn to implement the <code>LeakyRelu</code> and the <code>Tanh</code> class in <code>exercise_code/networks/layer.py</code> by completing the <code>forward</code> and the <code>backward</code> functions. You can test your implementation in the following two cells. </p>\n",
    "    <p> <b>Note:</b> Always remember to return a cache in <code>forward</code> for later backpropagation in <code>backward</code>. As we have seen above, the <code>cache</code> variable can be used differently for two activation functions.</p>\n",
    "</div>\n",
    "\n",
    "Use this cell to test your implementation of the `LeakyRelu` class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LeakyReluForwardTest passed.\n",
      "LeakyReluBackwardTest passed.\n",
      "Congratulations you have passed all the unit tests!!! Tests passed: 2/2\n",
      "Score: 100/100\n",
      "You secured a score of :100\n"
     ]
    }
   ],
   "source": [
    "from exercise_code.tests.layer_tests import *\n",
    "print(LeakyReluTestWrapper()())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And this cell to test your implementation of the `Tanh` class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TanhForwardTest passed.\n",
      "TanhBackwardTest passed.\n",
      "Congratulations you have passed all the unit tests!!! Tests passed: 2/2\n",
      "Score: 100/100\n",
      "You secured a score of :100\n"
     ]
    }
   ],
   "source": [
    "print(TanhTestWrapper()())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations, you now implemented all four different activation functions! These activation layers are now ready to be used when you start building your own network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Loss\n",
    "\n",
    "In order to measure how well a network is performing, we implemented several ```Loss``` classes (```L1```, ```MSE```, ```BCE```, each preferred for a certain type of problems) in ```exercise_code/networks/loss.py```.\n",
    "\n",
    "Each implemented a ```forward()``` method, which outputs a number that we use as a proxy for our network performance. \n",
    "\n",
    "Also, because our goal was to change the weights of the network such that this loss measure decreases, we were also interested in the gradients of the loss w.r.t the outputs of the network, $ \\nabla_{\\hat{y}} L({\\hat{y}}, {y}) $. This was implemented in ```backward()```. \n",
    "\n",
    "In previous exercises, we only worked with binary classification and used binary cross entropy (```BCE```) as a loss function.\n",
    "\n",
    "$$ BCE(\\hat{y}, y) = \\frac{1}{N} \\sum_{i=1}^N \\Big [-y_i \\log(\\hat{y_i}) - (1-y_i) \\log(1 - \\hat{y_i}) \\Big] $$ \n",
    "\n",
    "where\n",
    "- $ N $ was the number of samples we were considering\n",
    "- $\\hat{y}_i$ was the network's prediction for sample $i$. Note that this was a valid probability $\\in [0, 1]$, because we applied a [sigmoid](https://en.wikipedia.org/wiki/Sigmoid_function) activation on the last layer. \n",
    "- $ y_i $ was the ground truth label (0 or 1, depending on the class)\n",
    "\n",
    "Because we have 10 classes in the CIFAR10 dataset, we need a generalization of the binary cross entropy for multiple classes. This is simply called the cross entropy loss and has the following definition:\n",
    "\n",
    "$$ CE(\\hat{y}, y) = \\frac{1}{N} \\sum_{i=1}^N \\sum_{k=1}^{C} \\Big[ -y_{ik} \\log(\\hat{y}_{ik}) \\Big] $$\n",
    "\n",
    "where:\n",
    "- $ N $ is again the number of samples\n",
    "- $ C $ is the number of classes\n",
    "- $ \\hat{y}_{ik} $ is the probability that the model assigns for the $k$th class when the $i$th sample is the input. **Because we don't apply any activation function on the last layer of our network, its outputs for each sample will not be a valid probability distribution over the classes. We call these raw outputs of the network '[logits](https://datascience.stackexchange.com/questions/31041/what-does-logits-in-machine-learning-mean/31045)' and we will apply a [softmax](https://en.wikipedia.org/wiki/Softmax_function) activation in order to obtain a valid probability distribution.** \n",
    "- $y_{ik} = 1 $ iff the true label of the $i$th sample is $k$ and 0 otherwise. This is called a [one-hot encoding](https://machinelearningmastery.com/why-one-hot-encode-data-in-machine-learning/).\n",
    "\n",
    "You can check for yourself that when the number of classes $ C $ is 2, then the binary cross entropy is actually equivalent to the cross entropy.\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "    <h3>Task: Check Code</h3>\n",
    "    <p>Please check the implementation of the <code>CrossEntropyFromLogits</code> class, which can be found in <code>../exercise_06/exercise_code/networks/loss.py</code>. </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = CrossEntropyFromLogits()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just as with the previous losses we have seen, we can simply get the results of the forward and backward passes as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# y_out is the output of the neural network\n",
    "# y_truth is the actual label from the dataset\n",
    "loss.forward(y_out, y_truth)\n",
    "loss.backward(y_out, y_truth)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Optimizer\n",
    "\n",
    "Now, knowing the gradient of the loss w.r.t the ouputs of the network, as well as the local gradient for each layer of the network, we can use the chain rule to compute all gradients. \n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "    <h3>Task: Check Code</h3>\n",
    "    <p>We implemented several optimizer classes <code>SGD</code>, <code>Adam</code>, <code>sgd_momentum</code> class that implement different first-order parameter update rules, which can be found in <code>../exercise_06/exercise_code/networks/optimizer.py</code>. </p>\n",
    "    <p>The <code>step()</code> method iterates through all the parameters of a model and updates them using the gradient information.</p>\n",
    "</div>\n",
    "\n",
    "What the optimizer is doing, in pseudocode, is the following:\n",
    "\n",
    "```python\n",
    "for param in model:\n",
    "    # Use the gradient to update the weights.\n",
    "    update(param)\n",
    "    \n",
    "    # Reset the gradient after each update.\n",
    "    param.gradient = 0\n",
    "```\n",
    "\n",
    "```SGD``` had the simplest update rule:\n",
    "```python\n",
    "def update(param):\n",
    "    param = param - learning_rate * param.gradient\n",
    "```\n",
    "\n",
    "For the more complicated update rules, see ```exercise_code/networks/optimizer.py```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6 Solver\n",
    "\n",
    "The ```Solver``` is where all the above elements come together: Given a train and a validation dataloader, a model, a loss and an optimizer, it uses the training data to optimize a model in order to get better predictions. We simply call ```train()``` and it does its 'magic' for us!\n",
    "```python\n",
    "solver = Solver(model, \n",
    "                dataloaders['train'], \n",
    "                dataloaders['val'], \n",
    "                learning_rate=0.001, \n",
    "                loss_func=MSE(), \n",
    "                optimizer=SGD)\n",
    "\n",
    "solver.train(epochs=epochs)\n",
    "```\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "    <h3>Task: Check Code</h3>\n",
    "    <p>Please check out the implementation of <code>train()</code> in <code>../exercise_06/exercise_code/solver.py</code>. </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.7 Weight Regularization\n",
    "\n",
    "Before we finish this section of recap, we want to take a look at some regularization method that has been introduced in the lecture and that is super helpful to improve robustness of our model. Here, we talk about weight regularization.\n",
    "\n",
    "Weight regularization has been introduced to you as a method preventing our model from overfitting. Essentially, it is a term (solely depending on the weights of our model) that is added to the final loss and that encodes some preference for a certain set of weights $W$ over others. In the lecture, we compared two weight regularization methods and their respective preference for weight vectors. We made the following observation: \n",
    "\n",
    "1. L1 regularization: Enforces sparsity \n",
    "2. L2 regularization: Enforces that weights have similar values\n",
    "\n",
    "The most common weight regularization method is the L2 regularization. From the observations made in the lecture that makes totally sense - at least when we compare it to the L1 regularization. The L2 regularization penalty in the loss prefers smaller and more diffuse weight vectors and hence the model is encouraged to take into account all input dimensions to small amounts rather than a few input dimensions and very strongly.\n",
    "\n",
    "When using weight regularization, the loss function is a composition of two parts:\n",
    "$$L = \\underbrace{\\frac{1}{N} \\sum_{i} L_i}_{\\text{data loss}}  + \\underbrace{\\lambda R(W)}_{\\text{regularization loss}}$$\n",
    "The first one being the data loss, which is calculated with the Cross Entropy loss in our model. The second part is called the regularization loss $R(W)$ and is computed in the L2 case as follows:\n",
    "$$R(W) = \\sum_{k} \\sum_{l} w_{k,l}^2$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. An overview of hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n",
    "<img src=https://images.deepai.org/glossary-terms/05c646fe1676490aa0b8cab0732a02b2/hyperparams.png alt=hyperparameter width=700>\n",
    "\n",
    "A **hyperparameter** is a parameter that is set before the learning process begins. Recall that the parameters of weight matrix and bias vector are learned during the learning process.\n",
    "\n",
    "The hyperparameters are essential, for they control and affect the whole training and have a great impact on the performance of the model. \n",
    "\n",
    "Some examples of hyperparameters we have covered in lectures:\n",
    "* Network architecture\n",
    "    * Choice of activation function\n",
    "    * Number of layers\n",
    "    * ...\n",
    "* Learning rate\n",
    "* Number of epochs\n",
    "* Batch size\n",
    "* Regularization strength\n",
    "* Momentum\n",
    "* ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Start debugging your own network!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As already suggested in the lectures, you may always want to start from small and simple architectures, to make sure you are going the right way. \n",
    "\n",
    "First you may need to overfit a single training sample, then a few batches of training samples, then go deeper with larger neural networks and the whole training data.\n",
    "\n",
    "Here we always provide a default neural network (i.e. ClassificationNet) with arbitrary number of layers, which is a generalization from a fixed 2-layer neural network in exercise 5. You are welcome to implement your own network, in that case just implement **MyOwnNetwork** in ```exercise_code/networks/classification_net.py```. You can also copy things from ClassficationNet and make a little adjustment to your own network. For either way, just pick a network and comment out the other one, then run the cells below for debugging.\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "    <h3>Note:</h3>\n",
    "    <p>Please, make sure you don't modify the ClassificationNet itself so that you can always have a working network to fall back on</p>\n",
    "    <p>In order to pass this submissions, you can <b>first stick to the default ClassificationNet implementation without changing any code at all</b>. The goal of this submission is to find reasonable hyperparameters and the parameter options of the ClassificationNet are broad enough.</p>\n",
    "    <p>Once you have surpassed the submission goal, you can try to implement additional activation functions in the accompanying notebook, try different weight initializations or other adjustments by writing your own network architecture in the MyOwnNetwork class.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's begin with a 2-layer neural network, and overfit one single training sample.\n",
    "\n",
    "After training, let's evaluate the training process by plotting the loss curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from exercise_code.solver import Solver\n",
    "from exercise_code.networks.optimizer import SGD, Adam\n",
    "from exercise_code.networks import MyOwnNetwork\n",
    "\n",
    "num_layer = 2\n",
    "epochs = 20\n",
    "reg = 0.1\n",
    "batch_size = 4\n",
    "\n",
    "model = ClassificationNet(num_layer=num_layer, reg=reg)\n",
    "# model = MyOwnNetwork()\n",
    "\n",
    "loss = CrossEntropyFromLogits()\n",
    "\n",
    "# Make a new data loader with a single training image\n",
    "overfit_dataset = ImageFolderDataset(\n",
    "    mode='train',\n",
    "    root=cifar_root, \n",
    "    download_url=download_url,\n",
    "    transform=compose_transform,\n",
    "    limit_files=1\n",
    ")\n",
    "dataloaders['train_overfit_single_image'] = DataLoader(\n",
    "    dataset=overfit_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    drop_last=False,\n",
    ")\n",
    "\n",
    "# Decrease validation data for only debugging\n",
    "debugging_validation_dataset = ImageFolderDataset(\n",
    "    mode='val',\n",
    "    root=cifar_root, \n",
    "    download_url=download_url,\n",
    "    transform=compose_transform,\n",
    "    limit_files=100\n",
    ")\n",
    "dataloaders['val_500files'] = DataLoader(\n",
    "    dataset=debugging_validation_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    ")\n",
    "\n",
    "# solver = Solver(model, dataloaders['train_overfit_single_image'], dataloaders['val_500files'], \n",
    "#                 learning_rate=1e-3, loss_func=loss, optimizer=Adam)\n",
    "\n",
    "# solver.train(epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.title('Loss curves')\n",
    "# plt.plot(solver.train_loss_history, '-', label='train')\n",
    "# plt.plot(solver.val_loss_history, '-', label='val')\n",
    "# plt.legend(loc='lower right')\n",
    "# plt.xlabel('Iteration')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Training accuray: %.5f\" % (solver.get_dataset_accuracy(dataloaders['train_overfit_single_image'])))\n",
    "# print(\"Validation accuray: %.5f\" % (solver.get_dataset_accuracy(dataloaders['val_500files'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time let's try to overfit to a small set of training batch samples. Please observe the difference from above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from exercise_code.networks import MyOwnNetwork\n",
    "\n",
    "num_layer = 2\n",
    "epochs = 100\n",
    "reg = 0.1\n",
    "num_samples = 10\n",
    "\n",
    "model = ClassificationNet(num_layer=num_layer, reg=reg)\n",
    "# model = MyOwnNetwork()\n",
    "\n",
    "loss = CrossEntropyFromLogits()\n",
    "\n",
    "# Make a new data loader with a our num_samples training image\n",
    "overfit_dataset = ImageFolderDataset(\n",
    "    mode='train',\n",
    "    root=cifar_root, \n",
    "    download_url=download_url,\n",
    "    transform=compose_transform,\n",
    "    limit_files=num_samples\n",
    ")\n",
    "dataloaders['train_overfit_10samples'] = DataLoader(\n",
    "    dataset=overfit_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    drop_last=False,\n",
    ")\n",
    "\n",
    "# solver = Solver(model, dataloaders['train_overfit_10samples'], dataloaders['val_500files'], \n",
    "#                 learning_rate=1e-3, loss_func=loss, optimizer=Adam)\n",
    "\n",
    "# solver.train(epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.title('Loss curves')\n",
    "# plt.plot(solver.train_loss_history, '-', label='train')\n",
    "# plt.plot(solver.val_loss_history, '-', label='val')\n",
    "# plt.legend(loc='lower right')\n",
    "# plt.xlabel('Iteration')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Training accuray: %.5f\" % (solver.get_dataset_accuracy(dataloaders['train_overfit_10samples'])))\n",
    "# print(\"Validation accuray: %.5f\" % (solver.get_dataset_accuracy(dataloaders['val_500files'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you're overfitting the training data, that means the network's implementation is correct. However, as you have more samples to overfit, your accuracy will be way lower. You can increase the number of epochs above to achieve better results.\n",
    "\n",
    "Now let's try to feed all the training and validation data into the network, but this time we set the same hyperparameters for 2-layer and 5-layer networks, and compare the different behaviors.\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "    <h3>Note:</h3>\n",
    "    <p>This may take about 1 min for each epoch as the training set is quite large. For convenience, we only train on 1000 images for now but use the full validation set.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from exercise_code.networks import MyOwnNetwork\n",
    "\n",
    "num_layer = 2\n",
    "epochs = 5\n",
    "reg = 0.01\n",
    "\n",
    "# Make a new data loader with 1000 training samples\n",
    "num_samples = 1000\n",
    "overfit_dataset = ImageFolderDataset(\n",
    "    mode='train',\n",
    "    root=cifar_root, \n",
    "    download_url=download_url,\n",
    "    transform=compose_transform,\n",
    "    limit_files=num_samples\n",
    ")\n",
    "dataloaders['train_small'] = DataLoader(\n",
    "    dataset=overfit_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    drop_last=False,\n",
    ")\n",
    "\n",
    "\n",
    "# Change here if you want to use the full training set\n",
    "use_full_training_set = False\n",
    "if not use_full_training_set:\n",
    "    train_loader = dataloaders['train_small']\n",
    "else:\n",
    "    train_loader = dataloaders['train']\n",
    "    \n",
    "\n",
    "# model = ClassificationNet(num_layer=num_layer, reg=reg)\n",
    "# # model = MyOwnNetwork()\n",
    "\n",
    "# loss = CrossEntropyFromLogits()\n",
    "\n",
    "# solver = Solver(model, train_loader, dataloaders['val'], \n",
    "#                 learning_rate=1e-3, loss_func=loss, optimizer=Adam)\n",
    "\n",
    "# solver.train(epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.title('Loss curves')\n",
    "# plt.plot(solver.train_loss_history, '-', label='train')\n",
    "# plt.plot(solver.val_loss_history, '-', label='val')\n",
    "# plt.legend(loc='lower right')\n",
    "# plt.xlabel('Iteration')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Training accuray: %.5f\" % (solver.get_dataset_accuracy(train_loader)))\n",
    "# print(\"Validation accuray: %.5f\" % (solver.get_dataset_accuracy(dataloaders['val'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from exercise_code.networks import MyOwnNetwork\n",
    "\n",
    "num_layer = 5\n",
    "epochs = 5\n",
    "reg = 0.01\n",
    "\n",
    "model = ClassificationNet(num_layer=num_layer, reg=reg)\n",
    "# model = MyOwnNetwork()\n",
    "\n",
    "# Change here if you want to use the full training set\n",
    "use_full_training_set = False\n",
    "if not use_full_training_set:\n",
    "    train_loader = dataloaders['train_small']\n",
    "else:\n",
    "    train_loader = dataloaders['train']\n",
    "\n",
    "# loss = CrossEntropyFromLogits()\n",
    "\n",
    "# solver = Solver(model, train_loader, dataloaders['val'], \n",
    "#                 learning_rate=1e-3, loss_func=loss, optimizer=Adam)\n",
    "\n",
    "# solver.train(epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.title('Loss curves')\n",
    "# plt.plot(solver.train_loss_history, '-', label='train')\n",
    "# plt.plot(solver.val_loss_history, '-', label='val')\n",
    "# plt.legend(loc='lower right')\n",
    "# plt.xlabel('Iteration')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Training accuray: %.5f\" % (solver.get_dataset_accuracy(train_loader)))\n",
    "# print(\"Validation accuray: %.5f\" % (solver.get_dataset_accuracy(dataloaders['val'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see from above, the same hyperparameter set can decrease the loss for a 2-layer network, but for 5-layer network, it hardly works.\n",
    "\n",
    "The steps above are already mentioned in the lectures as debugging steps before training a neural network. \n",
    "\n",
    "If you implement your own network, make sure you do the steps above before tuning the hyperparameters as below. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Difficulty in tuning hyperparameters\n",
    "As can be seen through the results of training a larger network, training with whole data doesn't fit the training data as well as training with small number of training data. Besides, the architecture of neural network makes a difference, too. Small decisions on hyperparameters count. \n",
    "\n",
    "Usually, but not always, hyperparameters cannot be learned using well known gradient based methods (such as gradient descent), which are commonly employed to learn parameters. Besides, some hyperparameters can affect the structure of the model and the loss function.\n",
    "\n",
    "As mentioned before, hyperparameters need to be set before training. Tuning hyperparameters is hard, because you always have to try different combinations of the hyperparameters, train the network, do the validation and pick the best one. Besides, it is not guaranteed that you'll find the best combination.\n",
    "\n",
    "Next you will do hands on learning with hyperparameter tuning methods that are covered in lectures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Hyperparameter Tuning\n",
    "\n",
    "![alt text](https://blog.floydhub.com/content/images/2018/08/Screen-Shot-2018-08-22-at-17.59.25.png \"\")\n",
    "\n",
    "One of the main challenges in deep learning is finding the set of hyperparameters that performs best.\n",
    "\n",
    "So far, we have followed a manual approach by guessing hyperparameters, running the model, observing the result and maybe tweaking the hyperparameters based on this result. As you have probably noticed, this manual hyperparameter tuning is unstructured, inefficient and can become very tedious.\n",
    "\n",
    "\n",
    "A more systematic (and actually very simple) approach for hyperparameter tuning that you've already learned in the lecture  is implementing a **Grid Search**. \n",
    "\n",
    "\n",
    "\n",
    "## 3.1 Grid Search\n",
    "Grid search is a simple and naive, yet effective method to automate the hyperparameter tuning:\n",
    "\n",
    "* First, you define the set of parameters you want to tune, e.g. $\\{learning\\_rate, regularization\\_strength\\}$.\n",
    "\n",
    "* For each hyperparameter, you then define a set of possible values, e.g. $learning\\_rate = \\{0.0001, 0.001, 0.01, 0.1\\}$.\n",
    "\n",
    "* Then, you train a model for every possible combination of these hyperparameter values and afterwards select the combination that works best (e.g. in terms of accuracy on your validation set).\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "    <h3>Task: Check Code</h3>\n",
    "    <p>Check out our <code>grid_search</code> implementation in <code>../exercise_6/exercise_code/hyperparameter_tuning.py</code>. We show a simple for loop implementation and a more sophisticated one for multiple inputs. </p>\n",
    "</div>\n",
    " \n",
    " <div class=\"alert alert-warning\">\n",
    "    <h3>Note:</h3>\n",
    "    <p>To keep things simple for the beginning, it'll be enough to just focus on the hyperparameters <code>learning_rate</code> and <code>regularization_strength</code> here, as in the example above.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from exercise_code.networks import MyOwnNetwork\n",
    "\n",
    "# # Specify the used network\n",
    "# model_class = ClassificationNet\n",
    "\n",
    "# from exercise_code import hyperparameter_tuning\n",
    "# best_model, results = hyperparameter_tuning.grid_search(\n",
    "#     dataloaders['train_small'], dataloaders['val_500files'],\n",
    "#     grid_search_spaces = {\n",
    "#         \"learning_rate\": [1e-2, 1e-3, 1e-4], \n",
    "#         \"reg\": [1e-4]\n",
    "#     },\n",
    "#     model_class=model_class,\n",
    "#     epochs=1, patience=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the results of your grid search, you might already have found some hyperparameter combinations that work better than others. A common practice is to now repeat the grid search on a more narrow domain centered around the parameters that worked best. \n",
    "\n",
    "**Conclusion Grid Search**\n",
    "\n",
    "With grid search we now have automated the hyperparameter tuning to a certain degree. Another advantage is, that since the training of all models are independent of each other, you can parallelize the grid search, i.e.,  try out different hyperparameter configurations in parallel on different machines.\n",
    "\n",
    "However, as you have probably noticed, there is one big problem with this approach: the number of possible combinations to try out grows exponentially with the number of hyperparameters (\"curse of dimensionality\"). As we add more hyperparameters to the grid search, the search space will explode in time complexity, making this strategy unfeasible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Especially when your search space contains more than 3 or 4 dimensions, it is often better to use another, similar hyperparameter tuning method that you've already learned about: random search."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Random Search\n",
    "Random search is very similar to grid search, with the only difference, that instead of providing specific values for every hyperparameter, you only define a range for each hyperparameter - then, the values are sampled randomly from the provided ranges.\n",
    "\n",
    "![alt text](https://i.stack.imgur.com/cIDuR.png \"\")\n",
    "\n",
    "The figure above illustrates the difference in the hyperparameter space exploration between grid search and random search: assume you have 2 hyperparameters with each 3 values. Running a grid search results in training $3^2=9$ different models - but in the end, you've just tired out 3 values for each parameter. For random search on the other hand, after training 9 models you'll have tried out 9 different values for each hyperparameter, which often leads much faster to good results.\n",
    "\n",
    "To get a deeper understanding of random search and why it is more efficient than grid search, you should definitely check out this paper: http://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "    <h3>Task: Check Code</h3>\n",
    "    <p>Check out our <code>random_search</code> implementation in <code>../exercise_6/exercise_code/hyperparameter_tuning.py</code></p>\n",
    "</div>\n",
    "\n",
    "\n",
    "*Hint: regarding the sample space of each parameter, think about the scale for which it makes most sense to sample in. For example the learning rate is usually sampled on a logarithmic scale!*\n",
    "\n",
    "*For simplicity and speed, just use the `train_batches`-dataloader!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from exercise_code.hyperparameter_tuning import random_search\n",
    "# from exercise_code.networks import MyOwnNetwork\n",
    "\n",
    "# # Specify the used network\n",
    "# model_class = ClassificationNet\n",
    "\n",
    "# best_model, results = random_search(\n",
    "#     dataloaders['train_small'], dataloaders['val_500files'],\n",
    "#     random_search_spaces = {\n",
    "#         \"learning_rate\": ([1e-2, 1e-6], 'log'),\n",
    "#         \"reg\": ([1e-3, 1e-7], \"log\"),\n",
    "#         \"loss_func\": ([CrossEntropyFromLogits()], \"item\")\n",
    "#     },\n",
    "#     model_class=model_class,\n",
    "#     num_search = 1, epochs=20, patience=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's time to run it with the whole dataset, and let it search for a few hours for a nice configuration. \n",
    "\n",
    "However, to save some time, let's first implement an **early-stopping** mechanism, that you also already know from the lecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Early Stopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By now you've already seen a lot of training curves:\n",
    "\n",
    "<img src=http://fouryears.eu/wp-content/uploads/2017/12/early_stopping.png></img>\n",
    "\n",
    "Usually, at some point the validation loss goes up again, which is a sign that we're overfitting to our training data. Since it actually doesn't make any sense to train further at this point, it's common practice to apply \"early stopping\", i.e., cancel the training process when the validation loss doesn't improve anymore. The nice thing about this concept is, that not only it improves generalization through the prevention of overfitting, but also it saves us a lot of time - one of our most valuable resources in deep learning.\n",
    "\n",
    "Since there are natural fluctuations in the validation loss, you usually don't cancel the training process right at the first epoch when the validation-loss increases, but instead, you wait for some epochs (specified by the `patience`-parameter) and if the loss still doesn't improve, we stop.\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "    <h3>Task: Check Code</h3>\n",
    "    <p>Please check the implementation of the early stopping mechanism in <code>../exercise_6/exercise_code/solver.py</code>.\n",
    " </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Let's find the perfect model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You've now set everything up to start training your model and finding a nice set of hyper parameters using a combination of grid or random search!\n",
    "\n",
    "Since we'll now be training with a much larger number of samples, you should be aware that this process will definitely take some time! So be prepared to let your machine run for a while. \n",
    "\n",
    "At the beginning, it's a good approach to first do a coarse random search across a wide range of values to find promising sub-ranges of your parameter space. Afterwards, you can zoom in to these ranges and do another random search (or grid search) to finetune the configuration.\n",
    "\n",
    "You don't have to use the whole dataset at the beginning, instead you can also use a medium large subset of the samples. Also, you don't need to train for a large number of epochs - as mentioned above: we first want to get an overview about our hyper parameters.\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "    <h3>Task: Hyperparameters Tunning & Model Training </h3>\n",
    "        <p> Now, it is your turn to do the hyperparamater tuning. In the cell below, you can use the <code>random_search</code> function to find a good choice of parameters. Put in some reasonable ranges for the hyperparameters and evaluate them.\n",
    "    <p> <b>Note:</b> At the beginning, it's a good approach to first do a coarse random search across a <b> wide range of values</b> to find promising sub-ranges of your parameter space and use <b> a medium large subset of the dataset </b>instead the whole as well. Afterwards, you can zoom in to these ranges and do another random search (or grid search) to finetune the configuration. Use the cell below to play around and find good hyperparameters for your model!</p>\n",
    "        <p> Finally, once you've found some promising hyperparameters (or narrowed them down to promising subranges), it's time to utilize these hyperparameters to train your network on the whole dataset for a large number of epochs so that your own model can reach an acceptable performance. \n",
    "        <p> <b>Hint:</b> You may use a <code>Solver</code> class we provided before or directly use the <code>random_search</code> function (as you can also monitor the loss here) for model training.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating Config #1 [of 30]:\n",
      " {'num_layer': 3, 'hidden_size': 152, 'learning_rate': 8.942964570785848e-05, 'reg': 0.001833305257294005, 'activation': <exercise_code.networks.layer.Sigmoid object at 0x00000163E22FF188>, 'loss_func': <exercise_code.networks.loss.CrossEntropyFromLogits object at 0x00000163E22FF488>}\n",
      "(Epoch 1 / 20) train loss: 2.303679; val loss: 2.302924\n",
      "(Epoch 2 / 20) train loss: 2.305200; val loss: 2.303726\n",
      "(Epoch 3 / 20) train loss: 2.302952; val loss: 2.304813\n",
      "(Epoch 4 / 20) train loss: 2.303836; val loss: 2.303534\n",
      "(Epoch 5 / 20) train loss: 2.302751; val loss: 2.305861\n",
      "(Epoch 6 / 20) train loss: 2.302356; val loss: 2.303725\n",
      "Stopping early at epoch 5!\n",
      "\n",
      "Evaluating Config #2 [of 30]:\n",
      " {'num_layer': 2, 'hidden_size': 171, 'learning_rate': 0.016845145136959946, 'reg': 0.0034402415427515895, 'activation': <exercise_code.networks.layer.Tanh object at 0x00000163E22FF648>, 'loss_func': <exercise_code.networks.loss.CrossEntropyFromLogits object at 0x00000163E22FF488>}\n",
      "(Epoch 1 / 20) train loss: 2.304433; val loss: 2.304572\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\yimin\\OneDrive\\Study\\Master\\I2DL\\exercises\\exercise_06_cleaned\\exercise_code\\networks\\layer.py:176: RuntimeWarning: overflow encountered in exp\n",
      "  cache = (1/(1 + np.exp(-2*x))) # sigmoid(2x)\n",
      "c:\\Users\\yimin\\OneDrive\\Study\\Master\\I2DL\\exercises\\exercise_06_cleaned\\exercise_code\\networks\\layer.py:177: RuntimeWarning: overflow encountered in exp\n",
      "  ########################################################################\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 2 / 20) train loss: 11.907530; val loss: 14.328310\n",
      "(Epoch 3 / 20) train loss: 16.307635; val loss: 15.529187\n",
      "(Epoch 4 / 20) train loss: 15.504253; val loss: 15.610058\n",
      "(Epoch 5 / 20) train loss: 16.774284; val loss: 17.510217\n",
      "(Epoch 6 / 20) train loss: 16.930951; val loss: 15.063262\n",
      "Stopping early at epoch 5!\n",
      "\n",
      "Evaluating Config #3 [of 30]:\n",
      " {'num_layer': 2, 'hidden_size': 157, 'learning_rate': 1.4763883954028538e-06, 'reg': 5.04447061791477e-06, 'activation': <exercise_code.networks.layer.Sigmoid object at 0x00000163E22FF188>, 'loss_func': <exercise_code.networks.loss.CrossEntropyFromLogits object at 0x00000163E22FF488>}\n",
      "(Epoch 1 / 20) train loss: 2.302488; val loss: 2.302626\n",
      "(Epoch 2 / 20) train loss: 2.302376; val loss: 2.302307\n",
      "(Epoch 3 / 20) train loss: 2.301867; val loss: 2.301954\n",
      "(Epoch 4 / 20) train loss: 2.301415; val loss: 2.301566\n",
      "(Epoch 5 / 20) train loss: 2.300936; val loss: 2.301087\n",
      "(Epoch 6 / 20) train loss: 2.300454; val loss: 2.300742\n",
      "(Epoch 7 / 20) train loss: 2.299954; val loss: 2.300348\n",
      "(Epoch 8 / 20) train loss: 2.299434; val loss: 2.299853\n",
      "(Epoch 9 / 20) train loss: 2.298893; val loss: 2.299326\n",
      "(Epoch 10 / 20) train loss: 2.298331; val loss: 2.298860\n",
      "(Epoch 11 / 20) train loss: 2.297720; val loss: 2.298336\n",
      "(Epoch 12 / 20) train loss: 2.297085; val loss: 2.297749\n",
      "(Epoch 13 / 20) train loss: 2.296424; val loss: 2.297092\n",
      "(Epoch 14 / 20) train loss: 2.295711; val loss: 2.296437\n",
      "(Epoch 15 / 20) train loss: 2.294956; val loss: 2.295718\n",
      "(Epoch 16 / 20) train loss: 2.294171; val loss: 2.294988\n",
      "(Epoch 17 / 20) train loss: 2.293326; val loss: 2.294115\n",
      "(Epoch 18 / 20) train loss: 2.292452; val loss: 2.293267\n",
      "(Epoch 19 / 20) train loss: 2.291533; val loss: 2.292369\n",
      "(Epoch 20 / 20) train loss: 2.290564; val loss: 2.291405\n",
      "\n",
      "Evaluating Config #4 [of 30]:\n",
      " {'num_layer': 5, 'hidden_size': 110, 'learning_rate': 8.349890985392285e-06, 'reg': 6.12769143434659e-05, 'activation': <exercise_code.networks.layer.Relu object at 0x00000163E22FF508>, 'loss_func': <exercise_code.networks.loss.CrossEntropyFromLogits object at 0x00000163E22FF488>}\n",
      "(Epoch 1 / 20) train loss: 2.302608; val loss: 2.302608\n",
      "(Epoch 2 / 20) train loss: 2.302609; val loss: 2.302609\n",
      "(Epoch 3 / 20) train loss: 2.302598; val loss: 2.302605\n",
      "(Epoch 4 / 20) train loss: 2.302588; val loss: 2.302603\n",
      "(Epoch 5 / 20) train loss: 2.302578; val loss: 2.302601\n",
      "(Epoch 6 / 20) train loss: 2.302569; val loss: 2.302597\n",
      "(Epoch 7 / 20) train loss: 2.302559; val loss: 2.302592\n",
      "(Epoch 8 / 20) train loss: 2.302550; val loss: 2.302588\n",
      "(Epoch 9 / 20) train loss: 2.302540; val loss: 2.302588\n",
      "(Epoch 10 / 20) train loss: 2.302530; val loss: 2.302584\n",
      "(Epoch 11 / 20) train loss: 2.302520; val loss: 2.302582\n",
      "(Epoch 12 / 20) train loss: 2.302510; val loss: 2.302580\n",
      "(Epoch 13 / 20) train loss: 2.302500; val loss: 2.302576\n",
      "(Epoch 14 / 20) train loss: 2.302491; val loss: 2.302573\n",
      "(Epoch 15 / 20) train loss: 2.302481; val loss: 2.302574\n",
      "(Epoch 16 / 20) train loss: 2.302470; val loss: 2.302569\n",
      "(Epoch 17 / 20) train loss: 2.302461; val loss: 2.302568\n",
      "(Epoch 18 / 20) train loss: 2.302451; val loss: 2.302561\n",
      "(Epoch 19 / 20) train loss: 2.302441; val loss: 2.302561\n",
      "(Epoch 20 / 20) train loss: 2.302430; val loss: 2.302558\n",
      "\n",
      "Evaluating Config #5 [of 30]:\n",
      " {'num_layer': 4, 'hidden_size': 112, 'learning_rate': 0.0001529530011162552, 'reg': 4.732319352279539e-06, 'activation': <exercise_code.networks.layer.Relu object at 0x00000163E22FF508>, 'loss_func': <exercise_code.networks.loss.CrossEntropyFromLogits object at 0x00000163E22FF488>}\n",
      "(Epoch 1 / 20) train loss: 2.302587; val loss: 2.302587\n",
      "(Epoch 2 / 20) train loss: 2.302663; val loss: 2.302584\n",
      "(Epoch 3 / 20) train loss: 2.302456; val loss: 2.302469\n",
      "(Epoch 4 / 20) train loss: 2.302287; val loss: 2.302512\n",
      "(Epoch 5 / 20) train loss: 2.302068; val loss: 2.302498\n",
      "(Epoch 6 / 20) train loss: 2.301808; val loss: 2.302434\n",
      "(Epoch 7 / 20) train loss: 2.282177; val loss: 2.293935\n",
      "(Epoch 8 / 20) train loss: 2.133470; val loss: 2.077938\n",
      "(Epoch 9 / 20) train loss: 2.031977; val loss: 1.959532\n",
      "(Epoch 10 / 20) train loss: 1.970891; val loss: 2.000893\n",
      "(Epoch 11 / 20) train loss: 1.906875; val loss: 1.945243\n",
      "(Epoch 12 / 20) train loss: 1.849521; val loss: 2.037755\n",
      "(Epoch 13 / 20) train loss: 1.788726; val loss: 2.141573\n",
      "(Epoch 14 / 20) train loss: 1.719471; val loss: 2.215807\n",
      "(Epoch 15 / 20) train loss: 1.653115; val loss: 2.088039\n",
      "(Epoch 16 / 20) train loss: 1.583939; val loss: 2.238767\n",
      "Stopping early at epoch 15!\n",
      "\n",
      "Evaluating Config #6 [of 30]:\n",
      " {'num_layer': 4, 'hidden_size': 125, 'learning_rate': 0.0014187222770935147, 'reg': 1.3501453363482713e-05, 'activation': <exercise_code.networks.layer.Sigmoid object at 0x00000163E22FF188>, 'loss_func': <exercise_code.networks.loss.CrossEntropyFromLogits object at 0x00000163E22FF488>}\n",
      "(Epoch 1 / 20) train loss: 2.302523; val loss: 2.302935\n",
      "(Epoch 2 / 20) train loss: 2.306828; val loss: 2.284072\n",
      "(Epoch 3 / 20) train loss: 2.271514; val loss: 2.222157\n",
      "(Epoch 4 / 20) train loss: 2.224141; val loss: 2.145510\n",
      "(Epoch 5 / 20) train loss: 2.182801; val loss: 2.105176\n",
      "(Epoch 6 / 20) train loss: 2.148697; val loss: 2.081070\n",
      "(Epoch 7 / 20) train loss: 2.122936; val loss: 2.063528\n",
      "(Epoch 8 / 20) train loss: 2.107216; val loss: 2.091313\n",
      "(Epoch 9 / 20) train loss: 2.105895; val loss: 2.080645\n",
      "(Epoch 10 / 20) train loss: 2.086172; val loss: 2.125474\n",
      "(Epoch 11 / 20) train loss: 2.052037; val loss: 2.036095\n",
      "(Epoch 12 / 20) train loss: 2.048931; val loss: 2.053719\n",
      "(Epoch 13 / 20) train loss: 2.033876; val loss: 2.043574\n",
      "(Epoch 14 / 20) train loss: 2.026612; val loss: 2.079730\n",
      "(Epoch 15 / 20) train loss: 2.009756; val loss: 2.110968\n",
      "(Epoch 16 / 20) train loss: 2.016594; val loss: 2.070701\n",
      "Stopping early at epoch 15!\n",
      "\n",
      "Evaluating Config #7 [of 30]:\n",
      " {'num_layer': 4, 'hidden_size': 140, 'learning_rate': 5.5682092431429454e-05, 'reg': 8.751786981576506e-06, 'activation': <exercise_code.networks.layer.Relu object at 0x00000163E22FF508>, 'loss_func': <exercise_code.networks.loss.CrossEntropyFromLogits object at 0x00000163E22FF488>}\n",
      "(Epoch 1 / 20) train loss: 2.302589; val loss: 2.302589\n",
      "(Epoch 2 / 20) train loss: 2.302597; val loss: 2.302602\n",
      "(Epoch 3 / 20) train loss: 2.302536; val loss: 2.302577\n",
      "(Epoch 4 / 20) train loss: 2.302473; val loss: 2.302548\n",
      "(Epoch 5 / 20) train loss: 2.302412; val loss: 2.302548\n",
      "(Epoch 6 / 20) train loss: 2.302350; val loss: 2.302529\n",
      "(Epoch 7 / 20) train loss: 2.302280; val loss: 2.302507\n",
      "(Epoch 8 / 20) train loss: 2.302205; val loss: 2.302498\n",
      "(Epoch 9 / 20) train loss: 2.302141; val loss: 2.302438\n",
      "(Epoch 10 / 20) train loss: 2.302068; val loss: 2.302440\n",
      "(Epoch 11 / 20) train loss: 2.301981; val loss: 2.302447\n",
      "(Epoch 12 / 20) train loss: 2.301914; val loss: 2.302434\n",
      "(Epoch 13 / 20) train loss: 2.301835; val loss: 2.302419\n",
      "(Epoch 14 / 20) train loss: 2.301759; val loss: 2.302429\n",
      "(Epoch 15 / 20) train loss: 2.301657; val loss: 2.302331\n",
      "(Epoch 16 / 20) train loss: 2.295750; val loss: 2.285716\n",
      "(Epoch 17 / 20) train loss: 2.234874; val loss: 2.145773\n",
      "(Epoch 18 / 20) train loss: 2.098491; val loss: 2.028391\n",
      "(Epoch 19 / 20) train loss: 2.040910; val loss: 2.015644\n",
      "(Epoch 20 / 20) train loss: 1.994138; val loss: 2.014637\n",
      "\n",
      "Evaluating Config #8 [of 30]:\n",
      " {'num_layer': 4, 'hidden_size': 119, 'learning_rate': 1.1617775334682319e-05, 'reg': 0.0008491395444105826, 'activation': <exercise_code.networks.layer.Relu object at 0x00000163E22FF508>, 'loss_func': <exercise_code.networks.loss.CrossEntropyFromLogits object at 0x00000163E22FF488>}\n",
      "(Epoch 1 / 20) train loss: 2.302920; val loss: 2.302920\n",
      "(Epoch 2 / 20) train loss: 2.302911; val loss: 2.302893\n",
      "(Epoch 3 / 20) train loss: 2.302866; val loss: 2.302864\n",
      "(Epoch 4 / 20) train loss: 2.302827; val loss: 2.302832\n",
      "(Epoch 5 / 20) train loss: 2.302788; val loss: 2.302807\n",
      "(Epoch 6 / 20) train loss: 2.302753; val loss: 2.302780\n",
      "(Epoch 7 / 20) train loss: 2.302720; val loss: 2.302757\n",
      "(Epoch 8 / 20) train loss: 2.302689; val loss: 2.302734\n",
      "(Epoch 9 / 20) train loss: 2.302658; val loss: 2.302721\n",
      "(Epoch 10 / 20) train loss: 2.302629; val loss: 2.302699\n",
      "(Epoch 11 / 20) train loss: 2.302602; val loss: 2.302682\n",
      "(Epoch 12 / 20) train loss: 2.302577; val loss: 2.302668\n",
      "(Epoch 13 / 20) train loss: 2.302551; val loss: 2.302653\n",
      "(Epoch 14 / 20) train loss: 2.302528; val loss: 2.302639\n",
      "(Epoch 15 / 20) train loss: 2.302503; val loss: 2.302623\n",
      "(Epoch 16 / 20) train loss: 2.302482; val loss: 2.302610\n",
      "(Epoch 17 / 20) train loss: 2.302459; val loss: 2.302602\n",
      "(Epoch 18 / 20) train loss: 2.302439; val loss: 2.302591\n",
      "(Epoch 19 / 20) train loss: 2.302418; val loss: 2.302583\n",
      "(Epoch 20 / 20) train loss: 2.302397; val loss: 2.302575\n",
      "\n",
      "Evaluating Config #9 [of 30]:\n",
      " {'num_layer': 2, 'hidden_size': 191, 'learning_rate': 3.2288135536504085e-05, 'reg': 0.0021716000352031716, 'activation': <exercise_code.networks.layer.Tanh object at 0x00000163E22FF648>, 'loss_func': <exercise_code.networks.loss.CrossEntropyFromLogits object at 0x00000163E22FF488>}\n",
      "(Epoch 1 / 20) train loss: 2.303936; val loss: 2.303909\n",
      "(Epoch 2 / 20) train loss: 2.242074; val loss: 2.134312\n",
      "(Epoch 3 / 20) train loss: 2.118775; val loss: 2.053164\n",
      "(Epoch 4 / 20) train loss: 2.043457; val loss: 1.996947\n",
      "(Epoch 5 / 20) train loss: 1.989314; val loss: 1.969798\n",
      "(Epoch 6 / 20) train loss: 1.945208; val loss: 1.934703\n",
      "(Epoch 7 / 20) train loss: 1.909996; val loss: 1.940511\n",
      "(Epoch 8 / 20) train loss: 1.875534; val loss: 1.922032\n",
      "(Epoch 9 / 20) train loss: 1.843409; val loss: 1.906237\n",
      "(Epoch 10 / 20) train loss: 1.810141; val loss: 1.889051\n",
      "(Epoch 11 / 20) train loss: 1.779266; val loss: 1.895972\n",
      "(Epoch 12 / 20) train loss: 1.750727; val loss: 1.882864\n",
      "(Epoch 13 / 20) train loss: 1.722125; val loss: 1.882109\n",
      "(Epoch 14 / 20) train loss: 1.691477; val loss: 1.892668\n",
      "(Epoch 15 / 20) train loss: 1.666035; val loss: 1.889127\n",
      "(Epoch 16 / 20) train loss: 1.637246; val loss: 1.895145\n",
      "(Epoch 17 / 20) train loss: 1.609232; val loss: 1.881083\n",
      "(Epoch 18 / 20) train loss: 1.585806; val loss: 1.877171\n",
      "(Epoch 19 / 20) train loss: 1.562598; val loss: 1.879653\n",
      "(Epoch 20 / 20) train loss: 1.533634; val loss: 1.902266\n",
      "\n",
      "Evaluating Config #10 [of 30]:\n",
      " {'num_layer': 4, 'hidden_size': 198, 'learning_rate': 1.1244241382341776e-06, 'reg': 0.007977328682668918, 'activation': <exercise_code.networks.layer.Relu object at 0x00000163E22FF508>, 'loss_func': <exercise_code.networks.loss.CrossEntropyFromLogits object at 0x00000163E22FF488>}\n",
      "(Epoch 1 / 20) train loss: 2.308068; val loss: 2.308068\n",
      "(Epoch 2 / 20) train loss: 2.307876; val loss: 2.307686\n",
      "(Epoch 3 / 20) train loss: 2.307505; val loss: 2.307329\n",
      "(Epoch 4 / 20) train loss: 2.307159; val loss: 2.306996\n",
      "(Epoch 5 / 20) train loss: 2.306837; val loss: 2.306685\n",
      "(Epoch 6 / 20) train loss: 2.306535; val loss: 2.306395\n",
      "(Epoch 7 / 20) train loss: 2.306254; val loss: 2.306125\n",
      "(Epoch 8 / 20) train loss: 2.305992; val loss: 2.305872\n",
      "(Epoch 9 / 20) train loss: 2.305747; val loss: 2.305637\n",
      "(Epoch 10 / 20) train loss: 2.305519; val loss: 2.305417\n",
      "(Epoch 11 / 20) train loss: 2.305306; val loss: 2.305213\n",
      "(Epoch 12 / 20) train loss: 2.305108; val loss: 2.305022\n",
      "(Epoch 13 / 20) train loss: 2.304923; val loss: 2.304845\n",
      "(Epoch 14 / 20) train loss: 2.304750; val loss: 2.304679\n",
      "(Epoch 15 / 20) train loss: 2.304590; val loss: 2.304525\n",
      "(Epoch 16 / 20) train loss: 2.304440; val loss: 2.304382\n",
      "(Epoch 17 / 20) train loss: 2.304301; val loss: 2.304248\n",
      "(Epoch 18 / 20) train loss: 2.304171; val loss: 2.304124\n",
      "(Epoch 19 / 20) train loss: 2.304050; val loss: 2.304008\n",
      "(Epoch 20 / 20) train loss: 2.303938; val loss: 2.303901\n",
      "\n",
      "Evaluating Config #11 [of 30]:\n",
      " {'num_layer': 4, 'hidden_size': 150, 'learning_rate': 0.009186257712687168, 'reg': 0.0012982601439488883, 'activation': <exercise_code.networks.layer.LeakyRelu object at 0x00000163E0C4E888>, 'loss_func': <exercise_code.networks.loss.CrossEntropyFromLogits object at 0x00000163E22FF488>}\n",
      "(Epoch 1 / 20) train loss: 2.303242; val loss: 2.303242\n",
      "(Epoch 2 / 20) train loss: 2.306061; val loss: 2.301748\n",
      "(Epoch 3 / 20) train loss: 2.305502; val loss: 2.304757\n",
      "(Epoch 4 / 20) train loss: 2.304175; val loss: 2.308895\n",
      "(Epoch 5 / 20) train loss: 2.304232; val loss: 2.299730\n",
      "(Epoch 6 / 20) train loss: 2.304741; val loss: 2.304717\n",
      "(Epoch 7 / 20) train loss: 2.304392; val loss: 2.303407\n",
      "(Epoch 8 / 20) train loss: 2.304119; val loss: 2.301631\n",
      "(Epoch 9 / 20) train loss: 2.304239; val loss: 2.305804\n",
      "(Epoch 10 / 20) train loss: 2.304154; val loss: 2.304468\n",
      "Stopping early at epoch 9!\n",
      "\n",
      "Evaluating Config #12 [of 30]:\n",
      " {'num_layer': 2, 'hidden_size': 175, 'learning_rate': 3.7745644886634785e-06, 'reg': 8.822271276512582e-05, 'activation': <exercise_code.networks.layer.Tanh object at 0x00000163E22FF648>, 'loss_func': <exercise_code.networks.loss.CrossEntropyFromLogits object at 0x00000163E22FF488>}\n",
      "(Epoch 1 / 20) train loss: 2.302630; val loss: 2.302612\n",
      "(Epoch 2 / 20) train loss: 2.298640; val loss: 2.292035\n",
      "(Epoch 3 / 20) train loss: 2.283889; val loss: 2.270540\n",
      "(Epoch 4 / 20) train loss: 2.263200; val loss: 2.244475\n",
      "(Epoch 5 / 20) train loss: 2.242264; val loss: 2.220059\n",
      "(Epoch 6 / 20) train loss: 2.223389; val loss: 2.197776\n",
      "(Epoch 7 / 20) train loss: 2.206433; val loss: 2.177622\n",
      "(Epoch 8 / 20) train loss: 2.191119; val loss: 2.159600\n",
      "(Epoch 9 / 20) train loss: 2.176776; val loss: 2.143363\n",
      "(Epoch 10 / 20) train loss: 2.163383; val loss: 2.128792\n",
      "(Epoch 11 / 20) train loss: 2.150746; val loss: 2.115232\n",
      "(Epoch 12 / 20) train loss: 2.138833; val loss: 2.103276\n",
      "(Epoch 13 / 20) train loss: 2.127378; val loss: 2.092284\n",
      "(Epoch 14 / 20) train loss: 2.116496; val loss: 2.082137\n",
      "(Epoch 15 / 20) train loss: 2.106104; val loss: 2.072928\n",
      "(Epoch 16 / 20) train loss: 2.096191; val loss: 2.064426\n",
      "(Epoch 17 / 20) train loss: 2.086857; val loss: 2.056539\n",
      "(Epoch 18 / 20) train loss: 2.077880; val loss: 2.049294\n",
      "(Epoch 19 / 20) train loss: 2.069296; val loss: 2.042400\n",
      "(Epoch 20 / 20) train loss: 2.060994; val loss: 2.035989\n",
      "\n",
      "Evaluating Config #13 [of 30]:\n",
      " {'num_layer': 3, 'hidden_size': 136, 'learning_rate': 0.0004509698738767535, 'reg': 0.01704085141988666, 'activation': <exercise_code.networks.layer.Relu object at 0x00000163E22FF508>, 'loss_func': <exercise_code.networks.loss.CrossEntropyFromLogits object at 0x00000163E22FF488>}\n",
      "(Epoch 1 / 20) train loss: 2.310051; val loss: 2.310050\n",
      "(Epoch 2 / 20) train loss: 2.302845; val loss: 2.302183\n",
      "(Epoch 3 / 20) train loss: 2.302234; val loss: 2.302221\n",
      "(Epoch 4 / 20) train loss: 2.301821; val loss: 2.301989\n",
      "(Epoch 5 / 20) train loss: 2.301518; val loss: 2.302030\n",
      "(Epoch 6 / 20) train loss: 2.301210; val loss: 2.302240\n",
      "(Epoch 7 / 20) train loss: 2.300965; val loss: 2.302106\n",
      "(Epoch 8 / 20) train loss: 2.300754; val loss: 2.302169\n",
      "(Epoch 9 / 20) train loss: 2.300541; val loss: 2.302211\n",
      "Stopping early at epoch 8!\n",
      "\n",
      "Evaluating Config #14 [of 30]:\n",
      " {'num_layer': 3, 'hidden_size': 160, 'learning_rate': 0.00244724155428824, 'reg': 0.000353205831316804, 'activation': <exercise_code.networks.layer.Sigmoid object at 0x00000163E22FF188>, 'loss_func': <exercise_code.networks.loss.CrossEntropyFromLogits object at 0x00000163E22FF488>}\n",
      "(Epoch 1 / 20) train loss: 2.302790; val loss: 2.303097\n",
      "(Epoch 2 / 20) train loss: 2.331252; val loss: 2.221675\n",
      "(Epoch 3 / 20) train loss: 2.288383; val loss: 2.250012\n",
      "(Epoch 4 / 20) train loss: 2.288707; val loss: 2.254409\n",
      "(Epoch 5 / 20) train loss: 2.293915; val loss: 2.243964\n",
      "(Epoch 6 / 20) train loss: 2.271922; val loss: 2.305120\n",
      "(Epoch 7 / 20) train loss: 2.297722; val loss: 2.235456\n",
      "Stopping early at epoch 6!\n",
      "\n",
      "Evaluating Config #15 [of 30]:\n",
      " {'num_layer': 4, 'hidden_size': 190, 'learning_rate': 0.0007176039306968647, 'reg': 4.533695136621218e-06, 'activation': <exercise_code.networks.layer.LeakyRelu object at 0x00000163E0C4E888>, 'loss_func': <exercise_code.networks.loss.CrossEntropyFromLogits object at 0x00000163E22FF488>}\n",
      "(Epoch 1 / 20) train loss: 2.302588; val loss: 2.302588\n",
      "(Epoch 2 / 20) train loss: 2.302669; val loss: 2.302104\n",
      "(Epoch 3 / 20) train loss: 2.217893; val loss: 2.092105\n",
      "(Epoch 4 / 20) train loss: 2.047336; val loss: 2.002018\n",
      "(Epoch 5 / 20) train loss: 1.924789; val loss: 1.913565\n",
      "(Epoch 6 / 20) train loss: 1.806448; val loss: 1.916773\n",
      "(Epoch 7 / 20) train loss: 1.645463; val loss: 2.090182\n",
      "(Epoch 8 / 20) train loss: 1.532335; val loss: 1.929064\n",
      "(Epoch 9 / 20) train loss: 1.406228; val loss: 2.031190\n",
      "(Epoch 10 / 20) train loss: 1.235515; val loss: 2.143111\n",
      "Stopping early at epoch 9!\n",
      "\n",
      "Evaluating Config #16 [of 30]:\n",
      " {'num_layer': 3, 'hidden_size': 160, 'learning_rate': 0.023523465984086386, 'reg': 0.022606443222410566, 'activation': <exercise_code.networks.layer.Sigmoid object at 0x00000163E22FF188>, 'loss_func': <exercise_code.networks.loss.CrossEntropyFromLogits object at 0x00000163E22FF488>}\n",
      "(Epoch 1 / 20) train loss: 2.314435; val loss: 2.314044\n",
      "(Epoch 2 / 20) train loss: 3.224301; val loss: 2.440384\n",
      "(Epoch 3 / 20) train loss: 2.439381; val loss: 2.341900\n",
      "(Epoch 4 / 20) train loss: 2.345971; val loss: 2.336518\n",
      "(Epoch 5 / 20) train loss: 2.334073; val loss: 2.336874\n",
      "(Epoch 6 / 20) train loss: 2.326288; val loss: 2.325126\n",
      "Stopping early at epoch 5!\n",
      "\n",
      "Evaluating Config #17 [of 30]:\n",
      " {'num_layer': 2, 'hidden_size': 133, 'learning_rate': 0.05503859695312279, 'reg': 4.1042277861940106e-05, 'activation': <exercise_code.networks.layer.Relu object at 0x00000163E22FF508>, 'loss_func': <exercise_code.networks.loss.CrossEntropyFromLogits object at 0x00000163E22FF488>}\n",
      "(Epoch 1 / 20) train loss: 2.302589; val loss: 2.302599\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\yimin\\OneDrive\\Study\\Master\\I2DL\\exercises\\exercise_06_cleaned\\exercise_code\\networks\\loss.py:170: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -y_truth_one_hot * np.log(y_out_probs)\n",
      "c:\\Users\\yimin\\OneDrive\\Study\\Master\\I2DL\\exercises\\exercise_06_cleaned\\exercise_code\\networks\\loss.py:170: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -y_truth_one_hot * np.log(y_out_probs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 2 / 20) train loss: nan; val loss: nan\n",
      "(Epoch 3 / 20) train loss: nan; val loss: nan\n",
      "(Epoch 4 / 20) train loss: nan; val loss: nan\n",
      "(Epoch 5 / 20) train loss: nan; val loss: nan\n",
      "(Epoch 6 / 20) train loss: nan; val loss: nan\n",
      "Stopping early at epoch 5!\n",
      "\n",
      "Evaluating Config #18 [of 30]:\n",
      " {'num_layer': 3, 'hidden_size': 104, 'learning_rate': 0.0036146362919976145, 'reg': 0.0018956102628706988, 'activation': <exercise_code.networks.layer.Sigmoid object at 0x00000163E22FF188>, 'loss_func': <exercise_code.networks.loss.CrossEntropyFromLogits object at 0x00000163E22FF488>}\n",
      "(Epoch 1 / 20) train loss: 2.303416; val loss: 2.303819\n",
      "(Epoch 2 / 20) train loss: 2.363351; val loss: 2.361328\n",
      "(Epoch 3 / 20) train loss: 2.375486; val loss: 2.402039\n",
      "(Epoch 4 / 20) train loss: 2.380579; val loss: 2.400551\n",
      "(Epoch 5 / 20) train loss: 2.369878; val loss: 2.321220\n",
      "(Epoch 6 / 20) train loss: 2.389478; val loss: 2.347179\n",
      "Stopping early at epoch 5!\n",
      "\n",
      "Evaluating Config #19 [of 30]:\n",
      " {'num_layer': 4, 'hidden_size': 118, 'learning_rate': 0.00152524110977055, 'reg': 2.0196158006001015e-06, 'activation': <exercise_code.networks.layer.Tanh object at 0x00000163E22FF648>, 'loss_func': <exercise_code.networks.loss.CrossEntropyFromLogits object at 0x00000163E22FF488>}\n",
      "(Epoch 1 / 20) train loss: 2.302586; val loss: 2.302586\n",
      "(Epoch 2 / 20) train loss: 2.225610; val loss: 2.016346\n",
      "(Epoch 3 / 20) train loss: 2.092997; val loss: 2.021737\n",
      "(Epoch 4 / 20) train loss: 2.063190; val loss: 1.942513\n",
      "(Epoch 5 / 20) train loss: 2.007259; val loss: 1.918591\n",
      "(Epoch 6 / 20) train loss: 1.973315; val loss: 1.900871\n",
      "(Epoch 7 / 20) train loss: 1.935803; val loss: 1.990647\n",
      "(Epoch 8 / 20) train loss: 1.918740; val loss: 1.967548\n",
      "(Epoch 9 / 20) train loss: 1.903598; val loss: 1.946305\n",
      "(Epoch 10 / 20) train loss: 1.879994; val loss: 1.973290\n",
      "(Epoch 11 / 20) train loss: 1.843077; val loss: 1.919928\n",
      "Stopping early at epoch 10!\n",
      "\n",
      "Evaluating Config #20 [of 30]:\n",
      " {'num_layer': 3, 'hidden_size': 174, 'learning_rate': 0.039574013217682405, 'reg': 0.02342150440614171, 'activation': <exercise_code.networks.layer.Relu object at 0x00000163E22FF508>, 'loss_func': <exercise_code.networks.loss.CrossEntropyFromLogits object at 0x00000163E22FF488>}\n",
      "(Epoch 1 / 20) train loss: 2.315873; val loss: 2.315873\n",
      "(Epoch 2 / 20) train loss: 17.230193; val loss: 3.557363\n",
      "(Epoch 3 / 20) train loss: 2.747418; val loss: 2.435859\n",
      "(Epoch 4 / 20) train loss: 2.383892; val loss: 2.332569\n",
      "(Epoch 5 / 20) train loss: 2.338035; val loss: 2.305379\n",
      "(Epoch 6 / 20) train loss: 2.322419; val loss: 2.335737\n",
      "(Epoch 7 / 20) train loss: 2.319886; val loss: 2.312279\n",
      "(Epoch 8 / 20) train loss: 2.321055; val loss: 2.307811\n",
      "(Epoch 9 / 20) train loss: 2.321289; val loss: 2.295911\n",
      "(Epoch 10 / 20) train loss: 2.315786; val loss: 2.336364\n",
      "(Epoch 11 / 20) train loss: 2.321922; val loss: 2.296190\n",
      "(Epoch 12 / 20) train loss: 2.318001; val loss: 2.299691\n",
      "(Epoch 13 / 20) train loss: 2.314526; val loss: 2.317146\n",
      "(Epoch 14 / 20) train loss: 2.318944; val loss: 2.317311\n",
      "Stopping early at epoch 13!\n",
      "\n",
      "Evaluating Config #21 [of 30]:\n",
      " {'num_layer': 3, 'hidden_size': 143, 'learning_rate': 7.311110317119408e-05, 'reg': 0.0008747357844193525, 'activation': <exercise_code.networks.layer.Relu object at 0x00000163E22FF508>, 'loss_func': <exercise_code.networks.loss.CrossEntropyFromLogits object at 0x00000163E22FF488>}\n",
      "(Epoch 1 / 20) train loss: 2.302989; val loss: 2.302989\n",
      "(Epoch 2 / 20) train loss: 2.295406; val loss: 2.187106\n",
      "(Epoch 3 / 20) train loss: 2.149722; val loss: 2.078949\n",
      "(Epoch 4 / 20) train loss: 2.076366; val loss: 2.044029\n",
      "(Epoch 5 / 20) train loss: 2.044504; val loss: 2.012380\n",
      "(Epoch 6 / 20) train loss: 2.000942; val loss: 1.994463\n",
      "(Epoch 7 / 20) train loss: 1.943243; val loss: 1.923057\n",
      "(Epoch 8 / 20) train loss: 1.875041; val loss: 1.903656\n",
      "(Epoch 9 / 20) train loss: 1.827403; val loss: 1.879916\n",
      "(Epoch 10 / 20) train loss: 1.771840; val loss: 1.928238\n",
      "(Epoch 11 / 20) train loss: 1.733285; val loss: 1.865964\n",
      "(Epoch 12 / 20) train loss: 1.693661; val loss: 1.895313\n",
      "(Epoch 13 / 20) train loss: 1.655521; val loss: 1.998125\n",
      "(Epoch 14 / 20) train loss: 1.628322; val loss: 1.937131\n",
      "(Epoch 15 / 20) train loss: 1.591440; val loss: 1.936481\n",
      "(Epoch 16 / 20) train loss: 1.554680; val loss: 2.014744\n",
      "Stopping early at epoch 15!\n",
      "\n",
      "Evaluating Config #22 [of 30]:\n",
      " {'num_layer': 2, 'hidden_size': 140, 'learning_rate': 3.9903899016971645e-06, 'reg': 1.789911025122017e-05, 'activation': <exercise_code.networks.layer.LeakyRelu object at 0x00000163E0C4E888>, 'loss_func': <exercise_code.networks.loss.CrossEntropyFromLogits object at 0x00000163E22FF488>}\n",
      "(Epoch 1 / 20) train loss: 2.302573; val loss: 2.302524\n",
      "(Epoch 2 / 20) train loss: 2.301407; val loss: 2.299393\n",
      "(Epoch 3 / 20) train loss: 2.296156; val loss: 2.290790\n",
      "(Epoch 4 / 20) train loss: 2.284536; val loss: 2.274307\n",
      "(Epoch 5 / 20) train loss: 2.266095; val loss: 2.251128\n",
      "(Epoch 6 / 20) train loss: 2.243236; val loss: 2.224423\n",
      "(Epoch 7 / 20) train loss: 2.219325; val loss: 2.197404\n",
      "(Epoch 8 / 20) train loss: 2.196512; val loss: 2.173783\n",
      "(Epoch 9 / 20) train loss: 2.176090; val loss: 2.152171\n",
      "(Epoch 10 / 20) train loss: 2.157962; val loss: 2.132485\n",
      "(Epoch 11 / 20) train loss: 2.141682; val loss: 2.115807\n",
      "(Epoch 12 / 20) train loss: 2.126932; val loss: 2.101193\n",
      "(Epoch 13 / 20) train loss: 2.113381; val loss: 2.087650\n",
      "(Epoch 14 / 20) train loss: 2.100748; val loss: 2.075088\n",
      "(Epoch 15 / 20) train loss: 2.088848; val loss: 2.063958\n",
      "(Epoch 16 / 20) train loss: 2.077622; val loss: 2.054152\n",
      "(Epoch 17 / 20) train loss: 2.067000; val loss: 2.044440\n",
      "(Epoch 18 / 20) train loss: 2.056708; val loss: 2.034778\n",
      "(Epoch 19 / 20) train loss: 2.046912; val loss: 2.026827\n",
      "(Epoch 20 / 20) train loss: 2.037552; val loss: 2.018773\n",
      "\n",
      "Evaluating Config #23 [of 30]:\n",
      " {'num_layer': 3, 'hidden_size': 101, 'learning_rate': 0.010873810443780121, 'reg': 0.007812134088862858, 'activation': <exercise_code.networks.layer.Tanh object at 0x00000163E22FF648>, 'loss_func': <exercise_code.networks.loss.CrossEntropyFromLogits object at 0x00000163E22FF488>}\n",
      "(Epoch 1 / 20) train loss: 2.305098; val loss: 2.305098\n",
      "(Epoch 2 / 20) train loss: 4.968454; val loss: 4.765784\n",
      "(Epoch 3 / 20) train loss: 5.403076; val loss: 5.597774\n",
      "(Epoch 4 / 20) train loss: 5.233739; val loss: 6.691441\n",
      "(Epoch 5 / 20) train loss: 5.274155; val loss: 4.898111\n",
      "(Epoch 6 / 20) train loss: 5.223009; val loss: 5.477318\n",
      "Stopping early at epoch 5!\n",
      "\n",
      "Evaluating Config #24 [of 30]:\n",
      " {'num_layer': 3, 'hidden_size': 164, 'learning_rate': 0.0010430896840922929, 'reg': 0.008227265837597295, 'activation': <exercise_code.networks.layer.Sigmoid object at 0x00000163E22FF188>, 'loss_func': <exercise_code.networks.loss.CrossEntropyFromLogits object at 0x00000163E22FF488>}\n",
      "(Epoch 1 / 20) train loss: 2.306929; val loss: 2.306637\n",
      "(Epoch 2 / 20) train loss: 2.332724; val loss: 2.340255\n",
      "(Epoch 3 / 20) train loss: 2.334299; val loss: 2.329562\n",
      "(Epoch 4 / 20) train loss: 2.328383; val loss: 2.320294\n",
      "(Epoch 5 / 20) train loss: 2.310815; val loss: 2.395071\n",
      "(Epoch 6 / 20) train loss: 2.315095; val loss: 2.299051\n",
      "(Epoch 7 / 20) train loss: 2.313710; val loss: 2.281256\n",
      "(Epoch 8 / 20) train loss: 2.296486; val loss: 2.225997\n",
      "(Epoch 9 / 20) train loss: 2.284642; val loss: 2.244297\n",
      "(Epoch 10 / 20) train loss: 2.290273; val loss: 2.228048\n",
      "(Epoch 11 / 20) train loss: 2.296241; val loss: 2.187365\n",
      "(Epoch 12 / 20) train loss: 2.278068; val loss: 2.277165\n",
      "(Epoch 13 / 20) train loss: 2.284172; val loss: 2.268500\n",
      "(Epoch 14 / 20) train loss: 2.293385; val loss: 2.247906\n",
      "(Epoch 15 / 20) train loss: 2.272893; val loss: 2.150698\n",
      "(Epoch 16 / 20) train loss: 2.270695; val loss: 2.279260\n",
      "(Epoch 17 / 20) train loss: 2.278718; val loss: 2.168204\n",
      "(Epoch 18 / 20) train loss: 2.282130; val loss: 2.190883\n",
      "(Epoch 19 / 20) train loss: 2.282569; val loss: 2.187465\n",
      "(Epoch 20 / 20) train loss: 2.267037; val loss: 2.163937\n",
      "Stopping early at epoch 19!\n",
      "\n",
      "Evaluating Config #25 [of 30]:\n",
      " {'num_layer': 5, 'hidden_size': 187, 'learning_rate': 0.002540249712203581, 'reg': 2.5718207981765057e-05, 'activation': <exercise_code.networks.layer.Tanh object at 0x00000163E22FF648>, 'loss_func': <exercise_code.networks.loss.CrossEntropyFromLogits object at 0x00000163E22FF488>}\n",
      "(Epoch 1 / 20) train loss: 2.302603; val loss: 2.302603\n",
      "(Epoch 2 / 20) train loss: 2.358940; val loss: 2.282554\n",
      "(Epoch 3 / 20) train loss: 2.345191; val loss: 2.173058\n",
      "(Epoch 4 / 20) train loss: 2.274644; val loss: 2.140421\n",
      "(Epoch 5 / 20) train loss: 2.333886; val loss: 2.321772\n",
      "(Epoch 6 / 20) train loss: 2.267879; val loss: 2.241708\n",
      "(Epoch 7 / 20) train loss: 2.288029; val loss: 2.260517\n",
      "(Epoch 8 / 20) train loss: 2.308164; val loss: 2.457437\n",
      "(Epoch 9 / 20) train loss: 2.281920; val loss: 2.281907\n",
      "Stopping early at epoch 8!\n",
      "\n",
      "Evaluating Config #26 [of 30]:\n",
      " {'num_layer': 4, 'hidden_size': 131, 'learning_rate': 8.047965755298479e-05, 'reg': 0.00027919736132078863, 'activation': <exercise_code.networks.layer.Relu object at 0x00000163E22FF508>, 'loss_func': <exercise_code.networks.loss.CrossEntropyFromLogits object at 0x00000163E22FF488>}\n",
      "(Epoch 1 / 20) train loss: 2.302708; val loss: 2.302708\n",
      "(Epoch 2 / 20) train loss: 2.302702; val loss: 2.302648\n",
      "(Epoch 3 / 20) train loss: 2.302579; val loss: 2.302594\n",
      "(Epoch 4 / 20) train loss: 2.302460; val loss: 2.302520\n",
      "(Epoch 5 / 20) train loss: 2.302353; val loss: 2.302493\n",
      "(Epoch 6 / 20) train loss: 2.302233; val loss: 2.302416\n",
      "(Epoch 7 / 20) train loss: 2.302081; val loss: 2.302401\n",
      "(Epoch 8 / 20) train loss: 2.301942; val loss: 2.302371\n",
      "(Epoch 9 / 20) train loss: 2.301784; val loss: 2.302380\n",
      "(Epoch 10 / 20) train loss: 2.301650; val loss: 2.302322\n",
      "(Epoch 11 / 20) train loss: 2.301493; val loss: 2.302310\n",
      "(Epoch 12 / 20) train loss: 2.301266; val loss: 2.301943\n",
      "(Epoch 13 / 20) train loss: 2.271481; val loss: 2.235639\n",
      "(Epoch 14 / 20) train loss: 2.145435; val loss: 2.022948\n",
      "(Epoch 15 / 20) train loss: 2.036770; val loss: 1.982333\n",
      "(Epoch 16 / 20) train loss: 1.980656; val loss: 1.969685\n",
      "(Epoch 17 / 20) train loss: 1.936924; val loss: 1.949772\n",
      "(Epoch 18 / 20) train loss: 1.893779; val loss: 1.965956\n",
      "(Epoch 19 / 20) train loss: 1.848253; val loss: 1.992120\n",
      "(Epoch 20 / 20) train loss: 1.812872; val loss: 2.020416\n",
      "\n",
      "Evaluating Config #27 [of 30]:\n",
      " {'num_layer': 3, 'hidden_size': 104, 'learning_rate': 0.01147552777859783, 'reg': 0.00012041466118367627, 'activation': <exercise_code.networks.layer.Sigmoid object at 0x00000163E22FF188>, 'loss_func': <exercise_code.networks.loss.CrossEntropyFromLogits object at 0x00000163E22FF488>}\n",
      "(Epoch 1 / 20) train loss: 2.302552; val loss: 2.302420\n",
      "(Epoch 2 / 20) train loss: 2.403841; val loss: 2.387104\n",
      "(Epoch 3 / 20) train loss: 2.463569; val loss: 2.460599\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\yimin\\OneDrive\\Study\\Master\\I2DL\\exercises\\exercise_06_cleaned\\exercise_code\\networks\\layer.py:66: RuntimeWarning: overflow encountered in exp\n",
      "  outputs = 1 / (1 + np.exp(-x))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 4 / 20) train loss: 2.622837; val loss: 2.562382\n",
      "(Epoch 5 / 20) train loss: 2.700469; val loss: 2.771326\n",
      "(Epoch 6 / 20) train loss: 2.785025; val loss: 2.769407\n",
      "Stopping early at epoch 5!\n",
      "\n",
      "Evaluating Config #28 [of 30]:\n",
      " {'num_layer': 4, 'hidden_size': 168, 'learning_rate': 3.5726807973488973e-06, 'reg': 0.017952065828352532, 'activation': <exercise_code.networks.layer.Sigmoid object at 0x00000163E22FF188>, 'loss_func': <exercise_code.networks.loss.CrossEntropyFromLogits object at 0x00000163E22FF488>}\n",
      "(Epoch 1 / 20) train loss: 2.312881; val loss: 2.312458\n",
      "(Epoch 2 / 20) train loss: 2.311086; val loss: 2.308814\n",
      "(Epoch 3 / 20) train loss: 2.307799; val loss: 2.306386\n",
      "(Epoch 4 / 20) train loss: 2.305593; val loss: 2.304904\n",
      "(Epoch 5 / 20) train loss: 2.304036; val loss: 2.303764\n",
      "(Epoch 6 / 20) train loss: 2.302968; val loss: 2.303104\n",
      "(Epoch 7 / 20) train loss: 2.302254; val loss: 2.302697\n",
      "(Epoch 8 / 20) train loss: 2.301728; val loss: 2.302383\n",
      "(Epoch 9 / 20) train loss: 2.301320; val loss: 2.302335\n",
      "(Epoch 10 / 20) train loss: 2.301048; val loss: 2.302224\n",
      "(Epoch 11 / 20) train loss: 2.300825; val loss: 2.302147\n",
      "(Epoch 12 / 20) train loss: 2.300649; val loss: 2.302315\n",
      "(Epoch 13 / 20) train loss: 2.300516; val loss: 2.302263\n",
      "(Epoch 14 / 20) train loss: 2.300346; val loss: 2.302265\n",
      "(Epoch 15 / 20) train loss: 2.300289; val loss: 2.302309\n",
      "(Epoch 16 / 20) train loss: 2.300195; val loss: 2.302359\n",
      "Stopping early at epoch 15!\n",
      "\n",
      "Evaluating Config #29 [of 30]:\n",
      " {'num_layer': 5, 'hidden_size': 102, 'learning_rate': 5.821735847295048e-06, 'reg': 2.3514482938023524e-05, 'activation': <exercise_code.networks.layer.LeakyRelu object at 0x00000163E0C4E888>, 'loss_func': <exercise_code.networks.loss.CrossEntropyFromLogits object at 0x00000163E22FF488>}\n",
      "(Epoch 1 / 20) train loss: 2.302593; val loss: 2.302593\n",
      "(Epoch 2 / 20) train loss: 2.302593; val loss: 2.302593\n",
      "(Epoch 3 / 20) train loss: 2.302585; val loss: 2.302590\n",
      "(Epoch 4 / 20) train loss: 2.302580; val loss: 2.302588\n",
      "(Epoch 5 / 20) train loss: 2.302573; val loss: 2.302584\n",
      "(Epoch 6 / 20) train loss: 2.302566; val loss: 2.302584\n",
      "(Epoch 7 / 20) train loss: 2.302559; val loss: 2.302581\n",
      "(Epoch 8 / 20) train loss: 2.302553; val loss: 2.302579\n",
      "(Epoch 9 / 20) train loss: 2.302546; val loss: 2.302576\n",
      "(Epoch 10 / 20) train loss: 2.302539; val loss: 2.302575\n",
      "(Epoch 11 / 20) train loss: 2.302533; val loss: 2.302572\n",
      "(Epoch 12 / 20) train loss: 2.302525; val loss: 2.302573\n",
      "(Epoch 13 / 20) train loss: 2.302519; val loss: 2.302569\n",
      "(Epoch 14 / 20) train loss: 2.302512; val loss: 2.302564\n",
      "(Epoch 15 / 20) train loss: 2.302505; val loss: 2.302566\n",
      "(Epoch 16 / 20) train loss: 2.302498; val loss: 2.302565\n",
      "(Epoch 17 / 20) train loss: 2.302492; val loss: 2.302562\n",
      "(Epoch 18 / 20) train loss: 2.302484; val loss: 2.302560\n",
      "(Epoch 19 / 20) train loss: 2.302477; val loss: 2.302558\n",
      "(Epoch 20 / 20) train loss: 2.302471; val loss: 2.302556\n",
      "\n",
      "Evaluating Config #30 [of 30]:\n",
      " {'num_layer': 5, 'hidden_size': 103, 'learning_rate': 0.003115242703196979, 'reg': 0.00012037997888715435, 'activation': <exercise_code.networks.layer.Sigmoid object at 0x00000163E22FF188>, 'loss_func': <exercise_code.networks.loss.CrossEntropyFromLogits object at 0x00000163E22FF488>}\n",
      "(Epoch 1 / 20) train loss: 2.302604; val loss: 2.303202\n",
      "(Epoch 2 / 20) train loss: 2.310215; val loss: 2.304587\n",
      "(Epoch 3 / 20) train loss: 2.303774; val loss: 2.301353\n",
      "(Epoch 4 / 20) train loss: 2.300620; val loss: 2.286979\n",
      "(Epoch 5 / 20) train loss: 2.289413; val loss: 2.297709\n",
      "(Epoch 6 / 20) train loss: 2.279066; val loss: 2.288992\n",
      "(Epoch 7 / 20) train loss: 2.299863; val loss: 2.251411\n",
      "(Epoch 8 / 20) train loss: 2.261565; val loss: 2.294165\n",
      "(Epoch 9 / 20) train loss: 2.295290; val loss: 2.214893\n",
      "(Epoch 10 / 20) train loss: 2.282085; val loss: 2.193821\n",
      "(Epoch 11 / 20) train loss: 2.279983; val loss: 2.243080\n",
      "(Epoch 12 / 20) train loss: 2.302813; val loss: 2.204620\n",
      "(Epoch 13 / 20) train loss: 2.280775; val loss: 2.240954\n",
      "(Epoch 14 / 20) train loss: 2.281834; val loss: 2.223259\n",
      "(Epoch 15 / 20) train loss: 2.283477; val loss: 2.221236\n",
      "Stopping early at epoch 14!\n",
      "\n",
      "Search done. Best Val Loss = 1.8659639822364753\n",
      "Best Config: {'num_layer': 3, 'hidden_size': 143, 'learning_rate': 7.311110317119408e-05, 'reg': 0.0008747357844193525, 'activation': <exercise_code.networks.layer.Relu object at 0x00000163E22FF508>, 'loss_func': <exercise_code.networks.loss.CrossEntropyFromLogits object at 0x00000163E22FF488>}\n"
     ]
    }
   ],
   "source": [
    "from exercise_code.hyperparameter_tuning import random_search\n",
    "from exercise_code.networks import MyOwnNetwork, ClassificationNet\n",
    "\n",
    "best_model = ClassificationNet\n",
    "\n",
    "########################################################################\n",
    "# TODO:                                                                #\n",
    "# Implement your own neural network and find suitable hyperparameters  #\n",
    "# Be sure to edit the MyOwnNetwork class in the following code snippet #\n",
    "# to upload the correct model!                                         #\n",
    "########################################################################\n",
    "# write your code here\n",
    "best_model, results = random_search(\n",
    "    dataloaders['train_small'], dataloaders['val_500files'],\n",
    "    random_search_spaces = {\n",
    "        \"num_layer\": ([2, 5], 'int'),\n",
    "        \"hidden_size\": ([100, 200], \"int\"),\n",
    "        \"learning_rate\": ([1e-6, 1e-1], 'log'),\n",
    "        \"reg\": ([1e-6, 1e-1], \"log\"),\n",
    "        \"activation\": ([Sigmoid(), Relu(), Tanh(), LeakyRelu()], \"item\"),\n",
    "        \"loss_func\": ([CrossEntropyFromLogits()], \"item\")\n",
    "    },\n",
    "    model_class=best_model,\n",
    "    num_search = 30, epochs=20, patience=5)\n",
    "########################################################################\n",
    "#                           END OF YOUR CODE                           #\n",
    "########################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 1 / 100) train loss: 2.302988; val loss: 2.302988\n",
      "(Epoch 2 / 100) train loss: 2.302632; val loss: 2.298276\n",
      "(Epoch 3 / 100) train loss: 2.177716; val loss: 2.101902\n",
      "(Epoch 4 / 100) train loss: 2.065552; val loss: 2.041366\n",
      "(Epoch 5 / 100) train loss: 1.986703; val loss: 1.945116\n",
      "(Epoch 6 / 100) train loss: 1.900232; val loss: 1.887869\n",
      "(Epoch 7 / 100) train loss: 1.846362; val loss: 1.842626\n",
      "(Epoch 8 / 100) train loss: 1.800512; val loss: 1.804489\n",
      "(Epoch 9 / 100) train loss: 1.762108; val loss: 1.771112\n",
      "(Epoch 10 / 100) train loss: 1.728489; val loss: 1.746501\n",
      "(Epoch 11 / 100) train loss: 1.700503; val loss: 1.722177\n",
      "(Epoch 12 / 100) train loss: 1.674584; val loss: 1.704954\n",
      "(Epoch 13 / 100) train loss: 1.648981; val loss: 1.682695\n",
      "(Epoch 14 / 100) train loss: 1.624936; val loss: 1.666373\n",
      "(Epoch 15 / 100) train loss: 1.600543; val loss: 1.650738\n",
      "(Epoch 16 / 100) train loss: 1.577517; val loss: 1.634301\n",
      "(Epoch 17 / 100) train loss: 1.556484; val loss: 1.621621\n",
      "(Epoch 18 / 100) train loss: 1.535417; val loss: 1.612615\n",
      "(Epoch 19 / 100) train loss: 1.516766; val loss: 1.599565\n",
      "(Epoch 20 / 100) train loss: 1.496448; val loss: 1.589187\n",
      "(Epoch 21 / 100) train loss: 1.479874; val loss: 1.580153\n",
      "(Epoch 22 / 100) train loss: 1.461761; val loss: 1.569293\n",
      "(Epoch 23 / 100) train loss: 1.444093; val loss: 1.560699\n",
      "(Epoch 24 / 100) train loss: 1.429639; val loss: 1.553952\n",
      "(Epoch 25 / 100) train loss: 1.413455; val loss: 1.548647\n",
      "(Epoch 26 / 100) train loss: 1.399249; val loss: 1.544636\n",
      "(Epoch 27 / 100) train loss: 1.383586; val loss: 1.540514\n",
      "(Epoch 28 / 100) train loss: 1.371961; val loss: 1.536569\n",
      "(Epoch 29 / 100) train loss: 1.357189; val loss: 1.530541\n",
      "(Epoch 30 / 100) train loss: 1.345982; val loss: 1.526869\n",
      "(Epoch 31 / 100) train loss: 1.331806; val loss: 1.522311\n",
      "(Epoch 32 / 100) train loss: 1.319543; val loss: 1.520430\n",
      "(Epoch 33 / 100) train loss: 1.307826; val loss: 1.522058\n",
      "(Epoch 34 / 100) train loss: 1.296495; val loss: 1.516292\n",
      "(Epoch 35 / 100) train loss: 1.284438; val loss: 1.516632\n",
      "(Epoch 36 / 100) train loss: 1.272280; val loss: 1.520751\n",
      "(Epoch 37 / 100) train loss: 1.265017; val loss: 1.508883\n",
      "(Epoch 38 / 100) train loss: 1.249669; val loss: 1.524598\n",
      "(Epoch 39 / 100) train loss: 1.240568; val loss: 1.513544\n",
      "(Epoch 40 / 100) train loss: 1.230748; val loss: 1.510854\n",
      "(Epoch 41 / 100) train loss: 1.218427; val loss: 1.510291\n",
      "(Epoch 42 / 100) train loss: 1.211227; val loss: 1.515670\n",
      "(Epoch 43 / 100) train loss: 1.198734; val loss: 1.509809\n",
      "(Epoch 44 / 100) train loss: 1.188313; val loss: 1.510409\n",
      "(Epoch 45 / 100) train loss: 1.178135; val loss: 1.512359\n",
      "(Epoch 46 / 100) train loss: 1.169344; val loss: 1.514325\n",
      "(Epoch 47 / 100) train loss: 1.159778; val loss: 1.515481\n",
      "(Epoch 48 / 100) train loss: 1.148096; val loss: 1.517979\n",
      "(Epoch 49 / 100) train loss: 1.138807; val loss: 1.518968\n",
      "(Epoch 50 / 100) train loss: 1.130077; val loss: 1.524279\n",
      "(Epoch 51 / 100) train loss: 1.121266; val loss: 1.526182\n",
      "(Epoch 52 / 100) train loss: 1.111338; val loss: 1.524153\n",
      "(Epoch 53 / 100) train loss: 1.102635; val loss: 1.539637\n",
      "(Epoch 54 / 100) train loss: 1.093584; val loss: 1.534332\n",
      "(Epoch 55 / 100) train loss: 1.084989; val loss: 1.545273\n",
      "(Epoch 56 / 100) train loss: 1.077604; val loss: 1.539010\n",
      "(Epoch 57 / 100) train loss: 1.067483; val loss: 1.546448\n",
      "(Epoch 58 / 100) train loss: 1.058471; val loss: 1.541031\n",
      "(Epoch 59 / 100) train loss: 1.049693; val loss: 1.557950\n",
      "(Epoch 60 / 100) train loss: 1.041533; val loss: 1.549605\n",
      "(Epoch 61 / 100) train loss: 1.035872; val loss: 1.556756\n",
      "(Epoch 62 / 100) train loss: 1.025217; val loss: 1.564780\n",
      "(Epoch 63 / 100) train loss: 1.016441; val loss: 1.560425\n",
      "(Epoch 64 / 100) train loss: 1.008484; val loss: 1.570927\n",
      "(Epoch 65 / 100) train loss: 1.000073; val loss: 1.576418\n",
      "(Epoch 66 / 100) train loss: 0.994199; val loss: 1.582703\n",
      "(Epoch 67 / 100) train loss: 0.986826; val loss: 1.583448\n",
      "(Epoch 68 / 100) train loss: 0.976186; val loss: 1.592992\n",
      "(Epoch 69 / 100) train loss: 0.971362; val loss: 1.596658\n",
      "(Epoch 70 / 100) train loss: 0.960212; val loss: 1.624042\n",
      "(Epoch 71 / 100) train loss: 0.955646; val loss: 1.601419\n",
      "(Epoch 72 / 100) train loss: 0.949401; val loss: 1.616833\n",
      "(Epoch 73 / 100) train loss: 0.943200; val loss: 1.610312\n",
      "(Epoch 74 / 100) train loss: 0.932139; val loss: 1.617489\n",
      "(Epoch 75 / 100) train loss: 0.924609; val loss: 1.633673\n",
      "(Epoch 76 / 100) train loss: 0.916348; val loss: 1.629820\n",
      "(Epoch 77 / 100) train loss: 0.909127; val loss: 1.633809\n",
      "(Epoch 78 / 100) train loss: 0.903689; val loss: 1.650602\n",
      "(Epoch 79 / 100) train loss: 0.894749; val loss: 1.659287\n",
      "(Epoch 80 / 100) train loss: 0.888570; val loss: 1.654012\n",
      "(Epoch 81 / 100) train loss: 0.883181; val loss: 1.672766\n",
      "(Epoch 82 / 100) train loss: 0.873263; val loss: 1.674605\n",
      "(Epoch 83 / 100) train loss: 0.870531; val loss: 1.686790\n",
      "(Epoch 84 / 100) train loss: 0.860913; val loss: 1.684167\n",
      "(Epoch 85 / 100) train loss: 0.856353; val loss: 1.691305\n",
      "(Epoch 86 / 100) train loss: 0.847886; val loss: 1.702518\n",
      "(Epoch 87 / 100) train loss: 0.839994; val loss: 1.706271\n",
      "(Epoch 88 / 100) train loss: 0.834709; val loss: 1.712428\n",
      "(Epoch 89 / 100) train loss: 0.827321; val loss: 1.725223\n",
      "(Epoch 90 / 100) train loss: 0.821963; val loss: 1.733660\n",
      "(Epoch 91 / 100) train loss: 0.815300; val loss: 1.747251\n",
      "(Epoch 92 / 100) train loss: 0.808588; val loss: 1.742770\n",
      "(Epoch 93 / 100) train loss: 0.802449; val loss: 1.761708\n",
      "(Epoch 94 / 100) train loss: 0.795099; val loss: 1.764148\n",
      "(Epoch 95 / 100) train loss: 0.794024; val loss: 1.778166\n",
      "(Epoch 96 / 100) train loss: 0.786546; val loss: 1.782571\n",
      "(Epoch 97 / 100) train loss: 0.777304; val loss: 1.789299\n",
      "(Epoch 98 / 100) train loss: 0.771546; val loss: 1.798245\n",
      "(Epoch 99 / 100) train loss: 0.763138; val loss: 1.812166\n",
      "(Epoch 100 / 100) train loss: 0.758639; val loss: 1.809204\n"
     ]
    }
   ],
   "source": [
    "final_model = ClassificationNet(activation=Relu(), num_layer=3, hidden_size=143, reg=0.0008747357844193525)\n",
    "solver = Solver(final_model, dataloaders['train'], dataloaders['val'], \n",
    "                learning_rate=7.311110317119408e-05, loss_func=CrossEntropyFromLogits(), optimizer=Adam)\n",
    "solver.train(epochs=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time to edit the ranges above and adjust them to explore regions that performed well!\n",
    "\n",
    "Also, feel free to experiment around! Also the network architecture, optimizer options and activations functions, etc. are hyperparameters that you can change!\n",
    "\n",
    "Try to get your accuracy as high as possible! That's all what counts for this submission!\n",
    "\n",
    "You'll pass if you reach at least **48%** accuracy on our test set - but there will also be a leaderboard of all students of this course. Will you make it to the top?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 Checking the validation accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 79.69417735042735%\n",
      "Validation Accuracy: 49.96995192307692%\n"
     ]
    }
   ],
   "source": [
    "labels, pred, acc = final_model.get_dataset_prediction(dataloaders['train'])\n",
    "print(\"Train Accuracy: {}%\".format(acc*100))\n",
    "labels, pred, acc = final_model.get_dataset_prediction(dataloaders['val'])\n",
    "print(\"Validation Accuracy: {}%\".format(acc*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Test your model\n",
    "When you have finished your hyperparameter tuning and are sure you have your final model that performs well on the validation set (**you should at least get 48% accuracy on the validation set!**), it's time to run your  model on the test set.\n",
    "\n",
    "<div class=\"alert alert-danger\">\n",
    "    <h3>Important</h3>\n",
    "    <p>As you have learned in the lecture, you must only use the test set one single time! So only run the next cell if you are really sure your model works well enough and that you want to submit. Your test set is different from the test set on our server, so results may vary. Nevertheless, you will have a reasonable close approximation about your performance if you only do a final evaluation on the test set.</p>\n",
    "    <p>If you are an external student that can't use our submission webpage: this test performance is your final result and if you surpassed the threshold, you have completed this exercise :). Now, train again to aim for a better number!</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 49.08854166666667%\n"
     ]
    }
   ],
   "source": [
    "# comment this part out to see your model's performance on the test set.\n",
    "labels, pred, acc = final_model.get_dataset_prediction(dataloaders['test'])\n",
    "print(\"Test Accuracy: {}%\".format(acc*100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <h3>Note:</h3>\n",
    "    <p>The \"real\" test set is actually the dataset we're using for testing your model, which is <b>different</b> from the test-set you're using here.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Saving your Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from exercise_code.tests import save_pickle\n",
    "save_pickle({\"cifar_fcn\": final_model}, \"cifar_fcn.p\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relevant folders: ['exercise_code', 'models']\n",
      "notebooks files: ['1.cifar10_classification.ipynb']\n",
      "Adding folder exercise_code\n",
      "Adding folder models\n",
      "Adding notebook 1.cifar10_classification.ipynb\n",
      "Zipping successful! Zip is stored under: c:\\Users\\yimin\\OneDrive\\Study\\Master\\I2DL\\exercises\\exercise_06_cleaned\\exercise06.zip\n"
     ]
    }
   ],
   "source": [
    "from exercise_code.submit import submit_exercise\n",
    "\n",
    "submit_exercise('exercise06')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Submission Instructions\n",
    "\n",
    "Congratulations! You've just built your first image classifier! To complete the exercise, submit your final model to our submission portal - you probably know the procedure by now.\n",
    "\n",
    "1. Go on [our submission page](https://i2dl.vc.in.tum.de/), register for an account and login. We use your matriculation number and send an email with the login details to the mail account associated. When in doubt, login into tum-online and check your mails there. You will get an id which we need in the next step.\n",
    "2. Log into [our submission page](https://i2dl.vc.in.tum.de/) with your account details and upload the zip file.\n",
    "3. Your submission will be evaluated by our system and you will get feedback about the performance of it. You will get an email with your score as well as a message if you have surpassed the threshold.\n",
    "4. Within the working period, you can submit as many solutions as you want to get the best possible score.\n",
    "\n",
    "\n",
    "# 7. Submission Goals\n",
    "\n",
    "- Goal: Successfully implement a fully connected NN image classifier, tune hyperparameters.\n",
    "\n",
    "- Passing Criteria: This time, there are no unit tests that check specific components of your code. The only thing that's required to pass the submission, is your model to reach at least **48% accuracy** on __our__ test dataset. The submission system will show you a number between 0 and 100 which corresponds to your accuracy.\n",
    "\n",
    "- Submission start: __Nov 25, 2021 13:00:00__\n",
    "- Submission deadline : __Dec 01, 2021 15:59:59__ \n",
    "- You can make **$\\infty$** submissions until the deadline. Your __best submission__ will be considered for bonus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
