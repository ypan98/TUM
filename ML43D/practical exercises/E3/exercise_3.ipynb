{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Exercise 3: Shape Reconstruction\n",
    "\n",
    "**Submission Deadline**: 16.06.2022, 23:55\n",
    "\n",
    "We will take a look at two major approaches for 3D shape reconstruction in this last exercise.\n",
    "\n",
    "Like in exercise 2, you can run all trainings either locally or on Google Colab. Just follow the instructions below. \n",
    "\n",
    "Note that training reconstruction methods generally takes relatively long, even for simple shape completion. Training the generalization will take a few hours. *Thus, please make sure to start training well before the submission deadline.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 3.0. Running this notebook\n",
    "We recommend running this notebook on a cuda compatible local gpu. You can also run training on cpu, it will just take longer.\n",
    "\n",
    "We describe two options for executing the training parts of this exercise below: Using Google Colab or running it locally on your machine. If you are not planning on using Colab, just skip forward to Local Execution.\n",
    "\n",
    "### Google Colab\n",
    "\n",
    "If you don't have access to gpu and don't wish to train on CPU, you can use Google Colab. However, we experienced the issue that inline visualization of shapes or inline images didn't work on colab, so just keep that in mind.\n",
    "What you can also do is only train networks on colab, download the checkpoint, and visualize inference locally.\n",
    "\n",
    "In case you're using Google Colab, you can upload the exercise folder (containing `exercise_3.ipynb`, directory `exercise_3` and the file `requirements.txt`) as `3d-machine-learning` to google drive (make sure you don't upload extracted datasets files).\n",
    "Additionally you'd need to open the notebook `exercise_3.ipynb` in Colab using `File > Open Notebook > Upload`.\n",
    "\n",
    "Next you'll need to run these two cells for setting up the environment. Before you do that make sure your instance has a GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive', force_remount=True)\n",
    "#\n",
    "# # We assume you uploaded the exercise folder in root Google Drive folder\n",
    "#\n",
    "# !cp -r /content/drive/MyDrive/3d-machine-learning 3d-machine-learning/\n",
    "# os.chdir('/content/3d-machine-learning/')\n",
    "# print('Installing requirements')\n",
    "# !pip install -r requirements.txt\n",
    "#\n",
    "# # Make sure you restart runtime when directed by Colab\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Run this cell after restarting your colab runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# import sys\n",
    "# import torch\n",
    "# os.chdir('/content/3d-machine-learning/')\n",
    "# sys.path.insert(1, \"/content/3d-machine-learning/\")\n",
    "# print('CUDA availability:', torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Local Execution\n",
    "\n",
    "If you run this notebook locally, you have to first install the python dependiencies again. They are the same as for exercise 1 so you can re-use the environment you used last time. If you use [poetry](https://python-poetry.org), you can also simply re-install everything (`poetry install`) and then run this notebook via `poetry run jupyter notebook`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Imports\n",
    "\n",
    "The following imports should work regardless of whether you are using Colab or local execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import matplotlib as plt\n",
    "import k3d\n",
    "import trimesh\n",
    "import torch\n",
    "import skimage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Use the next cell to test whether a GPU was detected by pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 3.1 Shape Reconstruction from 3D SDF grids with 3D-EPN\n",
    "\n",
    "In the first part of this exercise, we will take a look at shape complation using [3D-EPN](https://arxiv.org/abs/1612.00101). This approach was also introduced in the lecture.\n",
    "\n",
    "The visualization below shows an overview of the method: From an incomplete shape observation (which you would get when scanning an object with a depth sensor for example), we use a 3D encoder-predictor network that first encodes the incomplete shapes into a common latent space using several 3D convolution layers and then decodes them again using multiple 3D transpose convolutions.\n",
    "\n",
    "This way, we get from a 32^3 SDF voxel grid to a 32^3 DF (unsigned) voxel grid that represents the completed shape. We only focus on this part here; in the original implementation, this 32^3 completed prediction would then be further improved (in an offline step after inference) by sampling parts from a shape database to get the final resolution to 128^3.\n",
    "\n",
    "<img src=\"exercise_3/images/3depn_teaser.png\" alt=\"3D-EPN Teaser\" style=\"width: 800px;\"/>\n",
    "\n",
    "The next steps will follow the structure we established in exercise 2: Taking a look at the dataset structure and downloading the data; then, implementing dataset, model, and training loop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### (a) Downloading the data\n",
    "We will use the original dataset used in the official implementation. It consists of SDF and DF grids (representing incomplete input data and complete target data) with a resolution of 32^3 each. Each input-target pair is generated from a ShapeNet shape.\n",
    "\n",
    "The incomplete SDF data are generated by sampling virtual camera trajectories around every object. Each trajectory is assigned an ID which is part of the file names (see below). The camera views for each trajectory are combined into a common SDF grid by volumetric fusion. It is easy to generate an SDF here since we know both camera location and object surface: Everything between camera and surface is known free space and outside the object, leading to a positive SDF sign. Everything behind the surface has a negative sign. For the complete shapes, however, deciding whether a voxel in the DF grid is inside or outside an object is not a trivial problem. This is why we use unsigned distance fields as target and prediction representation instead. This still encodes the distance to the closest surface but does not contain explicit information about the inside/outside location.\n",
    "\n",
    "In terms of dataset layout, we follow the ShapeNet directory structure as seen in the last exercise:\n",
    "Each folder in the `exercise_3/data/shapenet_dim32_sdf` and `exercise_3/data/shapenet_dim32_df` directories contains one shape category represented by a number, e.g. `02691156`.\n",
    "We provide the mapping between these numbers and the corresponding names in `exercise_3/data/shape_info.json`. Each of these shape category folders contains lots of shapes in sdf or df format. In addition to that, every shape now also contains multiple trajectories: 0 to 7, encoded as `__0__` to `__7__`. These 8 files are just different input representations, meaning they vary in the level of completeness and location of missing parts; they all map to the `.df` file with corresponding shape ID and `__0__` at the end.\n",
    "\n",
    "```\n",
    "# contents of exercise_2/data/shapenet_dim32_sdf\n",
    "02691156/                                           # Shape category folder with all its shapes\n",
    "    ├── 10155655850468db78d106ce0a280f87__0__.sdf   # Trajectory 0 for a shape of the category\n",
    "    ├── 10155655850468db78d106ce0a280f87__1__.sdf   # Trajectory 1 for the same shape\n",
    "    ├── :                                      \n",
    "    ├── 10155655850468db78d106ce0a280f87__7__.sdf   # Trajectory 7 for the same shape\n",
    "    ├── 10155655850468db78d106ce0a280f87__0__.sdf   # Trajectory 0 for another shape\n",
    "    ├── :                                           # And so on ...\n",
    "02933112/                                           # Another shape category folder\n",
    "02958343/                                           # In total you should have 8 shape category folders\n",
    ":\n",
    "\n",
    "# contents of exercise_2/data/shapenet_dim32_df\n",
    "02691156/                                           # Shape category folder with all its shapes\n",
    "    ├── 10155655850468db78d106ce0a280f87__0__.df    # A single shape of the category\n",
    "    ├── 1021a0914a7207aff927ed529ad90a11__0__.df    # Another shape of the category\n",
    "    ├── :                                           # And so on ...\n",
    "02933112/                                           # Another shape category folder\n",
    "02958343/                                           # In total you should have 55 shape category folders\n",
    ":\n",
    "```\n",
    "\n",
    "Download and extract the data with the code cell below.\n",
    "\n",
    "**Note**: If you are training on Google Colab and are running out of disk space, you can do the following:\n",
    "- Only download the zip files below without extracting them (comment out all lines after `print('Extracting ...')`)\n",
    "- Change `from exercise_3.data.shapenet import ShapeNet` to `from exercise_3.data.shapenet_zip import ShapeNet`\n",
    "- Implement your dataset in `shapenet_zip.py`. This implementation extracts the data on-the-fly without taking up any additional disk space. Your training will therefore run a bit slower.\n",
    "- Make sure you uncomment the lines setting the worker_init_fn in `train_3depn.py` (marked with TODOs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# print('Downloading ...')\n",
    "# # File sizes: 11GB for shapenet_dim32_sdf.zip (incomplete scans), 4GB for shapenet_dim32_df.zip (target shapes)\n",
    "# !wget http://kaldir.vc.in.tum.de/adai/CNNComplete/shapenet_dim32_sdf.zip -P exercise_3/data\n",
    "# !wget http://kaldir.vc.in.tum.de/adai/CNNComplete/shapenet_dim32_df.zip -P exercise_3/data\n",
    "# print('Extracting ...')\n",
    "# !unzip -q exercise_3/data/shapenet_dim32_sdf.zip -d exercise_3/data\n",
    "# !unzip -q exercise_3/data/shapenet_dim32_df.zip -d exercise_3/data\n",
    "# !rm exercise_3/data/shapenet_dim32_sdf.zip\n",
    "# !rm exercise_3/data/shapenet_dim32_df.zip\n",
    "# print('Done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### (b) Dataset\n",
    "\n",
    "The dataset implementation follows the same general structure as in exercise 2. We prepared an initial implementation already in `exercise_3/data/shapenet.py`; your task is to resolve all TODOs there.\n",
    "\n",
    "The data for SDFs and DFs in `.sdf`/`.df` files are stored in binary form as follows:\n",
    "```\n",
    "dimX    #uint64 \n",
    "dimY    #uint64 \n",
    "dimZ    #uint64 \n",
    "data    #(dimX*dimY*dimZ) floats for sdf/df values\n",
    "```\n",
    "The SDF values stored per-voxel represent the distance to the closest surface *in voxels*.\n",
    "\n",
    "You have to take care of three important steps before returning the SDF and DF for the corresponding `index` in `__getitem__`:\n",
    "1. **Truncation**: 3D-EPN uses a truncated SDF which means that for each voxel, the distance to the closest surface will be clamped to a max absolute value. This is helpful since we do not care about longer distances (Marching Cubes only cares about distances close to the surface). It allows us to focus our predictions on the voxels near the surface. We use a `truncation_distance` of 3 (voxels) which means we expect to get an SDF with values between -3 and 3 as input to the model.\n",
    "2. **Separation** of distances and sign: 3D-EPN uses as input a 2x32x32x32 SDF grid, with absolute distance values of the SDF in channel 0 and the signs (-1 or 1) in channel 1.\n",
    "3. **Log** scaling: We scale targets and prediction with a log operation to further guide predictions to focus on the surface voxels. Therefore, you should return target DFs as `log(df + 1)`.\n",
    "\n",
    "**Hint**: An easy way to load the data from `.sdf` and `.df` files is to use `np.fromfile`. First, load the dimensions, then the data, then reshape everything into the shape you loaded in the beginning. Make sure you get the datatypes and byte offsets right! If you are using the zip version of the dataset as explained above, you should use `np.frombuffer` instead of `np.fromfile` to load from the `data`-buffer. The syntax is identical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of train set: 153540\n",
      "Length of val set: 32304\n",
      "Length of overfit set: 64\n"
     ]
    }
   ],
   "source": [
    "from exercise_3.data.shapenet import ShapeNet\n",
    "\n",
    "# Create a dataset with train split\n",
    "train_dataset = ShapeNet('train')\n",
    "val_dataset = ShapeNet('val')\n",
    "overfit_dataset = ShapeNet('overfit')\n",
    "\n",
    "# Get length, which is a call to __len__ function\n",
    "print(f'Length of train set: {len(train_dataset)}')  # expected output: 153540\n",
    "# Get length, which is a call to __len__ function\n",
    "print(f'Length of val set: {len(val_dataset)}')  # expected output: 32304\n",
    "# Get length, which is a call to __len__ function\n",
    "print(f'Length of overfit set: {len(overfit_dataset)}')  # expected output: 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: 03001627/798a46965d9e0edfcea003eff0268278__3__-03001627/798a46965d9e0edfcea003eff0268278__0__\n",
      "Input SDF: (2, 32, 32, 32)\n",
      "Target DF: (32, 32, 32)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbb4d7bc3fcc4d129ce7fcac4c37e5f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize some shapes\n",
    "from exercise_3.util.visualization import visualize_mesh\n",
    "from skimage.measure import marching_cubes\n",
    "\n",
    "train_sample = train_dataset[1]\n",
    "print(f'Name: {train_sample[\"name\"]}')  # expected output: 03001627/798a46965d9e0edfcea003eff0268278__3__-03001627/798a46965d9e0edfcea003eff0268278__0__\n",
    "print(f'Input SDF: {train_sample[\"input_sdf\"].shape}')  # expected output: (2, 32, 32, 32)\n",
    "print(f'Target DF: {train_sample[\"target_df\"].shape}')  # expected output: (32, 32, 32)\n",
    "\n",
    "input_mesh = marching_cubes(train_sample['input_sdf'][0], level=1)\n",
    "visualize_mesh(input_mesh[0], input_mesh[1], flip_axes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: 04379243/a1be21c9a71d133dc5beea20858a99d5__5__-04379243/a1be21c9a71d133dc5beea20858a99d5__0__\n",
      "Input SDF: (2, 32, 32, 32)\n",
      "Target DF: (32, 32, 32)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37244f9e83ed454d9c97ce988d1c77f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_sample = train_dataset[223]\n",
    "print(f'Name: {train_sample[\"name\"]}')  # expected output: 04379243/a1be21c9a71d133dc5beea20858a99d5__5__-04379243/a1be21c9a71d133dc5beea20858a99d5__0__\n",
    "print(f'Input SDF: {train_sample[\"input_sdf\"].shape}')  # expected output: (2, 32, 32, 32)\n",
    "print(f'Target DF: {train_sample[\"target_df\"].shape}')  # expected output: (32, 32, 32)\n",
    "\n",
    "input_mesh = marching_cubes(train_sample['input_sdf'][0], level=1)\n",
    "visualize_mesh(input_mesh[0], input_mesh[1], flip_axes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: 03636649/3889631e42a84b0f51f77a6d7299806__2__-03636649/3889631e42a84b0f51f77a6d7299806__0__\n",
      "Input SDF: (2, 32, 32, 32)\n",
      "Target DF: (32, 32, 32)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41c4007fb91f41428a3e29ee7693c9c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_sample = train_dataset[95]\n",
    "print(f'Name: {train_sample[\"name\"]}')  # expected output: 03636649/3889631e42a84b0f51f77a6d7299806__2__-03636649/3889631e42a84b0f51f77a6d7299806__0__\n",
    "print(f'Input SDF: {train_sample[\"input_sdf\"].shape}')  # expected output: (2, 32, 32, 32)\n",
    "print(f'Target DF: {train_sample[\"target_df\"].shape}')  # expected output: (32, 32, 32)\n",
    "\n",
    "input_mesh = marching_cubes(train_sample['input_sdf'][0], level=1)\n",
    "visualize_mesh(input_mesh[0], input_mesh[1], flip_axes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### (c) Model\n",
    "\n",
    "The model architecture of 3D-EPN is visualized below:\n",
    "\n",
    "<img src=\"exercise_3/images/3depn.png\" alt=\"3D-EPN Architecture\" style=\"width: 800px;\"/>\n",
    "\n",
    "For this exercise, we simplify the model by omitting the classification part - this will not have a big impact since most of the shape completion performance comes from the 3D encoder-decoder unet.\n",
    "\n",
    "The model consists of three parts: The encoder, the bottleneck, and the decoder. Encoder and decoder are constructed with the same architecture, just mirrored.\n",
    "\n",
    "The details of each part are:\n",
    "- **Encoder**: 4 layers, each one containing a 3D convolution (with kernel size 4, as seen in the visualization), a 3D batch norm (except the very first layer), and a leaky ReLU with a negative slope of 0.2. Our goal is to reduce the spatial dimension from 32x32x32 to 1x1x1 and to get the feature dimension from 2 (absolute values and sign) to `num_features * 8`. We do this by using a stride of 2 and padding of 1 for all convolutions except for the last one where we use a stride of 1 and no padding. The feature channels are increased from 2 to `num_features` in the first layer and then doubled with every subsequent layer.\n",
    "- **Decoder**: Same architecture as encoder, just mirrored: Going from `num_features * 8 * 2` (the 2 will be explained later) to 1 (the DF values). The spatial dimensions go from 1x1x1 to 32x32x32. Each layer use a 3D Transpose convolution now, together with 3D batch norm and ReLU (no leaky ReLUs anymore). Note that the last layer uses neither Batch Norms nor a ReLU since we do not want to constrain the range of possible values for the prediction.\n",
    "- **Bottleneck**: This is realized with 2 fully connected layers, each one going from a vector of size 640 (which is `num_features * 8`) to a vector of size 640. Each such layer is followed by a ReLU activation.\n",
    "\n",
    "Some minor details:\n",
    "- **Skip connections** allow the decoder to use information from the encoder and also improve gradient flow. We use it here to connect the output of encoder layer 1 to decoder layer 4, the output of encoder layer 2 to decoder layer 3, and so on. This means that the input to a decoder layer is the concatenation of the previous decoder output with the corresponding encoder output, along the feature dimension. Hence, the number of input features for each decoder layer are twice those of the encoder layers, as mentioned above.\n",
    "- **Log scaling**: You also need to scale the final outputs of the network logarithmically: `out = log(abs(out) + 1)`. This is the same transformation you applied to the target shapes in the dataloader before and ensures that prediction and target volumes are comparable.\n",
    "\n",
    "With this in mind, implement the network architecture and `forward()` function in `exercise_3/model/threedepn.py`. You can check your architecture with the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   | Name          | Type            | Params  \n",
      "-----------------------------------------------------\n",
      "0  | enconder_l1   | Sequential      | 10320   \n",
      "1  | enconder_l1.0 | Conv3d          | 10320   \n",
      "2  | enconder_l1.1 | LeakyReLU       | 0       \n",
      "3  | enconder_l2   | Sequential      | 819680  \n",
      "4  | enconder_l2.0 | Conv3d          | 819360  \n",
      "5  | enconder_l2.1 | BatchNorm3d     | 320     \n",
      "6  | enconder_l2.2 | LeakyReLU       | 0       \n",
      "7  | enconder_l3   | Sequential      | 3277760 \n",
      "8  | enconder_l3.0 | Conv3d          | 3277120 \n",
      "9  | enconder_l3.1 | BatchNorm3d     | 640     \n",
      "10 | enconder_l3.2 | LeakyReLU       | 0       \n",
      "11 | enconder_l4   | Sequential      | 13109120\n",
      "12 | enconder_l4.0 | Conv3d          | 13107840\n",
      "13 | enconder_l4.1 | BatchNorm3d     | 1280    \n",
      "14 | enconder_l4.2 | LeakyReLU       | 0       \n",
      "15 | bottleneck    | Sequential      | 820480  \n",
      "16 | bottleneck.0  | Linear          | 410240  \n",
      "17 | bottleneck.1  | ReLU            | 0       \n",
      "18 | bottleneck.2  | Linear          | 410240  \n",
      "19 | bottleneck.3  | ReLU            | 0       \n",
      "20 | decoder_l1    | Sequential      | 26215360\n",
      "21 | decoder_l1.0  | ConvTranspose3d | 26214720\n",
      "22 | decoder_l1.1  | BatchNorm3d     | 640     \n",
      "23 | decoder_l1.2  | ReLU            | 0       \n",
      "24 | decoder_l2    | Sequential      | 6554080 \n",
      "25 | decoder_l2.0  | ConvTranspose3d | 6553760 \n",
      "26 | decoder_l2.1  | BatchNorm3d     | 320     \n",
      "27 | decoder_l2.2  | ReLU            | 0       \n",
      "28 | decoder_l3    | Sequential      | 1638640 \n",
      "29 | decoder_l3.0  | ConvTranspose3d | 1638480 \n",
      "30 | decoder_l3.1  | BatchNorm3d     | 160     \n",
      "31 | decoder_l3.2  | ReLU            | 0       \n",
      "32 | decoder_l4    | ConvTranspose3d | 10241   \n",
      "33 | TOTAL         | ThreeDEPN       | 52455681\n",
      "Output tensor shape:  torch.Size([4, 32, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "from exercise_3.model.threedepn import ThreeDEPN\n",
    "from exercise_3.util.model import summarize_model\n",
    "\n",
    "threedepn = ThreeDEPN()\n",
    "print(summarize_model(threedepn))  # Expected: Rows 0-34 and TOTAL = 52455681\n",
    "\n",
    "sdf = torch.randn(4, 1, 32, 32, 32) * 2. - 1.\n",
    "input_tensor = torch.cat([torch.abs(sdf), torch.sign(sdf)], dim=1)\n",
    "predictions = threedepn(input_tensor)\n",
    "\n",
    "print('Output tensor shape: ', predictions.shape)  # Expected: torch.Size([4, 32, 32, 32])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### (d) Training script and overfitting to a single shape reconstruction\n",
    "\n",
    "You can now go to the train script in `exercise_3/training/train_3depn.py` and fill in the missing pieces as you did for exercise 2. Then, verify that your training work by overfitting to a few samples below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n",
      "[004/00001] train_loss: 0.072312\n",
      "[009/00001] train_loss: 0.023063\n",
      "[012/00000] val_loss: 0.393603 | best_loss_val: 0.393603\n",
      "[014/00001] train_loss: 0.014254\n",
      "[019/00001] train_loss: 0.008715\n",
      "[024/00001] train_loss: 0.006135\n",
      "[024/00001] val_loss: 0.203616 | best_loss_val: 0.203616\n",
      "[029/00001] train_loss: 0.005049\n",
      "[034/00001] train_loss: 0.004321\n",
      "[037/00000] val_loss: 0.152098 | best_loss_val: 0.152098\n",
      "[039/00001] train_loss: 0.003761\n",
      "[044/00001] train_loss: 0.003431\n",
      "[049/00001] train_loss: 0.003256\n",
      "[049/00001] val_loss: 0.133632 | best_loss_val: 0.133632\n",
      "[054/00001] train_loss: 0.003050\n",
      "[059/00001] train_loss: 0.002889\n",
      "[062/00000] val_loss: 0.123743 | best_loss_val: 0.123743\n",
      "[064/00001] train_loss: 0.002756\n",
      "[069/00001] train_loss: 0.002715\n",
      "[074/00001] train_loss: 0.002623\n",
      "[074/00001] val_loss: 0.118347 | best_loss_val: 0.118347\n",
      "[079/00001] train_loss: 0.002564\n",
      "[084/00001] train_loss: 0.002485\n",
      "[087/00000] val_loss: 0.114679 | best_loss_val: 0.114679\n",
      "[089/00001] train_loss: 0.002466\n",
      "[094/00001] train_loss: 0.002420\n",
      "[099/00001] train_loss: 0.002384\n",
      "[099/00001] val_loss: 0.112055 | best_loss_val: 0.112055\n",
      "[104/00001] train_loss: 0.002360\n",
      "[109/00001] train_loss: 0.002328\n",
      "[112/00000] val_loss: 0.110764 | best_loss_val: 0.110764\n",
      "[114/00001] train_loss: 0.002340\n",
      "[119/00001] train_loss: 0.002287\n",
      "[124/00001] train_loss: 0.002270\n",
      "[124/00001] val_loss: 0.109636 | best_loss_val: 0.109636\n",
      "[129/00001] train_loss: 0.002258\n",
      "[134/00001] train_loss: 0.002274\n",
      "[137/00000] val_loss: 0.108950 | best_loss_val: 0.108950\n",
      "[139/00001] train_loss: 0.002248\n",
      "[144/00001] train_loss: 0.002239\n",
      "[149/00001] train_loss: 0.002233\n",
      "[149/00001] val_loss: 0.108488 | best_loss_val: 0.108488\n",
      "[154/00001] train_loss: 0.002228\n",
      "[159/00001] train_loss: 0.002219\n",
      "[162/00000] val_loss: 0.108131 | best_loss_val: 0.108131\n",
      "[164/00001] train_loss: 0.002233\n",
      "[169/00001] train_loss: 0.002216\n",
      "[174/00001] train_loss: 0.002215\n",
      "[174/00001] val_loss: 0.107965 | best_loss_val: 0.107965\n",
      "[179/00001] train_loss: 0.002214\n",
      "[184/00001] train_loss: 0.002237\n",
      "[187/00000] val_loss: 0.107841 | best_loss_val: 0.107841\n",
      "[189/00001] train_loss: 0.002210\n",
      "[194/00001] train_loss: 0.002222\n",
      "[199/00001] train_loss: 0.002215\n",
      "[199/00001] val_loss: 0.107700 | best_loss_val: 0.107700\n",
      "[204/00001] train_loss: 0.002203\n",
      "[209/00001] train_loss: 0.002214\n",
      "[212/00000] val_loss: 0.107655 | best_loss_val: 0.107655\n",
      "[214/00001] train_loss: 0.002251\n",
      "[219/00001] train_loss: 0.002213\n",
      "[224/00001] train_loss: 0.002231\n",
      "[224/00001] val_loss: 0.107611 | best_loss_val: 0.107611\n",
      "[229/00001] train_loss: 0.002201\n",
      "[234/00001] train_loss: 0.002219\n",
      "[237/00000] val_loss: 0.107644 | best_loss_val: 0.107611\n",
      "[239/00001] train_loss: 0.002205\n",
      "[244/00001] train_loss: 0.002207\n",
      "[249/00001] train_loss: 0.002195\n",
      "[249/00001] val_loss: 0.107574 | best_loss_val: 0.107574\n"
     ]
    }
   ],
   "source": [
    "from exercise_3.training import train_3depn\n",
    "config = {\n",
    "    'experiment_name': '3_1_3depn_overfitting',\n",
    "    'device': 'cuda:0',  # change this to cpu if you do not have a GPU\n",
    "    'is_overfit': True,\n",
    "    'batch_size': 32,\n",
    "    'resume_ckpt': None,\n",
    "    'learning_rate': 0.001,\n",
    "    'max_epochs': 250,\n",
    "    'print_every_n': 10,\n",
    "    'validate_every_n': 25,\n",
    "}\n",
    "train_3depn.main(config)  # should be able to get <0.0025 train_loss and <0.13 val_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### (e) Training over the entire training set\n",
    "If the overfitting works, we can go ahead with training on the entire dataset.\n",
    "\n",
    "**Note**: As is the case with most reconstruction networks and considering the size of the model (> 50M parameters), this training will take a few hours on a GPU. *Please make sure to start training early enough before the submission deadline.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n",
      "[000/00049] train_loss: 0.036249\n",
      "[000/00099] train_loss: 0.015123\n",
      "[000/00149] train_loss: 0.012674\n",
      "[000/00199] train_loss: 0.010545\n",
      "[000/00249] train_loss: 0.009893\n",
      "[000/00299] train_loss: 0.009115\n",
      "[000/00349] train_loss: 0.008874\n",
      "[000/00399] train_loss: 0.009117\n",
      "[000/00449] train_loss: 0.008481\n",
      "[000/00499] train_loss: 0.008160\n",
      "[000/00549] train_loss: 0.007862\n",
      "[000/00599] train_loss: 0.007277\n",
      "[000/00649] train_loss: 0.007725\n",
      "[000/00699] train_loss: 0.007090\n",
      "[000/00749] train_loss: 0.007384\n",
      "[000/00799] train_loss: 0.007132\n",
      "[000/00849] train_loss: 0.006627\n",
      "[000/00899] train_loss: 0.006512\n",
      "[000/00949] train_loss: 0.006636\n",
      "[000/00999] train_loss: 0.006655\n",
      "[000/00999] val_loss: 0.164279 | best_loss_val: 0.164279\n",
      "[000/01049] train_loss: 0.006853\n",
      "[000/01099] train_loss: 0.006251\n",
      "[000/01149] train_loss: 0.006260\n",
      "[000/01199] train_loss: 0.006170\n",
      "[000/01249] train_loss: 0.006088\n",
      "[000/01299] train_loss: 0.006403\n",
      "[000/01349] train_loss: 0.005972\n",
      "[000/01399] train_loss: 0.006508\n",
      "[000/01449] train_loss: 0.006314\n",
      "[000/01499] train_loss: 0.005730\n",
      "[000/01549] train_loss: 0.006153\n",
      "[000/01599] train_loss: 0.005887\n",
      "[000/01649] train_loss: 0.006153\n",
      "[000/01699] train_loss: 0.005607\n",
      "[000/01749] train_loss: 0.005661\n",
      "[000/01799] train_loss: 0.005481\n",
      "[000/01849] train_loss: 0.005999\n",
      "[000/01899] train_loss: 0.006322\n",
      "[000/01949] train_loss: 0.005524\n",
      "[000/01999] train_loss: 0.005508\n",
      "[000/01999] val_loss: 0.140402 | best_loss_val: 0.140402\n",
      "[000/02049] train_loss: 0.005505\n",
      "[000/02099] train_loss: 0.005560\n",
      "[000/02149] train_loss: 0.005613\n",
      "[000/02199] train_loss: 0.005324\n",
      "[000/02249] train_loss: 0.005240\n",
      "[000/02299] train_loss: 0.005584\n",
      "[000/02349] train_loss: 0.005223\n",
      "[000/02399] train_loss: 0.005362\n",
      "[000/02449] train_loss: 0.005413\n",
      "[000/02499] train_loss: 0.005229\n",
      "[000/02549] train_loss: 0.005138\n",
      "[000/02599] train_loss: 0.005180\n",
      "[000/02649] train_loss: 0.004925\n",
      "[000/02699] train_loss: 0.005386\n",
      "[000/02749] train_loss: 0.005102\n",
      "[000/02799] train_loss: 0.005126\n",
      "[000/02849] train_loss: 0.004956\n",
      "[000/02899] train_loss: 0.005147\n",
      "[000/02949] train_loss: 0.005159\n",
      "[000/02999] train_loss: 0.005252\n",
      "[000/02999] val_loss: 0.178842 | best_loss_val: 0.140402\n",
      "[000/03049] train_loss: 0.004677\n",
      "[000/03099] train_loss: 0.004926\n",
      "[000/03149] train_loss: 0.004840\n",
      "[000/03199] train_loss: 0.005043\n",
      "[000/03249] train_loss: 0.004958\n",
      "[000/03299] train_loss: 0.005282\n",
      "[000/03349] train_loss: 0.004846\n",
      "[000/03399] train_loss: 0.005025\n",
      "[000/03449] train_loss: 0.004896\n",
      "[000/03499] train_loss: 0.004876\n",
      "[000/03549] train_loss: 0.004623\n",
      "[000/03599] train_loss: 0.004715\n",
      "[000/03649] train_loss: 0.005066\n",
      "[000/03699] train_loss: 0.004708\n",
      "[000/03749] train_loss: 0.004781\n",
      "[000/03799] train_loss: 0.004865\n",
      "[000/03849] train_loss: 0.004726\n",
      "[000/03899] train_loss: 0.004614\n",
      "[000/03949] train_loss: 0.004960\n",
      "[000/03999] train_loss: 0.004649\n",
      "[000/03999] val_loss: 0.187539 | best_loss_val: 0.140402\n",
      "[000/04049] train_loss: 0.005006\n",
      "[000/04099] train_loss: 0.004828\n",
      "[000/04149] train_loss: 0.004835\n",
      "[000/04199] train_loss: 0.004920\n",
      "[000/04249] train_loss: 0.004450\n",
      "[000/04299] train_loss: 0.004825\n",
      "[000/04349] train_loss: 0.004748\n",
      "[000/04399] train_loss: 0.004427\n",
      "[000/04449] train_loss: 0.004588\n",
      "[000/04499] train_loss: 0.004747\n",
      "[000/04549] train_loss: 0.004318\n",
      "[000/04599] train_loss: 0.004374\n",
      "[000/04649] train_loss: 0.004814\n",
      "[000/04699] train_loss: 0.004583\n",
      "[000/04749] train_loss: 0.004611\n",
      "[001/00000] train_loss: 0.004492\n",
      "[001/00050] train_loss: 0.004258\n",
      "[001/00100] train_loss: 0.004428\n",
      "[001/00150] train_loss: 0.004590\n",
      "[001/00200] train_loss: 0.004410\n",
      "[001/00200] val_loss: 0.144647 | best_loss_val: 0.140402\n",
      "[001/00250] train_loss: 0.004510\n",
      "[001/00300] train_loss: 0.004131\n",
      "[001/00350] train_loss: 0.004266\n",
      "[001/00400] train_loss: 0.004190\n",
      "[001/00450] train_loss: 0.004343\n",
      "[001/00500] train_loss: 0.004738\n",
      "[001/00550] train_loss: 0.004313\n",
      "[001/00600] train_loss: 0.004494\n",
      "[001/00650] train_loss: 0.004520\n",
      "[001/00700] train_loss: 0.004592\n",
      "[001/00750] train_loss: 0.004265\n",
      "[001/00800] train_loss: 0.004153\n",
      "[001/00850] train_loss: 0.004309\n",
      "[001/00900] train_loss: 0.004325\n",
      "[001/00950] train_loss: 0.004209\n",
      "[001/01000] train_loss: 0.004326\n",
      "[001/01050] train_loss: 0.004187\n",
      "[001/01100] train_loss: 0.004194\n",
      "[001/01150] train_loss: 0.004456\n",
      "[001/01200] train_loss: 0.004320\n",
      "[001/01200] val_loss: 0.157689 | best_loss_val: 0.140402\n",
      "[001/01250] train_loss: 0.004008\n",
      "[001/01300] train_loss: 0.004427\n",
      "[001/01350] train_loss: 0.004161\n",
      "[001/01400] train_loss: 0.003847\n",
      "[001/01450] train_loss: 0.004378\n",
      "[001/01500] train_loss: 0.004297\n",
      "[001/01550] train_loss: 0.004398\n",
      "[001/01600] train_loss: 0.004313\n",
      "[001/01650] train_loss: 0.004180\n",
      "[001/01700] train_loss: 0.003807\n",
      "[001/01750] train_loss: 0.004241\n",
      "[001/01800] train_loss: 0.004419\n",
      "[001/01850] train_loss: 0.004311\n",
      "[001/01900] train_loss: 0.004122\n",
      "[001/01950] train_loss: 0.004684\n",
      "[001/02000] train_loss: 0.004103\n",
      "[001/02050] train_loss: 0.004049\n",
      "[001/02100] train_loss: 0.004022\n",
      "[001/02150] train_loss: 0.004071\n",
      "[001/02200] train_loss: 0.003989\n",
      "[001/02200] val_loss: 0.142496 | best_loss_val: 0.140402\n",
      "[001/02250] train_loss: 0.004381\n",
      "[001/02300] train_loss: 0.004104\n",
      "[001/02350] train_loss: 0.004311\n",
      "[001/02400] train_loss: 0.004424\n",
      "[001/02450] train_loss: 0.003875\n",
      "[001/02500] train_loss: 0.004152\n",
      "[001/02550] train_loss: 0.004233\n",
      "[001/02600] train_loss: 0.004176\n",
      "[001/02650] train_loss: 0.004341\n",
      "[001/02700] train_loss: 0.004372\n",
      "[001/02750] train_loss: 0.004373\n",
      "[001/02800] train_loss: 0.004007\n",
      "[001/02850] train_loss: 0.003933\n",
      "[001/02900] train_loss: 0.003881\n",
      "[001/02950] train_loss: 0.004006\n",
      "[001/03000] train_loss: 0.004040\n",
      "[001/03050] train_loss: 0.003993\n",
      "[001/03100] train_loss: 0.004135\n",
      "[001/03150] train_loss: 0.004103\n",
      "[001/03200] train_loss: 0.004232\n",
      "[001/03200] val_loss: 0.150348 | best_loss_val: 0.140402\n",
      "[001/03250] train_loss: 0.003833\n",
      "[001/03300] train_loss: 0.004076\n",
      "[001/03350] train_loss: 0.004106\n",
      "[001/03400] train_loss: 0.003596\n",
      "[001/03450] train_loss: 0.003995\n",
      "[001/03500] train_loss: 0.003859\n",
      "[001/03550] train_loss: 0.004130\n",
      "[001/03600] train_loss: 0.003978\n",
      "[001/03650] train_loss: 0.003613\n",
      "[001/03700] train_loss: 0.004147\n",
      "[001/03750] train_loss: 0.004064\n",
      "[001/03800] train_loss: 0.003806\n",
      "[001/03850] train_loss: 0.004017\n",
      "[001/03900] train_loss: 0.003858\n",
      "[001/03950] train_loss: 0.004012\n",
      "[001/04000] train_loss: 0.003985\n",
      "[001/04050] train_loss: 0.004131\n",
      "[001/04100] train_loss: 0.004117\n",
      "[001/04150] train_loss: 0.003984\n",
      "[001/04200] train_loss: 0.003891\n",
      "[001/04200] val_loss: 0.106865 | best_loss_val: 0.106865\n",
      "[001/04250] train_loss: 0.003829\n",
      "[001/04300] train_loss: 0.003862\n",
      "[001/04350] train_loss: 0.003924\n",
      "[001/04400] train_loss: 0.003833\n",
      "[001/04450] train_loss: 0.003889\n",
      "[001/04500] train_loss: 0.003736\n",
      "[001/04550] train_loss: 0.003632\n",
      "[001/04600] train_loss: 0.003650\n",
      "[001/04650] train_loss: 0.003964\n",
      "[001/04700] train_loss: 0.003676\n",
      "[001/04750] train_loss: 0.003814\n",
      "[002/00001] train_loss: 0.003712\n",
      "[002/00051] train_loss: 0.003803\n",
      "[002/00101] train_loss: 0.003721\n",
      "[002/00151] train_loss: 0.003524\n",
      "[002/00201] train_loss: 0.003766\n",
      "[002/00251] train_loss: 0.003642\n",
      "[002/00301] train_loss: 0.003682\n",
      "[002/00351] train_loss: 0.003701\n",
      "[002/00401] train_loss: 0.003594\n",
      "[002/00401] val_loss: 0.159855 | best_loss_val: 0.106865\n",
      "[002/00451] train_loss: 0.003450\n",
      "[002/00501] train_loss: 0.003671\n",
      "[002/00551] train_loss: 0.003704\n",
      "[002/00601] train_loss: 0.003633\n",
      "[002/00651] train_loss: 0.003861\n",
      "[002/00701] train_loss: 0.003621\n",
      "[002/00751] train_loss: 0.003615\n",
      "[002/00801] train_loss: 0.003777\n",
      "[002/00851] train_loss: 0.003575\n",
      "[002/00901] train_loss: 0.003671\n",
      "[002/00951] train_loss: 0.003652\n",
      "[002/01001] train_loss: 0.003815\n",
      "[002/01051] train_loss: 0.003740\n",
      "[002/01101] train_loss: 0.004049\n",
      "[002/01151] train_loss: 0.003522\n",
      "[002/01201] train_loss: 0.003509\n",
      "[002/01251] train_loss: 0.003437\n",
      "[002/01301] train_loss: 0.003349\n",
      "[002/01351] train_loss: 0.003544\n",
      "[002/01401] train_loss: 0.003809\n",
      "[002/01401] val_loss: 0.412677 | best_loss_val: 0.106865\n",
      "[002/01451] train_loss: 0.003552\n",
      "[002/01501] train_loss: 0.003745\n",
      "[002/01551] train_loss: 0.003892\n",
      "[002/01601] train_loss: 0.003559\n",
      "[002/01651] train_loss: 0.003640\n",
      "[002/01701] train_loss: 0.003662\n",
      "[002/01751] train_loss: 0.003568\n",
      "[002/01801] train_loss: 0.003738\n",
      "[002/01851] train_loss: 0.003827\n",
      "[002/01901] train_loss: 0.003561\n",
      "[002/01951] train_loss: 0.003567\n",
      "[002/02001] train_loss: 0.003485\n",
      "[002/02051] train_loss: 0.003558\n",
      "[002/02101] train_loss: 0.003531\n",
      "[002/02151] train_loss: 0.003779\n",
      "[002/02201] train_loss: 0.003624\n",
      "[002/02251] train_loss: 0.003794\n",
      "[002/02301] train_loss: 0.003683\n",
      "[002/02351] train_loss: 0.003358\n",
      "[002/02401] train_loss: 0.003499\n",
      "[002/02401] val_loss: 0.395587 | best_loss_val: 0.106865\n",
      "[002/02451] train_loss: 0.003789\n",
      "[002/02501] train_loss: 0.003429\n",
      "[002/02551] train_loss: 0.003721\n",
      "[002/02601] train_loss: 0.003499\n",
      "[002/02651] train_loss: 0.003787\n",
      "[002/02701] train_loss: 0.003710\n",
      "[002/02751] train_loss: 0.003525\n",
      "[002/02801] train_loss: 0.003465\n",
      "[002/02851] train_loss: 0.003392\n",
      "[002/02901] train_loss: 0.003551\n",
      "[002/02951] train_loss: 0.003508\n",
      "[002/03001] train_loss: 0.003541\n",
      "[002/03051] train_loss: 0.003418\n",
      "[002/03101] train_loss: 0.003447\n",
      "[002/03151] train_loss: 0.003428\n",
      "[002/03201] train_loss: 0.003611\n",
      "[002/03251] train_loss: 0.003501\n",
      "[002/03301] train_loss: 0.003588\n",
      "[002/03351] train_loss: 0.003595\n",
      "[002/03401] train_loss: 0.003375\n",
      "[002/03401] val_loss: 0.494164 | best_loss_val: 0.106865\n",
      "[002/03451] train_loss: 0.003622\n",
      "[002/03501] train_loss: 0.003570\n",
      "[002/03551] train_loss: 0.003600\n",
      "[002/03601] train_loss: 0.003350\n",
      "[002/03651] train_loss: 0.003506\n",
      "[002/03701] train_loss: 0.003384\n",
      "[002/03751] train_loss: 0.003586\n",
      "[002/03801] train_loss: 0.003481\n",
      "[002/03851] train_loss: 0.003722\n",
      "[002/03901] train_loss: 0.003590\n",
      "[002/03951] train_loss: 0.003535\n",
      "[002/04001] train_loss: 0.003659\n",
      "[002/04051] train_loss: 0.003444\n",
      "[002/04101] train_loss: 0.003389\n",
      "[002/04151] train_loss: 0.003478\n",
      "[002/04201] train_loss: 0.003430\n",
      "[002/04251] train_loss: 0.003433\n",
      "[002/04301] train_loss: 0.003366\n",
      "[002/04351] train_loss: 0.003454\n",
      "[002/04401] train_loss: 0.003429\n",
      "[002/04401] val_loss: 0.154116 | best_loss_val: 0.106865\n",
      "[002/04451] train_loss: 0.003356\n",
      "[002/04501] train_loss: 0.003511\n",
      "[002/04551] train_loss: 0.003402\n",
      "[002/04601] train_loss: 0.003409\n",
      "[002/04651] train_loss: 0.003378\n",
      "[002/04701] train_loss: 0.003288\n",
      "[002/04751] train_loss: 0.003427\n",
      "[003/00002] train_loss: 0.003546\n",
      "[003/00052] train_loss: 0.003347\n",
      "[003/00102] train_loss: 0.003344\n",
      "[003/00152] train_loss: 0.003345\n",
      "[003/00202] train_loss: 0.003475\n",
      "[003/00252] train_loss: 0.003340\n",
      "[003/00302] train_loss: 0.003263\n",
      "[003/00352] train_loss: 0.003278\n",
      "[003/00402] train_loss: 0.003193\n",
      "[003/00452] train_loss: 0.003348\n",
      "[003/00502] train_loss: 0.003281\n",
      "[003/00552] train_loss: 0.003207\n",
      "[003/00602] train_loss: 0.003181\n",
      "[003/00602] val_loss: 0.105051 | best_loss_val: 0.105051\n",
      "[003/00652] train_loss: 0.003069\n",
      "[003/00702] train_loss: 0.003329\n",
      "[003/00752] train_loss: 0.003205\n",
      "[003/00802] train_loss: 0.003271\n",
      "[003/00852] train_loss: 0.003330\n",
      "[003/00902] train_loss: 0.003238\n",
      "[003/00952] train_loss: 0.003281\n",
      "[003/01002] train_loss: 0.003276\n",
      "[003/01052] train_loss: 0.002985\n",
      "[003/01102] train_loss: 0.003142\n",
      "[003/01152] train_loss: 0.003100\n",
      "[003/01202] train_loss: 0.003151\n",
      "[003/01252] train_loss: 0.003394\n",
      "[003/01302] train_loss: 0.003162\n",
      "[003/01352] train_loss: 0.003303\n",
      "[003/01402] train_loss: 0.003331\n",
      "[003/01452] train_loss: 0.003236\n",
      "[003/01502] train_loss: 0.003195\n",
      "[003/01552] train_loss: 0.003459\n",
      "[003/01602] train_loss: 0.003207\n",
      "[003/01602] val_loss: 0.128792 | best_loss_val: 0.105051\n",
      "[003/01652] train_loss: 0.003375\n",
      "[003/01702] train_loss: 0.003200\n",
      "[003/01752] train_loss: 0.003069\n",
      "[003/01802] train_loss: 0.003229\n",
      "[003/01852] train_loss: 0.003290\n",
      "[003/01902] train_loss: 0.003188\n",
      "[003/01952] train_loss: 0.003364\n",
      "[003/02002] train_loss: 0.003111\n",
      "[003/02052] train_loss: 0.003222\n",
      "[003/02102] train_loss: 0.003220\n",
      "[003/02152] train_loss: 0.003289\n",
      "[003/02202] train_loss: 0.003062\n",
      "[003/02252] train_loss: 0.003115\n",
      "[003/02302] train_loss: 0.003317\n",
      "[003/02352] train_loss: 0.003347\n",
      "[003/02402] train_loss: 0.003370\n",
      "[003/02452] train_loss: 0.003125\n",
      "[003/02502] train_loss: 0.003291\n",
      "[003/02552] train_loss: 0.003231\n",
      "[003/02602] train_loss: 0.003219\n",
      "[003/02602] val_loss: 0.327584 | best_loss_val: 0.105051\n",
      "[003/02652] train_loss: 0.003051\n",
      "[003/02702] train_loss: 0.003386\n",
      "[003/02752] train_loss: 0.003222\n",
      "[003/02802] train_loss: 0.002957\n",
      "[003/02852] train_loss: 0.003333\n",
      "[003/02902] train_loss: 0.003200\n",
      "[003/02952] train_loss: 0.003008\n",
      "[003/03002] train_loss: 0.003251\n",
      "[003/03052] train_loss: 0.003183\n",
      "[003/03102] train_loss: 0.003209\n",
      "[003/03152] train_loss: 0.003273\n",
      "[003/03202] train_loss: 0.003364\n",
      "[003/03252] train_loss: 0.003323\n",
      "[003/03302] train_loss: 0.003441\n",
      "[003/03352] train_loss: 0.003211\n",
      "[003/03402] train_loss: 0.003076\n",
      "[003/03452] train_loss: 0.003188\n",
      "[003/03502] train_loss: 0.003110\n",
      "[003/03552] train_loss: 0.003197\n",
      "[003/03602] train_loss: 0.003297\n",
      "[003/03602] val_loss: 0.109236 | best_loss_val: 0.105051\n",
      "[003/03652] train_loss: 0.003250\n",
      "[003/03702] train_loss: 0.003238\n",
      "[003/03752] train_loss: 0.003080\n",
      "[003/03802] train_loss: 0.002988\n",
      "[003/03852] train_loss: 0.003060\n",
      "[003/03902] train_loss: 0.003107\n",
      "[003/03952] train_loss: 0.003250\n",
      "[003/04002] train_loss: 0.003061\n",
      "[003/04052] train_loss: 0.003313\n",
      "[003/04102] train_loss: 0.003301\n",
      "[003/04152] train_loss: 0.003091\n",
      "[003/04202] train_loss: 0.003068\n",
      "[003/04252] train_loss: 0.003329\n",
      "[003/04302] train_loss: 0.003010\n",
      "[003/04352] train_loss: 0.003050\n",
      "[003/04402] train_loss: 0.003212\n",
      "[003/04452] train_loss: 0.003107\n",
      "[003/04502] train_loss: 0.003141\n",
      "[003/04552] train_loss: 0.003154\n",
      "[003/04602] train_loss: 0.003201\n",
      "[003/04602] val_loss: 0.105209 | best_loss_val: 0.105051\n",
      "[003/04652] train_loss: 0.003006\n",
      "[003/04702] train_loss: 0.003277\n",
      "[003/04752] train_loss: 0.003310\n",
      "[004/00003] train_loss: 0.003112\n",
      "[004/00053] train_loss: 0.003030\n",
      "[004/00103] train_loss: 0.002934\n",
      "[004/00153] train_loss: 0.003121\n",
      "[004/00203] train_loss: 0.002764\n",
      "[004/00253] train_loss: 0.002901\n",
      "[004/00303] train_loss: 0.002985\n",
      "[004/00353] train_loss: 0.002970\n",
      "[004/00403] train_loss: 0.002981\n",
      "[004/00453] train_loss: 0.002921\n",
      "[004/00503] train_loss: 0.003054\n",
      "[004/00553] train_loss: 0.002856\n",
      "[004/00603] train_loss: 0.003029\n",
      "[004/00653] train_loss: 0.002885\n",
      "[004/00703] train_loss: 0.002974\n",
      "[004/00753] train_loss: 0.002881\n",
      "[004/00803] train_loss: 0.002837\n",
      "[004/00803] val_loss: 0.096275 | best_loss_val: 0.096275\n",
      "[004/00853] train_loss: 0.002763\n",
      "[004/00903] train_loss: 0.002852\n",
      "[004/00953] train_loss: 0.002925\n",
      "[004/01003] train_loss: 0.002789\n",
      "[004/01053] train_loss: 0.002994\n",
      "[004/01103] train_loss: 0.002927\n",
      "[004/01153] train_loss: 0.002800\n",
      "[004/01203] train_loss: 0.002757\n",
      "[004/01253] train_loss: 0.002955\n",
      "[004/01303] train_loss: 0.002954\n",
      "[004/01353] train_loss: 0.003110\n",
      "[004/01403] train_loss: 0.002785\n",
      "[004/01453] train_loss: 0.002884\n",
      "[004/01503] train_loss: 0.003003\n",
      "[004/01553] train_loss: 0.002897\n",
      "[004/01603] train_loss: 0.002829\n",
      "[004/01653] train_loss: 0.003022\n",
      "[004/01703] train_loss: 0.002927\n",
      "[004/01753] train_loss: 0.002862\n",
      "[004/01803] train_loss: 0.002828\n",
      "[004/01803] val_loss: 0.151127 | best_loss_val: 0.096275\n",
      "[004/01853] train_loss: 0.003039\n",
      "[004/01903] train_loss: 0.003096\n",
      "[004/01953] train_loss: 0.002888\n",
      "[004/02003] train_loss: 0.003122\n",
      "[004/02053] train_loss: 0.002855\n",
      "[004/02103] train_loss: 0.003099\n",
      "[004/02153] train_loss: 0.002887\n",
      "[004/02203] train_loss: 0.003099\n",
      "[004/02253] train_loss: 0.002993\n",
      "[004/02303] train_loss: 0.002977\n",
      "[004/02353] train_loss: 0.003049\n",
      "[004/02403] train_loss: 0.002874\n",
      "[004/02453] train_loss: 0.002937\n",
      "[004/02503] train_loss: 0.002655\n",
      "[004/02553] train_loss: 0.003139\n",
      "[004/02603] train_loss: 0.003023\n",
      "[004/02653] train_loss: 0.002811\n",
      "[004/02703] train_loss: 0.003035\n",
      "[004/02753] train_loss: 0.002912\n",
      "[004/02803] train_loss: 0.002697\n",
      "[004/02803] val_loss: 0.086942 | best_loss_val: 0.086942\n",
      "[004/02853] train_loss: 0.002943\n",
      "[004/02903] train_loss: 0.002823\n",
      "[004/02953] train_loss: 0.002955\n",
      "[004/03003] train_loss: 0.002925\n",
      "[004/03053] train_loss: 0.002957\n",
      "[004/03103] train_loss: 0.002828\n",
      "[004/03153] train_loss: 0.003041\n",
      "[004/03203] train_loss: 0.002765\n",
      "[004/03253] train_loss: 0.003011\n",
      "[004/03303] train_loss: 0.003206\n",
      "[004/03353] train_loss: 0.002937\n",
      "[004/03403] train_loss: 0.003091\n",
      "[004/03453] train_loss: 0.002916\n",
      "[004/03503] train_loss: 0.003071\n",
      "[004/03553] train_loss: 0.002926\n",
      "[004/03603] train_loss: 0.002893\n",
      "[004/03653] train_loss: 0.002955\n",
      "[004/03703] train_loss: 0.002866\n",
      "[004/03753] train_loss: 0.002974\n",
      "[004/03803] train_loss: 0.002898\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[1;32mIn [13]\u001B[0m, in \u001B[0;36m<cell line: 12>\u001B[1;34m()\u001B[0m\n\u001B[0;32m      1\u001B[0m config \u001B[38;5;241m=\u001B[39m {\n\u001B[0;32m      2\u001B[0m     \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mexperiment_name\u001B[39m\u001B[38;5;124m'\u001B[39m: \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m3_1_3depn_generalization\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[0;32m      3\u001B[0m     \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdevice\u001B[39m\u001B[38;5;124m'\u001B[39m: \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcuda:0\u001B[39m\u001B[38;5;124m'\u001B[39m,  \u001B[38;5;66;03m# change this to cpu if you do not have a GPU\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     10\u001B[0m     \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mvalidate_every_n\u001B[39m\u001B[38;5;124m'\u001B[39m: \u001B[38;5;241m1000\u001B[39m,\n\u001B[0;32m     11\u001B[0m }\n\u001B[1;32m---> 12\u001B[0m \u001B[43mtrain_3depn\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\TUM\\ML3D\\E3\\exercise_3\\training\\train_3depn.py:153\u001B[0m, in \u001B[0;36mmain\u001B[1;34m(config)\u001B[0m\n\u001B[0;32m    150\u001B[0m Path(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mexercise_3/runs/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mconfig[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mexperiment_name\u001B[39m\u001B[38;5;124m\"\u001B[39m]\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\u001B[38;5;241m.\u001B[39mmkdir(exist_ok\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, parents\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m    152\u001B[0m \u001B[38;5;66;03m# Start training\u001B[39;00m\n\u001B[1;32m--> 153\u001B[0m \u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_dataloader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mval_dataloader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\TUM\\ML3D\\E3\\exercise_3\\training\\train_3depn.py:65\u001B[0m, in \u001B[0;36mtrain\u001B[1;34m(model, train_dataloader, val_dataloader, device, config)\u001B[0m\n\u001B[0;32m     63\u001B[0m \u001B[38;5;66;03m# Evaluation on entire validation set\u001B[39;00m\n\u001B[0;32m     64\u001B[0m loss_val \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0.\u001B[39m\n\u001B[1;32m---> 65\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m batch_val \u001B[38;5;129;01min\u001B[39;00m val_dataloader:\n\u001B[0;32m     66\u001B[0m     ShapeNet\u001B[38;5;241m.\u001B[39mmove_batch_to_device(batch_val, device)\n\u001B[0;32m     68\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mno_grad():\n",
      "File \u001B[1;32mD:\\Program Files\\Conda\\envs\\mlgs\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:530\u001B[0m, in \u001B[0;36m_BaseDataLoaderIter.__next__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    528\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    529\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reset()\n\u001B[1;32m--> 530\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_next_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    531\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m    532\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_kind \u001B[38;5;241m==\u001B[39m _DatasetKind\u001B[38;5;241m.\u001B[39mIterable \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[0;32m    533\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[0;32m    534\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called:\n",
      "File \u001B[1;32mD:\\Program Files\\Conda\\envs\\mlgs\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:570\u001B[0m, in \u001B[0;36m_SingleProcessDataLoaderIter._next_data\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    568\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_next_data\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    569\u001B[0m     index \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_next_index()  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[1;32m--> 570\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_dataset_fetcher\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfetch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mindex\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[0;32m    571\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory:\n\u001B[0;32m    572\u001B[0m         data \u001B[38;5;241m=\u001B[39m _utils\u001B[38;5;241m.\u001B[39mpin_memory\u001B[38;5;241m.\u001B[39mpin_memory(data)\n",
      "File \u001B[1;32mD:\\Program Files\\Conda\\envs\\mlgs\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:49\u001B[0m, in \u001B[0;36m_MapDatasetFetcher.fetch\u001B[1;34m(self, possibly_batched_index)\u001B[0m\n\u001B[0;32m     47\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mfetch\u001B[39m(\u001B[38;5;28mself\u001B[39m, possibly_batched_index):\n\u001B[0;32m     48\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mauto_collation:\n\u001B[1;32m---> 49\u001B[0m         data \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[idx] \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m possibly_batched_index]\n\u001B[0;32m     50\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m     51\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[possibly_batched_index]\n",
      "File \u001B[1;32mD:\\Program Files\\Conda\\envs\\mlgs\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:49\u001B[0m, in \u001B[0;36m<listcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m     47\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mfetch\u001B[39m(\u001B[38;5;28mself\u001B[39m, possibly_batched_index):\n\u001B[0;32m     48\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mauto_collation:\n\u001B[1;32m---> 49\u001B[0m         data \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdataset\u001B[49m\u001B[43m[\u001B[49m\u001B[43midx\u001B[49m\u001B[43m]\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m possibly_batched_index]\n\u001B[0;32m     50\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m     51\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[possibly_batched_index]\n",
      "File \u001B[1;32mD:\\TUM\\ML3D\\E3\\exercise_3\\data\\shapenet.py:25\u001B[0m, in \u001B[0;36mShapeNet.__getitem__\u001B[1;34m(self, index)\u001B[0m\n\u001B[0;32m     22\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__getitem__\u001B[39m(\u001B[38;5;28mself\u001B[39m, index):\n\u001B[0;32m     23\u001B[0m     sdf_id, df_id \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mitems[index]\u001B[38;5;241m.\u001B[39msplit(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m---> 25\u001B[0m     input_sdf \u001B[38;5;241m=\u001B[39m \u001B[43mShapeNet\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_shape_sdf\u001B[49m\u001B[43m(\u001B[49m\u001B[43msdf_id\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     26\u001B[0m     target_df \u001B[38;5;241m=\u001B[39m ShapeNet\u001B[38;5;241m.\u001B[39mget_shape_df(df_id)\n\u001B[0;32m     28\u001B[0m     \u001B[38;5;66;03m# TODO Apply truncation to sdf and df\u001B[39;00m\n",
      "File \u001B[1;32mD:\\TUM\\ML3D\\E3\\exercise_3\\data\\shapenet.py:54\u001B[0m, in \u001B[0;36mShapeNet.get_shape_sdf\u001B[1;34m(shapenet_id)\u001B[0m\n\u001B[0;32m     52\u001B[0m sdf \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m     53\u001B[0m \u001B[38;5;66;03m# TODO implement sdf data loading\u001B[39;00m\n\u001B[1;32m---> 54\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mopen\u001B[39m(ShapeNet\u001B[38;5;241m.\u001B[39mdataset_sdf_path \u001B[38;5;241m/\u001B[39m Path(\u001B[38;5;28mstr\u001B[39m(shapenet_id) \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.sdf\u001B[39m\u001B[38;5;124m\"\u001B[39m), \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mrb\u001B[39m\u001B[38;5;124m'\u001B[39m) \u001B[38;5;28;01mas\u001B[39;00m f:\n\u001B[0;32m     55\u001B[0m     shapes \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mfromfile(f, count\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m3\u001B[39m, dtype\u001B[38;5;241m=\u001B[39mnp\u001B[38;5;241m.\u001B[39muint64)\n\u001B[0;32m     56\u001B[0m     data \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mfromfile(f, dtype\u001B[38;5;241m=\u001B[39mnp\u001B[38;5;241m.\u001B[39mfloat32)\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "config = {\n",
    "    'experiment_name': '3_1_3depn_generalization',\n",
    "    'device': 'cuda:0',  # change this to cpu if you do not have a GPU\n",
    "    'is_overfit': False,\n",
    "    'batch_size': 32,\n",
    "    'resume_ckpt': None,\n",
    "    'learning_rate': 0.001,\n",
    "    'max_epochs': 5,\n",
    "    'print_every_n': 50,\n",
    "    'validate_every_n': 1000,\n",
    "}\n",
    "train_3depn.main(config)  # should be able to get best_loss_val < 0.1 after a few hours and 5 epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### (f) Inference\n",
    "\n",
    "Implement the missing bits in `exercise_3/inference/infer_3depn.py`. You should then be able to see your reconstructions below.\n",
    "\n",
    "The outputs of our provided visualization functions are, from left to right:\n",
    "- Input, partial shape\n",
    "- Predicted completion\n",
    "- Target shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from exercise_3.util.visualization import visualize_meshes\n",
    "from exercise_3.inference.infer_3depn import InferenceHandler3DEPN\n",
    "\n",
    "# create a handler for inference using a trained checkpoint\n",
    "inferer = InferenceHandler3DEPN('exercise_3/runs/3_1_3depn_generalization/model_best.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03b3f4db5c1d4a26880754652fe63890",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "input_sdf = ShapeNet.get_shape_sdf('03636649/b286c9c136784db2af1744fdb1fbe7df__0__')\n",
    "target_df = ShapeNet.get_shape_df('03636649/b286c9c136784db2af1744fdb1fbe7df__0__')\n",
    "\n",
    "input_mesh, reconstructed_mesh, target_mesh = inferer.infer_single(input_sdf, target_df)\n",
    "visualize_meshes([input_mesh, reconstructed_mesh, target_mesh], flip_axes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b32b8b863d346e3a3c2529d14bbebe0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "input_sdf = ShapeNet.get_shape_sdf('03636649/23eaba9bdd51a5b0dfe9cab879fd37e8__1__')\n",
    "target_df = ShapeNet.get_shape_df('03636649/23eaba9bdd51a5b0dfe9cab879fd37e8__0__')\n",
    "\n",
    "input_mesh, reconstructed_mesh, target_mesh = inferer.infer_single(input_sdf, target_df)\n",
    "visualize_meshes([input_mesh, reconstructed_mesh, target_mesh], flip_axes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3cbaa33c36945db9ff013638e188bbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "input_sdf = ShapeNet.get_shape_sdf('02691156/5de2cc606b65b960e0b6546e08902f28__0__')\n",
    "target_df = ShapeNet.get_shape_df('02691156/5de2cc606b65b960e0b6546e08902f28__0__')\n",
    "\n",
    "input_mesh, reconstructed_mesh, target_mesh = inferer.infer_single(input_sdf, target_df)\n",
    "visualize_meshes([input_mesh, reconstructed_mesh, target_mesh], flip_axes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 3.2 DeepSDF\n",
    "\n",
    "\n",
    "Here, we will take a look at 3D-reconstruction using [DeepSDF](https://arxiv.org/abs/1901.05103). We recommend reading the paper before attempting the exercise.\n",
    "\n",
    "DeepSDF is an auto-decoder based approach that learns a continuous SDF representation for a class of shapes. Once trained, it can be used for shape representation, interpolation and shape completion. We'll look at each of these\n",
    "applications.\n",
    "\n",
    "<img src=\"exercise_3/images/deepsdf_teaser.png\" alt=\"deepsdf_teaser\" style=\"width: 800px;\"/>\n",
    "\n",
    "During training, the autodecoder optimizes both the network parameters and the latent codes representing each of the training shapes. Once trained, to reconstruct a shape given its SDF observations, a latent code is\n",
    "optimized keeping the network parameters fixed, such that the optimized latent code gives the lowest error with observed SDF values.\n",
    "\n",
    "An advantage that implicit representations have over voxel/grid based approaches is that they are not tied to a particular grid resolution, and can be evaluated at any resolution once trained.\n",
    "\n",
    "Similar to previous exercise, we'll first download the processed dataset, look at the implementation of the dataset, the model and the trainer, try out overfitting and generalization over the entire dataset, and finally inference on unseen samples.\n",
    "\n",
    "### (a) Downloading the data\n",
    "\n",
    "Whereas volumetric models output entire 3d shape representations, implicit models like DeepSDF work on per point basis. The network takes in a 3D-coordinate (and additionally the latent vector) and outputs the SDF value at the queried point. To train such a model,\n",
    "we therefore need, for each of the training shapes, a bunch of points with their corresponding SDF values for supervision. Points are sampled more aggressively near the surface of the object as we want to capture a more detailed SDF near the surface. For those curious,\n",
    "data preparation is decribed in more detail in section 5 of the paper.\n",
    "\n",
    "We'll be using the ShapeNet Sofa class for the experiments in this exercise. We've already prepared this data, so that you don't need to deal with the preprocessing. For each shape, the following files are provided:\n",
    "- `mesh.obj` representing the mesh representation of the shape\n",
    "- `sdf.npz` file containing large number of points sampled on and around the mesh and their sdf values; contains numpy arrays under keys \"pos\" and \"neg\", containing points with positive and negative sdf values respectively\n",
    "\n",
    "```\n",
    "# contents of exercise_3/data/sdf_sofas\n",
    "1faa4c299b93a3e5593ebeeedbff73b/                    # shape 0\n",
    "    ├── mesh.obj                                    # shape 0 mesh\n",
    "    ├── sdf.npz                                     # shape 0 sdf\n",
    "    ├── surface.obj                                 # shape 0 surface\n",
    "1fde48d83065ef5877a929f61fea4d0/                    # shape 1\n",
    "1fe1411b6c8097acf008d8a3590fb522/                   # shape 2\n",
    ":\n",
    "```\n",
    "Download and extract the data with the code cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# print('Downloading ...')\n",
    "# # File sizes: ~10GB\n",
    "# !wget https://www.dropbox.com/s/4k5pw126nzus8ef/sdf_sofas.zip\\?dl\\=0 -O exercise_3/data/sdf_sofas.zip -P exercise_3/data\n",
    "#\n",
    "# print('Extracting ...')\n",
    "# !unzip -q exercise_3/data/sdf_sofas.zip -d exercise_3/data\n",
    "# !rm exercise_3/data/sdf_sofas.zip\n",
    "#\n",
    "# print('Done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### (b) Dataset\n",
    "\n",
    "We provide a partial implementation of the dataset in `exercise_3/data/shape_implicit.py`.\n",
    "Your task is to complete the `#TODOs` so that the dataset works as specified by the docstrings.\n",
    "\n",
    "Once done, you can try running the following code blocks as sanity checks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of train set: 1226\n",
      "Length of val set: 137\n",
      "Length of overfit set: 1\n"
     ]
    }
   ],
   "source": [
    "from exercise_3.data.shape_implicit import ShapeImplicit\n",
    "\n",
    "num_points_to_samples = 40000\n",
    "train_dataset = ShapeImplicit(num_points_to_samples, \"train\")\n",
    "val_dataset = ShapeImplicit(num_points_to_samples, \"val\")\n",
    "overfit_dataset = ShapeImplicit(num_points_to_samples, \"overfit\")\n",
    "\n",
    "# Get length, which is a call to __len__ function\n",
    "print(f'Length of train set: {len(train_dataset)}')  # expected output: 1226\n",
    "# Get length, which is a call to __len__ function\n",
    "print(f'Length of val set: {len(val_dataset)}')  # expected output: 137\n",
    "# Get length, which is a call to __len__ function\n",
    "print(f'Length of overfit set: {len(overfit_dataset)}')  # expected output: 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Let's take a look at the points sampled for a particular shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from exercise_3.util.visualization import visualize_mesh, visualize_pointcloud\n",
    "\n",
    "shape_id = train_dataset[0]['name']\n",
    "points = train_dataset[0]['points']\n",
    "sdf = train_dataset[0]['sdf']\n",
    "\n",
    "# sampled points inside the shape\n",
    "inside_points = points[sdf[:, 0] < 0, :].numpy()\n",
    "\n",
    "# sampled points outside the shape\n",
    "outside_points = points[sdf[:, 0] > 0, :].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mesh\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Program Files\\Conda\\envs\\mlgs\\lib\\site-packages\\traittypes\\traittypes.py:97: UserWarning: Given trait value dtype \"uint32\" does not match required type \"uint32\". A coerced copy has been created.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cdad159b9ae46faaabd0e39ee6be6a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mesh = ShapeImplicit.get_mesh(shape_id)\n",
    "print('Mesh')\n",
    "visualize_mesh(mesh.vertices, mesh.faces, flip_axes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled points with negative SDF (inside)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74c7d467442348bf9d699f97c9c0e88b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('Sampled points with negative SDF (inside)')\n",
    "visualize_pointcloud(inside_points, 0.025, flip_axes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled points with positive SDF (outside)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72f17207b71c41e89bc4d00b3b3573cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('Sampled points with positive SDF (outside)')\n",
    "visualize_pointcloud(outside_points, 0.025, flip_axes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "You'll notice that more points are sampled close to the surface rather than away from the surface."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### (c) Model\n",
    "\n",
    "The DeepSDF auto-decoder architecture is visualized below:\n",
    "\n",
    "<img src=\"exercise_3/images/deepsdf_architecture.png\" alt=\"deepsdf_arch\" style=\"width: 640px;\"/>\n",
    "\n",
    "Things to note:\n",
    "\n",
    "- The network takes in the latent code for a shape concatenated with the query 3d coordinate, making up a 259 length vector (assuming latent code length is 256).\n",
    "- The network consist of a sequence of weight-normed linear layers, each followed by a ReLU and a dropout. For weight norming a layer, check out `torch.nn.utils.weight_norm`. Each of these linear layers outputs a 512 dimensional vector, except the 4th layer which outputs a 253 dimensional vector.\n",
    "- The output of the 4th layer is concatenated with the input, making the input to the 5th layer a 512 dimensional vector.\n",
    "- The final layer is a simple linear layer without any norm, dropout or non-linearity, with a single dimensional output representing the SDF value.\n",
    "\n",
    "Implement this architecture in file `exercise_3/model/deepsdf.py`.\n",
    "\n",
    "Here are some basic sanity tests once you're done with your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   | Name      | Type           | Params \n",
      "-----------------------------------------------\n",
      "0  | block1    | Sequential     | 790010 \n",
      "1  | block1.0  | Linear         | 133632 \n",
      "2  | block1.1  | ReLU           | 0      \n",
      "3  | block1.2  | Dropout        | 0      \n",
      "4  | block1.3  | Linear         | 263168 \n",
      "5  | block1.4  | ReLU           | 0      \n",
      "6  | block1.5  | Dropout        | 0      \n",
      "7  | block1.6  | Linear         | 263168 \n",
      "8  | block1.7  | ReLU           | 0      \n",
      "9  | block1.8  | Dropout        | 0      \n",
      "10 | block1.9  | Linear         | 130042 \n",
      "11 | block1.10 | ReLU           | 0      \n",
      "12 | block1.11 | Dropout        | 0      \n",
      "13 | block2    | Sequential     | 1053185\n",
      "14 | block2.0  | Linear         | 263168 \n",
      "15 | block2.1  | ReLU           | 0      \n",
      "16 | block2.2  | Dropout        | 0      \n",
      "17 | block2.3  | Linear         | 263168 \n",
      "18 | block2.4  | ReLU           | 0      \n",
      "19 | block2.5  | Dropout        | 0      \n",
      "20 | block2.6  | Linear         | 263168 \n",
      "21 | block2.7  | ReLU           | 0      \n",
      "22 | block2.8  | Dropout        | 0      \n",
      "23 | block2.9  | Linear         | 263168 \n",
      "24 | block2.10 | ReLU           | 0      \n",
      "25 | block2.11 | Dropout        | 0      \n",
      "26 | block2.12 | Linear         | 513    \n",
      "27 | TOTAL     | DeepSDFDecoder | 1843195\n",
      "\n",
      "Output tensor shape:  torch.Size([4096, 1])\n",
      "\n",
      "Number of traininable params: 1.84M\n"
     ]
    }
   ],
   "source": [
    "from exercise_3.model.deepsdf import DeepSDFDecoder\n",
    "from exercise_3.util.model import summarize_model\n",
    "\n",
    "deepsdf = DeepSDFDecoder(latent_size=256)\n",
    "print(summarize_model(deepsdf))\n",
    "\n",
    "# input to the network is a concatenation of point coordinates (3) and the latent code (256 in this example);\n",
    "# here we use a batch of 4096 points\n",
    "input_tensor = torch.randn(4096, 3 + 256)\n",
    "predictions = deepsdf(input_tensor)\n",
    "\n",
    "print('\\nOutput tensor shape: ', predictions.shape)  # expected output: 4096, 1\n",
    "\n",
    "num_trainable_params = sum(p.numel() for p in deepsdf.parameters() if p.requires_grad) / 1e6\n",
    "print(f'\\nNumber of traininable params: {num_trainable_params:.2f}M')  # expected output: ~1.8M"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### (d) Training script and overfitting to a single shape\n",
    "\n",
    "Fill in the train script in `exercise_3/training/train_deepsdf.py`, and verify that your training work by overfitting to a few samples below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n",
      "[049/00000] train_loss: 0.037918\n",
      "[099/00000] train_loss: 0.025567\n",
      "[149/00000] train_loss: 0.019200\n",
      "[199/00000] train_loss: 0.014411\n",
      "[249/00000] train_loss: 0.012506\n",
      "[299/00000] train_loss: 0.011363\n",
      "[349/00000] train_loss: 0.010811\n",
      "[399/00000] train_loss: 0.009871\n",
      "[449/00000] train_loss: 0.009470\n",
      "[499/00000] train_loss: 0.009218\n",
      "[549/00000] train_loss: 0.008507\n",
      "[599/00000] train_loss: 0.008203\n",
      "[649/00000] train_loss: 0.008074\n",
      "[699/00000] train_loss: 0.007957\n",
      "[749/00000] train_loss: 0.007767\n",
      "[799/00000] train_loss: 0.007758\n",
      "[849/00000] train_loss: 0.007661\n",
      "[899/00000] train_loss: 0.007513\n",
      "[949/00000] train_loss: 0.007402\n",
      "[999/00000] train_loss: 0.007276\n",
      "[1049/00000] train_loss: 0.007081\n",
      "[1099/00000] train_loss: 0.007017\n",
      "[1149/00000] train_loss: 0.006948\n",
      "[1199/00000] train_loss: 0.006941\n",
      "[1249/00000] train_loss: 0.006830\n",
      "[1299/00000] train_loss: 0.006824\n",
      "[1349/00000] train_loss: 0.006728\n",
      "[1399/00000] train_loss: 0.006690\n",
      "[1449/00000] train_loss: 0.006694\n",
      "[1499/00000] train_loss: 0.006626\n",
      "[1549/00000] train_loss: 0.006545\n",
      "[1599/00000] train_loss: 0.006527\n",
      "[1649/00000] train_loss: 0.006470\n",
      "[1699/00000] train_loss: 0.006439\n",
      "[1749/00000] train_loss: 0.006401\n",
      "[1799/00000] train_loss: 0.006400\n",
      "[1849/00000] train_loss: 0.006366\n",
      "[1899/00000] train_loss: 0.006347\n",
      "[1949/00000] train_loss: 0.006355\n",
      "[1999/00000] train_loss: 0.006307\n"
     ]
    }
   ],
   "source": [
    "from exercise_3.training import train_deepsdf\n",
    "\n",
    "overfit_config = {\n",
    "    'experiment_name': '3_2_deepsdf_overfit',\n",
    "    'device': 'cuda:0',  # change this to cpu if you do not have a GPU\n",
    "    'is_overfit': True,\n",
    "    'num_sample_points': 4096,\n",
    "    'latent_code_length': 256,\n",
    "    'batch_size': 1,\n",
    "    'resume_ckpt': None,\n",
    "    'learning_rate_model': 0.0005,\n",
    "    'learning_rate_code': 0.001,\n",
    "    'lambda_code_regularization': 0.0001,\n",
    "    'max_epochs': 2000,\n",
    "    'print_every_n': 50,\n",
    "    'visualize_every_n': 250,\n",
    "}\n",
    "\n",
    "train_deepsdf.main(overfit_config)  # expected loss around 0.0062"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Let's visualize the overfitted shape reconstruction to check if it looks reasonable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GT\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d670bd8cb80547e9a738a1ac20089b7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overfit\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "903d43ec89d740ecb454b4391f782431",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load and visualize GT mesh of the overfit sample\n",
    "gt_mesh = ShapeImplicit.get_mesh('7e728818848f191bee7d178666aae23d')\n",
    "print('GT')\n",
    "visualize_mesh(gt_mesh.vertices, gt_mesh.faces, flip_axes=True)\n",
    "\n",
    "# Load and visualize reconstructed overfit sample; it's okay if they don't look visually exact, since we don't run \n",
    "# the training too long and have a learning rate decay while training \n",
    "mesh_path = \"exercise_3/runs/3_2_deepsdf_overfit/meshes/01999_000.obj\"\n",
    "overfit_output = trimesh.load(mesh_path)\n",
    "print('Overfit')\n",
    "visualize_mesh(overfit_output.vertices, overfit_output.faces, flip_axes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### (e) Training over entire train set\n",
    "\n",
    "Once overfitting works, we can train on the entire train set.\n",
    "\n",
    "Note: This training will take a few hours on a GPU (took ~3 hrs for 500 epochs on our 2080Ti, which already gave decent results). Please make sure to start training early enough before the submission deadline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n",
      "[000/00049] train_loss: 0.037385\n",
      "[000/00099] train_loss: 0.034046\n",
      "[000/00149] train_loss: 0.034315\n",
      "[000/00199] train_loss: 0.033476\n",
      "[000/00249] train_loss: 0.033353\n",
      "[000/00299] train_loss: 0.032586\n",
      "[000/00349] train_loss: 0.032393\n",
      "[000/00399] train_loss: 0.031931\n",
      "[000/00449] train_loss: 0.032730\n",
      "[000/00499] train_loss: 0.031647\n",
      "[000/00549] train_loss: 0.033156\n",
      "[000/00599] train_loss: 0.031827\n",
      "[000/00649] train_loss: 0.032304\n",
      "[000/00699] train_loss: 0.031404\n",
      "[000/00749] train_loss: 0.032246\n",
      "[000/00799] train_loss: 0.030611\n",
      "[000/00849] train_loss: 0.032123\n",
      "[000/00899] train_loss: 0.032131\n",
      "[000/00949] train_loss: 0.032215\n",
      "[000/00999] train_loss: 0.032766\n",
      "[000/01049] train_loss: 0.031809\n",
      "[000/01099] train_loss: 0.030970\n",
      "[000/01149] train_loss: 0.031898\n",
      "[000/01199] train_loss: 0.031158\n",
      "[001/00023] train_loss: 0.031138\n",
      "[001/00073] train_loss: 0.031629\n",
      "[001/00123] train_loss: 0.030259\n",
      "[001/00173] train_loss: 0.031422\n",
      "[001/00223] train_loss: 0.030821\n",
      "[001/00273] train_loss: 0.031804\n",
      "[001/00323] train_loss: 0.030778\n",
      "[001/00373] train_loss: 0.031463\n",
      "[001/00423] train_loss: 0.031542\n",
      "[001/00473] train_loss: 0.030001\n",
      "[001/00523] train_loss: 0.030606\n",
      "[001/00573] train_loss: 0.031573\n",
      "[001/00623] train_loss: 0.031076\n",
      "[001/00673] train_loss: 0.029525\n",
      "[001/00723] train_loss: 0.030205\n",
      "[001/00773] train_loss: 0.029147\n",
      "[001/00823] train_loss: 0.030710\n",
      "[001/00873] train_loss: 0.030929\n",
      "[001/00923] train_loss: 0.031018\n",
      "[001/00973] train_loss: 0.030902\n",
      "[001/01023] train_loss: 0.030123\n",
      "[001/01073] train_loss: 0.030108\n",
      "[001/01123] train_loss: 0.030149\n",
      "[001/01173] train_loss: 0.030815\n",
      "[001/01223] train_loss: 0.031150\n",
      "[002/00047] train_loss: 0.029758\n",
      "[002/00097] train_loss: 0.030637\n",
      "[002/00147] train_loss: 0.030470\n",
      "[002/00197] train_loss: 0.030336\n",
      "[002/00247] train_loss: 0.028890\n",
      "[002/00297] train_loss: 0.030101\n",
      "[002/00347] train_loss: 0.029390\n",
      "[002/00397] train_loss: 0.028478\n",
      "[002/00447] train_loss: 0.030315\n",
      "[002/00497] train_loss: 0.028941\n",
      "[002/00547] train_loss: 0.029798\n",
      "[002/00597] train_loss: 0.028386\n",
      "[002/00647] train_loss: 0.028207\n",
      "[002/00697] train_loss: 0.029686\n",
      "[002/00747] train_loss: 0.030116\n",
      "[002/00797] train_loss: 0.028960\n",
      "[002/00847] train_loss: 0.029569\n",
      "[002/00897] train_loss: 0.029452\n",
      "[002/00947] train_loss: 0.029632\n",
      "[002/00997] train_loss: 0.029158\n",
      "[002/01047] train_loss: 0.028506\n",
      "[002/01097] train_loss: 0.030120\n",
      "[002/01147] train_loss: 0.027820\n",
      "[002/01197] train_loss: 0.029287\n",
      "[003/00021] train_loss: 0.029925\n",
      "[003/00071] train_loss: 0.029796\n",
      "[003/00121] train_loss: 0.028824\n",
      "[003/00171] train_loss: 0.028698\n",
      "[003/00221] train_loss: 0.029134\n",
      "[003/00271] train_loss: 0.027720\n",
      "[003/00321] train_loss: 0.028057\n",
      "[003/00371] train_loss: 0.028333\n",
      "[003/00421] train_loss: 0.027370\n",
      "[003/00471] train_loss: 0.028216\n",
      "[003/00521] train_loss: 0.027333\n",
      "[003/00571] train_loss: 0.027539\n",
      "[003/00621] train_loss: 0.028574\n",
      "[003/00671] train_loss: 0.027856\n",
      "[003/00721] train_loss: 0.026424\n",
      "[003/00771] train_loss: 0.026526\n",
      "[003/00821] train_loss: 0.027088\n",
      "[003/00871] train_loss: 0.026748\n",
      "[003/00921] train_loss: 0.028446\n",
      "[003/00971] train_loss: 0.027505\n",
      "[003/01021] train_loss: 0.027437\n",
      "[003/01071] train_loss: 0.026777\n",
      "[003/01121] train_loss: 0.027031\n",
      "[003/01171] train_loss: 0.028053\n",
      "[003/01221] train_loss: 0.026503\n",
      "[004/00045] train_loss: 0.028264\n",
      "[004/00095] train_loss: 0.028285\n",
      "[004/00145] train_loss: 0.026027\n",
      "[004/00195] train_loss: 0.026838\n",
      "[004/00245] train_loss: 0.026775\n",
      "[004/00295] train_loss: 0.026419\n",
      "[004/00345] train_loss: 0.026920\n",
      "[004/00395] train_loss: 0.026509\n",
      "[004/00445] train_loss: 0.026968\n",
      "[004/00495] train_loss: 0.025778\n",
      "[004/00545] train_loss: 0.025727\n",
      "[004/00595] train_loss: 0.025514\n",
      "[004/00645] train_loss: 0.026584\n",
      "[004/00695] train_loss: 0.025091\n",
      "[004/00745] train_loss: 0.025594\n",
      "[004/00795] train_loss: 0.026048\n",
      "[004/00845] train_loss: 0.026511\n",
      "[004/00895] train_loss: 0.025759\n",
      "[004/00945] train_loss: 0.025017\n",
      "[004/00995] train_loss: 0.026052\n",
      "[004/01045] train_loss: 0.025947\n",
      "[004/01095] train_loss: 0.026762\n",
      "[004/01145] train_loss: 0.026903\n",
      "[004/01195] train_loss: 0.026377\n",
      "[005/00019] train_loss: 0.026713\n",
      "[005/00069] train_loss: 0.026499\n",
      "[005/00119] train_loss: 0.026218\n",
      "[005/00169] train_loss: 0.026099\n",
      "[005/00219] train_loss: 0.025777\n",
      "[005/00269] train_loss: 0.024814\n",
      "[005/00319] train_loss: 0.024952\n",
      "[005/00369] train_loss: 0.024917\n",
      "[005/00419] train_loss: 0.024773\n",
      "[005/00469] train_loss: 0.024696\n",
      "[005/00519] train_loss: 0.025386\n",
      "[005/00569] train_loss: 0.025109\n",
      "[005/00619] train_loss: 0.024293\n",
      "[005/00669] train_loss: 0.025322\n",
      "[005/00719] train_loss: 0.025428\n",
      "[005/00769] train_loss: 0.026445\n",
      "[005/00819] train_loss: 0.024801\n",
      "[005/00869] train_loss: 0.025311\n",
      "[005/00919] train_loss: 0.024691\n",
      "[005/00969] train_loss: 0.025109\n",
      "[005/01019] train_loss: 0.025566\n",
      "[005/01069] train_loss: 0.024777\n",
      "[005/01119] train_loss: 0.025479\n",
      "[005/01169] train_loss: 0.024702\n",
      "[005/01219] train_loss: 0.024958\n",
      "[006/00043] train_loss: 0.025241\n",
      "[006/00093] train_loss: 0.026243\n",
      "[006/00143] train_loss: 0.025510\n",
      "[006/00193] train_loss: 0.024795\n",
      "[006/00243] train_loss: 0.024260\n",
      "[006/00293] train_loss: 0.024567\n",
      "[006/00343] train_loss: 0.024871\n",
      "[006/00393] train_loss: 0.024999\n",
      "[006/00443] train_loss: 0.024079\n",
      "[006/00493] train_loss: 0.023719\n",
      "[006/00543] train_loss: 0.024719\n",
      "[006/00593] train_loss: 0.023763\n",
      "[006/00643] train_loss: 0.024516\n",
      "[006/00693] train_loss: 0.023331\n",
      "[006/00743] train_loss: 0.023253\n",
      "[006/00793] train_loss: 0.022595\n",
      "[006/00843] train_loss: 0.024247\n",
      "[006/00893] train_loss: 0.024292\n",
      "[006/00943] train_loss: 0.024409\n",
      "[006/00993] train_loss: 0.023950\n",
      "[006/01043] train_loss: 0.024059\n",
      "[006/01093] train_loss: 0.023349\n",
      "[006/01143] train_loss: 0.023269\n",
      "[006/01193] train_loss: 0.023085\n",
      "[007/00017] train_loss: 0.024304\n",
      "[007/00067] train_loss: 0.025942\n",
      "[007/00117] train_loss: 0.025319\n",
      "[007/00167] train_loss: 0.024901\n",
      "[007/00217] train_loss: 0.023672\n",
      "[007/00267] train_loss: 0.024430\n",
      "[007/00317] train_loss: 0.023878\n",
      "[007/00367] train_loss: 0.023669\n",
      "[007/00417] train_loss: 0.023516\n",
      "[007/00467] train_loss: 0.023180\n",
      "[007/00517] train_loss: 0.023902\n",
      "[007/00567] train_loss: 0.023313\n",
      "[007/00617] train_loss: 0.023709\n",
      "[007/00667] train_loss: 0.023439\n",
      "[007/00717] train_loss: 0.022859\n",
      "[007/00767] train_loss: 0.022850\n",
      "[007/00817] train_loss: 0.023744\n",
      "[007/00867] train_loss: 0.022996\n",
      "[007/00917] train_loss: 0.022859\n",
      "[007/00967] train_loss: 0.023011\n",
      "[007/01017] train_loss: 0.022569\n",
      "[007/01067] train_loss: 0.023288\n",
      "[007/01117] train_loss: 0.023140\n",
      "[007/01167] train_loss: 0.022746\n",
      "[007/01217] train_loss: 0.023556\n",
      "[008/00041] train_loss: 0.025389\n",
      "[008/00091] train_loss: 0.024299\n",
      "[008/00141] train_loss: 0.024348\n",
      "[008/00191] train_loss: 0.023237\n",
      "[008/00241] train_loss: 0.023716\n",
      "[008/00291] train_loss: 0.022380\n",
      "[008/00341] train_loss: 0.023498\n",
      "[008/00391] train_loss: 0.023980\n",
      "[008/00441] train_loss: 0.023446\n",
      "[008/00491] train_loss: 0.023227\n",
      "[008/00541] train_loss: 0.023270\n",
      "[008/00591] train_loss: 0.023414\n",
      "[008/00641] train_loss: 0.022819\n",
      "[008/00691] train_loss: 0.022258\n",
      "[008/00741] train_loss: 0.022181\n",
      "[008/00791] train_loss: 0.022393\n",
      "[008/00841] train_loss: 0.022662\n",
      "[008/00891] train_loss: 0.022372\n",
      "[008/00941] train_loss: 0.022815\n",
      "[008/00991] train_loss: 0.023593\n",
      "[008/01041] train_loss: 0.022401\n",
      "[008/01091] train_loss: 0.022557\n",
      "[008/01141] train_loss: 0.021864\n",
      "[008/01191] train_loss: 0.022316\n",
      "[009/00015] train_loss: 0.021997\n",
      "[009/00065] train_loss: 0.024547\n",
      "[009/00115] train_loss: 0.023666\n",
      "[009/00165] train_loss: 0.023720\n",
      "[009/00215] train_loss: 0.023091\n",
      "[009/00265] train_loss: 0.022084\n",
      "[009/00315] train_loss: 0.023272\n",
      "[009/00365] train_loss: 0.022245\n",
      "[009/00415] train_loss: 0.021632\n",
      "[009/00465] train_loss: 0.022074\n",
      "[009/00515] train_loss: 0.022578\n",
      "[009/00565] train_loss: 0.022451\n",
      "[009/00615] train_loss: 0.022151\n",
      "[009/00665] train_loss: 0.022386\n",
      "[009/00715] train_loss: 0.023635\n",
      "[009/00765] train_loss: 0.022809\n",
      "[009/00815] train_loss: 0.021316\n",
      "[009/00865] train_loss: 0.022116\n",
      "[009/00915] train_loss: 0.022097\n",
      "[009/00965] train_loss: 0.021309\n",
      "[009/01015] train_loss: 0.020958\n",
      "[009/01065] train_loss: 0.020877\n",
      "[009/01115] train_loss: 0.021959\n",
      "[009/01165] train_loss: 0.021667\n",
      "[009/01215] train_loss: 0.021984\n",
      "[010/00039] train_loss: 0.023817\n",
      "[010/00089] train_loss: 0.023958\n",
      "[010/00139] train_loss: 0.022762\n",
      "[010/00189] train_loss: 0.021884\n",
      "[010/00239] train_loss: 0.022609\n",
      "[010/00289] train_loss: 0.021928\n",
      "[010/00339] train_loss: 0.022500\n",
      "[010/00389] train_loss: 0.022016\n",
      "[010/00439] train_loss: 0.021166\n",
      "[010/00489] train_loss: 0.022078\n",
      "[010/00539] train_loss: 0.022229\n",
      "[010/00589] train_loss: 0.021431\n",
      "[010/00639] train_loss: 0.022218\n",
      "[010/00689] train_loss: 0.022470\n",
      "[010/00739] train_loss: 0.022042\n",
      "[010/00789] train_loss: 0.021243\n",
      "[010/00839] train_loss: 0.022159\n",
      "[010/00889] train_loss: 0.022293\n",
      "[010/00939] train_loss: 0.021367\n",
      "[010/00989] train_loss: 0.022496\n",
      "[010/01039] train_loss: 0.022317\n",
      "[010/01089] train_loss: 0.021147\n",
      "[010/01139] train_loss: 0.020938\n",
      "[010/01189] train_loss: 0.020388\n",
      "[011/00013] train_loss: 0.021941\n",
      "[011/00063] train_loss: 0.022590\n",
      "[011/00113] train_loss: 0.022482\n",
      "[011/00163] train_loss: 0.022266\n",
      "[011/00213] train_loss: 0.022989\n",
      "[011/00263] train_loss: 0.023307\n",
      "[011/00313] train_loss: 0.022300\n",
      "[011/00363] train_loss: 0.021198\n",
      "[011/00413] train_loss: 0.022802\n",
      "[011/00463] train_loss: 0.021754\n",
      "[011/00513] train_loss: 0.020993\n",
      "[011/00563] train_loss: 0.020630\n",
      "[011/00613] train_loss: 0.022228\n",
      "[011/00663] train_loss: 0.021235\n",
      "[011/00713] train_loss: 0.021309\n",
      "[011/00763] train_loss: 0.020890\n",
      "[011/00813] train_loss: 0.021185\n",
      "[011/00863] train_loss: 0.021262\n",
      "[011/00913] train_loss: 0.020750\n",
      "[011/00963] train_loss: 0.021843\n",
      "[011/01013] train_loss: 0.019828\n",
      "[011/01063] train_loss: 0.022142\n",
      "[011/01113] train_loss: 0.021867\n",
      "[011/01163] train_loss: 0.020705\n",
      "[011/01213] train_loss: 0.021174\n",
      "[012/00037] train_loss: 0.023670\n",
      "[012/00087] train_loss: 0.024270\n",
      "[012/00137] train_loss: 0.022015\n",
      "[012/00187] train_loss: 0.021799\n",
      "[012/00237] train_loss: 0.021494\n",
      "[012/00287] train_loss: 0.022186\n",
      "[012/00337] train_loss: 0.021782\n",
      "[012/00387] train_loss: 0.021725\n",
      "[012/00437] train_loss: 0.021151\n",
      "[012/00487] train_loss: 0.021054\n",
      "[012/00537] train_loss: 0.020888\n",
      "[012/00587] train_loss: 0.020291\n",
      "[012/00637] train_loss: 0.021133\n",
      "[012/00687] train_loss: 0.021241\n",
      "[012/00737] train_loss: 0.021001\n",
      "[012/00787] train_loss: 0.021203\n",
      "[012/00837] train_loss: 0.020931\n",
      "[012/00887] train_loss: 0.020465\n",
      "[012/00937] train_loss: 0.021013\n",
      "[012/00987] train_loss: 0.020746\n",
      "[012/01037] train_loss: 0.020582\n",
      "[012/01087] train_loss: 0.020817\n",
      "[012/01137] train_loss: 0.020719\n",
      "[012/01187] train_loss: 0.020841\n",
      "[013/00011] train_loss: 0.021427\n",
      "[013/00061] train_loss: 0.023712\n",
      "[013/00111] train_loss: 0.022748\n",
      "[013/00161] train_loss: 0.022452\n",
      "[013/00211] train_loss: 0.022089\n",
      "[013/00261] train_loss: 0.021128\n",
      "[013/00311] train_loss: 0.022037\n",
      "[013/00361] train_loss: 0.021064\n",
      "[013/00411] train_loss: 0.021280\n",
      "[013/00461] train_loss: 0.021003\n",
      "[013/00511] train_loss: 0.021446\n",
      "[013/00561] train_loss: 0.021258\n",
      "[013/00611] train_loss: 0.019968\n",
      "[013/00661] train_loss: 0.019966\n",
      "[013/00711] train_loss: 0.021119\n",
      "[013/00761] train_loss: 0.020283\n",
      "[013/00811] train_loss: 0.020050\n",
      "[013/00861] train_loss: 0.020953\n",
      "[013/00911] train_loss: 0.020031\n",
      "[013/00961] train_loss: 0.020980\n",
      "[013/01011] train_loss: 0.020135\n",
      "[013/01061] train_loss: 0.019829\n",
      "[013/01111] train_loss: 0.020266\n",
      "[013/01161] train_loss: 0.020370\n",
      "[013/01211] train_loss: 0.020634\n",
      "[014/00035] train_loss: 0.022726\n",
      "[014/00085] train_loss: 0.022073\n",
      "[014/00135] train_loss: 0.021922\n",
      "[014/00185] train_loss: 0.021026\n",
      "[014/00235] train_loss: 0.021925\n",
      "[014/00285] train_loss: 0.020504\n",
      "[014/00335] train_loss: 0.020061\n",
      "[014/00385] train_loss: 0.020798\n",
      "[014/00435] train_loss: 0.020575\n",
      "[014/00485] train_loss: 0.020194\n",
      "[014/00535] train_loss: 0.020639\n",
      "[014/00585] train_loss: 0.019961\n",
      "[014/00635] train_loss: 0.019650\n",
      "[014/00685] train_loss: 0.021015\n",
      "[014/00735] train_loss: 0.020305\n",
      "[014/00785] train_loss: 0.019732\n",
      "[014/00835] train_loss: 0.019629\n",
      "[014/00885] train_loss: 0.019668\n",
      "[014/00935] train_loss: 0.020052\n",
      "[014/00985] train_loss: 0.020152\n",
      "[014/01035] train_loss: 0.021397\n",
      "[014/01085] train_loss: 0.019622\n",
      "[014/01135] train_loss: 0.020029\n",
      "[014/01185] train_loss: 0.020137\n",
      "[015/00009] train_loss: 0.020718\n",
      "[015/00059] train_loss: 0.023011\n",
      "[015/00109] train_loss: 0.023835\n",
      "[015/00159] train_loss: 0.021890\n",
      "[015/00209] train_loss: 0.021850\n",
      "[015/00259] train_loss: 0.021009\n",
      "[015/00309] train_loss: 0.021474\n",
      "[015/00359] train_loss: 0.021531\n",
      "[015/00409] train_loss: 0.020279\n",
      "[015/00459] train_loss: 0.021163\n",
      "[015/00509] train_loss: 0.019247\n",
      "[015/00559] train_loss: 0.020212\n",
      "[015/00609] train_loss: 0.019120\n",
      "[015/00659] train_loss: 0.019308\n",
      "[015/00709] train_loss: 0.019984\n",
      "[015/00759] train_loss: 0.020374\n",
      "[015/00809] train_loss: 0.019668\n",
      "[015/00859] train_loss: 0.019460\n",
      "[015/00909] train_loss: 0.019742\n",
      "[015/00959] train_loss: 0.020381\n",
      "[015/01009] train_loss: 0.020866\n",
      "[015/01059] train_loss: 0.020392\n",
      "[015/01109] train_loss: 0.020589\n",
      "[015/01159] train_loss: 0.020344\n",
      "[015/01209] train_loss: 0.019255\n",
      "[016/00033] train_loss: 0.022136\n",
      "[016/00083] train_loss: 0.022346\n",
      "[016/00133] train_loss: 0.021102\n",
      "[016/00183] train_loss: 0.021537\n",
      "[016/00233] train_loss: 0.021045\n",
      "[016/00283] train_loss: 0.020308\n",
      "[016/00333] train_loss: 0.021446\n",
      "[016/00383] train_loss: 0.018684\n",
      "[016/00433] train_loss: 0.019629\n",
      "[016/00483] train_loss: 0.021087\n",
      "[016/00533] train_loss: 0.020875\n",
      "[016/00583] train_loss: 0.020021\n",
      "[016/00633] train_loss: 0.020520\n",
      "[016/00683] train_loss: 0.019350\n",
      "[016/00733] train_loss: 0.020030\n",
      "[016/00783] train_loss: 0.019627\n",
      "[016/00833] train_loss: 0.019451\n",
      "[016/00883] train_loss: 0.019354\n",
      "[016/00933] train_loss: 0.020139\n",
      "[016/00983] train_loss: 0.019423\n",
      "[016/01033] train_loss: 0.019068\n",
      "[016/01083] train_loss: 0.019486\n",
      "[016/01133] train_loss: 0.019783\n",
      "[016/01183] train_loss: 0.019463\n",
      "[017/00007] train_loss: 0.020369\n",
      "[017/00057] train_loss: 0.022253\n",
      "[017/00107] train_loss: 0.022398\n",
      "[017/00157] train_loss: 0.020711\n",
      "[017/00207] train_loss: 0.020562\n",
      "[017/00257] train_loss: 0.020872\n",
      "[017/00307] train_loss: 0.019477\n",
      "[017/00357] train_loss: 0.020672\n",
      "[017/00407] train_loss: 0.020812\n",
      "[017/00457] train_loss: 0.020054\n",
      "[017/00507] train_loss: 0.019673\n",
      "[017/00557] train_loss: 0.020365\n",
      "[017/00607] train_loss: 0.019255\n",
      "[017/00657] train_loss: 0.019374\n",
      "[017/00707] train_loss: 0.018497\n",
      "[017/00757] train_loss: 0.018793\n",
      "[017/00807] train_loss: 0.019190\n",
      "[017/00857] train_loss: 0.019113\n",
      "[017/00907] train_loss: 0.019242\n",
      "[017/00957] train_loss: 0.019316\n",
      "[017/01007] train_loss: 0.019565\n",
      "[017/01057] train_loss: 0.019966\n",
      "[017/01107] train_loss: 0.019259\n",
      "[017/01157] train_loss: 0.018415\n",
      "[017/01207] train_loss: 0.019488\n",
      "[018/00031] train_loss: 0.021679\n",
      "[018/00081] train_loss: 0.021706\n",
      "[018/00131] train_loss: 0.020991\n",
      "[018/00181] train_loss: 0.021171\n",
      "[018/00231] train_loss: 0.020388\n",
      "[018/00281] train_loss: 0.020771\n",
      "[018/00331] train_loss: 0.021079\n",
      "[018/00381] train_loss: 0.019759\n",
      "[018/00431] train_loss: 0.018965\n",
      "[018/00481] train_loss: 0.019924\n",
      "[018/00531] train_loss: 0.019523\n",
      "[018/00581] train_loss: 0.019909\n",
      "[018/00631] train_loss: 0.019880\n",
      "[018/00681] train_loss: 0.019651\n",
      "[018/00731] train_loss: 0.019209\n",
      "[018/00781] train_loss: 0.019812\n",
      "[018/00831] train_loss: 0.018890\n",
      "[018/00881] train_loss: 0.020070\n",
      "[018/00931] train_loss: 0.018846\n",
      "[018/00981] train_loss: 0.018486\n",
      "[018/01031] train_loss: 0.019744\n",
      "[018/01081] train_loss: 0.019300\n",
      "[018/01131] train_loss: 0.019230\n",
      "[018/01181] train_loss: 0.019509\n",
      "[019/00005] train_loss: 0.020102\n",
      "[019/00055] train_loss: 0.020809\n",
      "[019/00105] train_loss: 0.021476\n",
      "[019/00155] train_loss: 0.020253\n",
      "[019/00205] train_loss: 0.020122\n",
      "[019/00255] train_loss: 0.021094\n",
      "[019/00305] train_loss: 0.019600\n",
      "[019/00355] train_loss: 0.019555\n",
      "[019/00405] train_loss: 0.018910\n",
      "[019/00455] train_loss: 0.020325\n",
      "[019/00505] train_loss: 0.020038\n",
      "[019/00555] train_loss: 0.019497\n",
      "[019/00605] train_loss: 0.019670\n",
      "[019/00655] train_loss: 0.019043\n",
      "[019/00705] train_loss: 0.019142\n",
      "[019/00755] train_loss: 0.020255\n",
      "[019/00805] train_loss: 0.020234\n",
      "[019/00855] train_loss: 0.018500\n",
      "[019/00905] train_loss: 0.019588\n",
      "[019/00955] train_loss: 0.019523\n",
      "[019/01005] train_loss: 0.018814\n",
      "[019/01055] train_loss: 0.018913\n",
      "[019/01105] train_loss: 0.018775\n",
      "[019/01155] train_loss: 0.018694\n",
      "[019/01205] train_loss: 0.019300\n",
      "[020/00029] train_loss: 0.020839\n",
      "[020/00079] train_loss: 0.022413\n",
      "[020/00129] train_loss: 0.020072\n",
      "[020/00179] train_loss: 0.020975\n",
      "[020/00229] train_loss: 0.019684\n",
      "[020/00279] train_loss: 0.019568\n",
      "[020/00329] train_loss: 0.019823\n",
      "[020/00379] train_loss: 0.018755\n",
      "[020/00429] train_loss: 0.019164\n",
      "[020/00479] train_loss: 0.020119\n",
      "[020/00529] train_loss: 0.019605\n",
      "[020/00579] train_loss: 0.018566\n",
      "[020/00629] train_loss: 0.019769\n",
      "[020/00679] train_loss: 0.019239\n",
      "[020/00729] train_loss: 0.018116\n",
      "[020/00779] train_loss: 0.018222\n",
      "[020/00829] train_loss: 0.018455\n",
      "[020/00879] train_loss: 0.018422\n",
      "[020/00929] train_loss: 0.018367\n",
      "[020/00979] train_loss: 0.019049\n",
      "[020/01029] train_loss: 0.019640\n",
      "[020/01079] train_loss: 0.019196\n",
      "[020/01129] train_loss: 0.020266\n",
      "[020/01179] train_loss: 0.019198\n",
      "[021/00003] train_loss: 0.018669\n",
      "[021/00053] train_loss: 0.022238\n",
      "[021/00103] train_loss: 0.021469\n",
      "[021/00153] train_loss: 0.020728\n",
      "[021/00203] train_loss: 0.020852\n",
      "[021/00253] train_loss: 0.019693\n",
      "[021/00303] train_loss: 0.019386\n",
      "[021/00353] train_loss: 0.019189\n",
      "[021/00403] train_loss: 0.019162\n",
      "[021/00453] train_loss: 0.018784\n",
      "[021/00503] train_loss: 0.018848\n",
      "[021/00553] train_loss: 0.018816\n",
      "[021/00603] train_loss: 0.018449\n",
      "[021/00653] train_loss: 0.018732\n",
      "[021/00703] train_loss: 0.018974\n",
      "[021/00753] train_loss: 0.019333\n",
      "[021/00803] train_loss: 0.018766\n",
      "[021/00853] train_loss: 0.018779\n",
      "[021/00903] train_loss: 0.019203\n",
      "[021/00953] train_loss: 0.018480\n",
      "[021/01003] train_loss: 0.018353\n",
      "[021/01053] train_loss: 0.018878\n",
      "[021/01103] train_loss: 0.018594\n",
      "[021/01153] train_loss: 0.020132\n",
      "[021/01203] train_loss: 0.018906\n",
      "[022/00027] train_loss: 0.020077\n",
      "[022/00077] train_loss: 0.020523\n",
      "[022/00127] train_loss: 0.021483\n",
      "[022/00177] train_loss: 0.020009\n",
      "[022/00227] train_loss: 0.019831\n",
      "[022/00277] train_loss: 0.018943\n",
      "[022/00327] train_loss: 0.019090\n",
      "[022/00377] train_loss: 0.018120\n",
      "[022/00427] train_loss: 0.018858\n",
      "[022/00477] train_loss: 0.018210\n",
      "[022/00527] train_loss: 0.019370\n",
      "[022/00577] train_loss: 0.019282\n",
      "[022/00627] train_loss: 0.018817\n",
      "[022/00677] train_loss: 0.019065\n",
      "[022/00727] train_loss: 0.019463\n",
      "[022/00777] train_loss: 0.019105\n",
      "[022/00827] train_loss: 0.018541\n",
      "[022/00877] train_loss: 0.019146\n",
      "[022/00927] train_loss: 0.017993\n",
      "[022/00977] train_loss: 0.018490\n",
      "[022/01027] train_loss: 0.019377\n",
      "[022/01077] train_loss: 0.019058\n",
      "[022/01127] train_loss: 0.019719\n",
      "[022/01177] train_loss: 0.019068\n",
      "[023/00001] train_loss: 0.019016\n",
      "[023/00051] train_loss: 0.022519\n",
      "[023/00101] train_loss: 0.020477\n",
      "[023/00151] train_loss: 0.019825\n",
      "[023/00201] train_loss: 0.019587\n",
      "[023/00251] train_loss: 0.019118\n",
      "[023/00301] train_loss: 0.019717\n",
      "[023/00351] train_loss: 0.019792\n",
      "[023/00401] train_loss: 0.018915\n",
      "[023/00451] train_loss: 0.018439\n",
      "[023/00501] train_loss: 0.019372\n",
      "[023/00551] train_loss: 0.020011\n",
      "[023/00601] train_loss: 0.019901\n",
      "[023/00651] train_loss: 0.018972\n",
      "[023/00701] train_loss: 0.018546\n",
      "[023/00751] train_loss: 0.018784\n",
      "[023/00801] train_loss: 0.019079\n",
      "[023/00851] train_loss: 0.019273\n",
      "[023/00901] train_loss: 0.018979\n",
      "[023/00951] train_loss: 0.017925\n",
      "[023/01001] train_loss: 0.018785\n",
      "[023/01051] train_loss: 0.019061\n",
      "[023/01101] train_loss: 0.018576\n",
      "[023/01151] train_loss: 0.018355\n",
      "[023/01201] train_loss: 0.017696\n",
      "[024/00025] train_loss: 0.020836\n",
      "[024/00075] train_loss: 0.022126\n",
      "[024/00125] train_loss: 0.020310\n",
      "[024/00175] train_loss: 0.019960\n",
      "[024/00225] train_loss: 0.019269\n",
      "[024/00275] train_loss: 0.019625\n",
      "[024/00325] train_loss: 0.019025\n",
      "[024/00375] train_loss: 0.018680\n",
      "[024/00425] train_loss: 0.019249\n",
      "[024/00475] train_loss: 0.019094\n",
      "[024/00525] train_loss: 0.018462\n",
      "[024/00575] train_loss: 0.019415\n",
      "[024/00625] train_loss: 0.018009\n",
      "[024/00675] train_loss: 0.018519\n",
      "[024/00725] train_loss: 0.018149\n",
      "[024/00775] train_loss: 0.017820\n",
      "[024/00825] train_loss: 0.017884\n",
      "[024/00875] train_loss: 0.018375\n",
      "[024/00925] train_loss: 0.018268\n",
      "[024/00975] train_loss: 0.018733\n",
      "[024/01025] train_loss: 0.017945\n",
      "[024/01075] train_loss: 0.018624\n",
      "[024/01125] train_loss: 0.017630\n",
      "[024/01175] train_loss: 0.018451\n",
      "[024/01225] train_loss: 0.018107\n",
      "[025/00049] train_loss: 0.021974\n",
      "[025/00099] train_loss: 0.020725\n",
      "[025/00149] train_loss: 0.020186\n",
      "[025/00199] train_loss: 0.018973\n",
      "[025/00249] train_loss: 0.019113\n",
      "[025/00299] train_loss: 0.019188\n",
      "[025/00349] train_loss: 0.018283\n",
      "[025/00399] train_loss: 0.019702\n",
      "[025/00449] train_loss: 0.018103\n",
      "[025/00499] train_loss: 0.018625\n",
      "[025/00549] train_loss: 0.018217\n",
      "[025/00599] train_loss: 0.018359\n",
      "[025/00649] train_loss: 0.018163\n",
      "[025/00699] train_loss: 0.018977\n",
      "[025/00749] train_loss: 0.018835\n",
      "[025/00799] train_loss: 0.020294\n",
      "[025/00849] train_loss: 0.018394\n",
      "[025/00899] train_loss: 0.018057\n",
      "[025/00949] train_loss: 0.018821\n",
      "[025/00999] train_loss: 0.017697\n",
      "[025/01049] train_loss: 0.018945\n",
      "[025/01099] train_loss: 0.017766\n",
      "[025/01149] train_loss: 0.017975\n",
      "[025/01199] train_loss: 0.018306\n",
      "[026/00023] train_loss: 0.020038\n",
      "[026/00073] train_loss: 0.021029\n",
      "[026/00123] train_loss: 0.020045\n",
      "[026/00173] train_loss: 0.019268\n",
      "[026/00223] train_loss: 0.019572\n",
      "[026/00273] train_loss: 0.018859\n",
      "[026/00323] train_loss: 0.018118\n",
      "[026/00373] train_loss: 0.019127\n",
      "[026/00423] train_loss: 0.019220\n",
      "[026/00473] train_loss: 0.019350\n",
      "[026/00523] train_loss: 0.018364\n",
      "[026/00573] train_loss: 0.018808\n",
      "[026/00623] train_loss: 0.018193\n",
      "[026/00673] train_loss: 0.017897\n",
      "[026/00723] train_loss: 0.017597\n",
      "[026/00773] train_loss: 0.018269\n",
      "[026/00823] train_loss: 0.017748\n",
      "[026/00873] train_loss: 0.017947\n",
      "[026/00923] train_loss: 0.018169\n",
      "[026/00973] train_loss: 0.018195\n",
      "[026/01023] train_loss: 0.018754\n",
      "[026/01073] train_loss: 0.019348\n",
      "[026/01123] train_loss: 0.016729\n",
      "[026/01173] train_loss: 0.017926\n",
      "[026/01223] train_loss: 0.018242\n",
      "[027/00047] train_loss: 0.022110\n",
      "[027/00097] train_loss: 0.019277\n",
      "[027/00147] train_loss: 0.019189\n",
      "[027/00197] train_loss: 0.019184\n",
      "[027/00247] train_loss: 0.018636\n",
      "[027/00297] train_loss: 0.018945\n",
      "[027/00347] train_loss: 0.018687\n",
      "[027/00397] train_loss: 0.018581\n",
      "[027/00447] train_loss: 0.018439\n",
      "[027/00497] train_loss: 0.018302\n",
      "[027/00547] train_loss: 0.018025\n",
      "[027/00597] train_loss: 0.017843\n",
      "[027/00647] train_loss: 0.017469\n",
      "[027/00697] train_loss: 0.018680\n",
      "[027/00747] train_loss: 0.018013\n",
      "[027/00797] train_loss: 0.017810\n",
      "[027/00847] train_loss: 0.017817\n",
      "[027/00897] train_loss: 0.018117\n",
      "[027/00947] train_loss: 0.018459\n",
      "[027/00997] train_loss: 0.018712\n",
      "[027/01047] train_loss: 0.017636\n",
      "[027/01097] train_loss: 0.017837\n",
      "[027/01147] train_loss: 0.017585\n",
      "[027/01197] train_loss: 0.017810\n",
      "[028/00021] train_loss: 0.019788\n",
      "[028/00071] train_loss: 0.020422\n",
      "[028/00121] train_loss: 0.019373\n",
      "[028/00171] train_loss: 0.019302\n",
      "[028/00221] train_loss: 0.019221\n",
      "[028/00271] train_loss: 0.019227\n",
      "[028/00321] train_loss: 0.018044\n",
      "[028/00371] train_loss: 0.018791\n",
      "[028/00421] train_loss: 0.017835\n",
      "[028/00471] train_loss: 0.016912\n",
      "[028/00521] train_loss: 0.018454\n",
      "[028/00571] train_loss: 0.017960\n",
      "[028/00621] train_loss: 0.018466\n",
      "[028/00671] train_loss: 0.018421\n",
      "[028/00721] train_loss: 0.016801\n",
      "[028/00771] train_loss: 0.017528\n",
      "[028/00821] train_loss: 0.018712\n",
      "[028/00871] train_loss: 0.018283\n",
      "[028/00921] train_loss: 0.018431\n",
      "[028/00971] train_loss: 0.018332\n",
      "[028/01021] train_loss: 0.017851\n",
      "[028/01071] train_loss: 0.017451\n",
      "[028/01121] train_loss: 0.018105\n",
      "[028/01171] train_loss: 0.017698\n",
      "[028/01221] train_loss: 0.017766\n",
      "[029/00045] train_loss: 0.021949\n",
      "[029/00095] train_loss: 0.020954\n",
      "[029/00145] train_loss: 0.019508\n",
      "[029/00195] train_loss: 0.018289\n",
      "[029/00245] train_loss: 0.019771\n",
      "[029/00295] train_loss: 0.018463\n",
      "[029/00345] train_loss: 0.019015\n",
      "[029/00395] train_loss: 0.017851\n",
      "[029/00445] train_loss: 0.018163\n",
      "[029/00495] train_loss: 0.019548\n",
      "[029/00545] train_loss: 0.017856\n",
      "[029/00595] train_loss: 0.017504\n",
      "[029/00645] train_loss: 0.016927\n",
      "[029/00695] train_loss: 0.018093\n",
      "[029/00745] train_loss: 0.018143\n",
      "[029/00795] train_loss: 0.017608\n",
      "[029/00845] train_loss: 0.018075\n",
      "[029/00895] train_loss: 0.018608\n",
      "[029/00945] train_loss: 0.016897\n",
      "[029/00995] train_loss: 0.017493\n",
      "[029/01045] train_loss: 0.017045\n",
      "[029/01095] train_loss: 0.017726\n",
      "[029/01145] train_loss: 0.017586\n",
      "[029/01195] train_loss: 0.018429\n",
      "[030/00019] train_loss: 0.019602\n",
      "[030/00069] train_loss: 0.020646\n",
      "[030/00119] train_loss: 0.019779\n",
      "[030/00169] train_loss: 0.018118\n",
      "[030/00219] train_loss: 0.018587\n",
      "[030/00269] train_loss: 0.018439\n",
      "[030/00319] train_loss: 0.018311\n",
      "[030/00369] train_loss: 0.018436\n",
      "[030/00419] train_loss: 0.018450\n",
      "[030/00469] train_loss: 0.017336\n",
      "[030/00519] train_loss: 0.017559\n",
      "[030/00569] train_loss: 0.018278\n",
      "[030/00619] train_loss: 0.017951\n",
      "[030/00669] train_loss: 0.017487\n",
      "[030/00719] train_loss: 0.018185\n",
      "[030/00769] train_loss: 0.018361\n",
      "[030/00819] train_loss: 0.017270\n",
      "[030/00869] train_loss: 0.017569\n",
      "[030/00919] train_loss: 0.016978\n",
      "[030/00969] train_loss: 0.017759\n",
      "[030/01019] train_loss: 0.017559\n",
      "[030/01069] train_loss: 0.017746\n",
      "[030/01119] train_loss: 0.017384\n",
      "[030/01169] train_loss: 0.017611\n",
      "[030/01219] train_loss: 0.017531\n",
      "[031/00043] train_loss: 0.020261\n",
      "[031/00093] train_loss: 0.021462\n",
      "[031/00143] train_loss: 0.019791\n",
      "[031/00193] train_loss: 0.018604\n",
      "[031/00243] train_loss: 0.018580\n",
      "[031/00293] train_loss: 0.018453\n",
      "[031/00343] train_loss: 0.017596\n",
      "[031/00393] train_loss: 0.017912\n",
      "[031/00443] train_loss: 0.018999\n",
      "[031/00493] train_loss: 0.019055\n",
      "[031/00543] train_loss: 0.018080\n",
      "[031/00593] train_loss: 0.017475\n",
      "[031/00643] train_loss: 0.017628\n",
      "[031/00693] train_loss: 0.017656\n",
      "[031/00743] train_loss: 0.017321\n",
      "[031/00793] train_loss: 0.017576\n",
      "[031/00843] train_loss: 0.017889\n",
      "[031/00893] train_loss: 0.018019\n",
      "[031/00943] train_loss: 0.018030\n",
      "[031/00993] train_loss: 0.016803\n",
      "[031/01043] train_loss: 0.016927\n",
      "[031/01093] train_loss: 0.017636\n",
      "[031/01143] train_loss: 0.018158\n",
      "[031/01193] train_loss: 0.017043\n",
      "[032/00017] train_loss: 0.019119\n",
      "[032/00067] train_loss: 0.019329\n",
      "[032/00117] train_loss: 0.019550\n",
      "[032/00167] train_loss: 0.019166\n",
      "[032/00217] train_loss: 0.018540\n",
      "[032/00267] train_loss: 0.017384\n",
      "[032/00317] train_loss: 0.018240\n",
      "[032/00367] train_loss: 0.018648\n",
      "[032/00417] train_loss: 0.017438\n",
      "[032/00467] train_loss: 0.017590\n",
      "[032/00517] train_loss: 0.017962\n",
      "[032/00567] train_loss: 0.017658\n",
      "[032/00617] train_loss: 0.017511\n",
      "[032/00667] train_loss: 0.017681\n",
      "[032/00717] train_loss: 0.018675\n",
      "[032/00767] train_loss: 0.017970\n",
      "[032/00817] train_loss: 0.017274\n",
      "[032/00867] train_loss: 0.017391\n",
      "[032/00917] train_loss: 0.017722\n",
      "[032/00967] train_loss: 0.018164\n",
      "[032/01017] train_loss: 0.018087\n",
      "[032/01067] train_loss: 0.017655\n",
      "[032/01117] train_loss: 0.017494\n",
      "[032/01167] train_loss: 0.018002\n",
      "[032/01217] train_loss: 0.017599\n",
      "[033/00041] train_loss: 0.020200\n",
      "[033/00091] train_loss: 0.018895\n",
      "[033/00141] train_loss: 0.019489\n",
      "[033/00191] train_loss: 0.019073\n",
      "[033/00241] train_loss: 0.019008\n",
      "[033/00291] train_loss: 0.018454\n",
      "[033/00341] train_loss: 0.018103\n",
      "[033/00391] train_loss: 0.018031\n",
      "[033/00441] train_loss: 0.017816\n",
      "[033/00491] train_loss: 0.017359\n",
      "[033/00541] train_loss: 0.017733\n",
      "[033/00591] train_loss: 0.016660\n",
      "[033/00641] train_loss: 0.016813\n",
      "[033/00691] train_loss: 0.017202\n",
      "[033/00741] train_loss: 0.017337\n",
      "[033/00791] train_loss: 0.017682\n",
      "[033/00841] train_loss: 0.017667\n",
      "[033/00891] train_loss: 0.017692\n",
      "[033/00941] train_loss: 0.017431\n",
      "[033/00991] train_loss: 0.017426\n",
      "[033/01041] train_loss: 0.017752\n",
      "[033/01091] train_loss: 0.016921\n",
      "[033/01141] train_loss: 0.017658\n",
      "[033/01191] train_loss: 0.017588\n",
      "[034/00015] train_loss: 0.019121\n",
      "[034/00065] train_loss: 0.019732\n",
      "[034/00115] train_loss: 0.019690\n",
      "[034/00165] train_loss: 0.019010\n",
      "[034/00215] train_loss: 0.018651\n",
      "[034/00265] train_loss: 0.018820\n",
      "[034/00315] train_loss: 0.018562\n",
      "[034/00365] train_loss: 0.018638\n",
      "[034/00415] train_loss: 0.018190\n",
      "[034/00465] train_loss: 0.018245\n",
      "[034/00515] train_loss: 0.017212\n",
      "[034/00565] train_loss: 0.017033\n",
      "[034/00615] train_loss: 0.017033\n",
      "[034/00665] train_loss: 0.016828\n",
      "[034/00715] train_loss: 0.017788\n",
      "[034/00765] train_loss: 0.017329\n",
      "[034/00815] train_loss: 0.018135\n",
      "[034/00865] train_loss: 0.017213\n",
      "[034/00915] train_loss: 0.017215\n",
      "[034/00965] train_loss: 0.017169\n",
      "[034/01015] train_loss: 0.016571\n",
      "[034/01065] train_loss: 0.017898\n",
      "[034/01115] train_loss: 0.017890\n",
      "[034/01165] train_loss: 0.016574\n",
      "[034/01215] train_loss: 0.018230\n",
      "[035/00039] train_loss: 0.020656\n",
      "[035/00089] train_loss: 0.019653\n",
      "[035/00139] train_loss: 0.019715\n",
      "[035/00189] train_loss: 0.018752\n",
      "[035/00239] train_loss: 0.019373\n",
      "[035/00289] train_loss: 0.018098\n",
      "[035/00339] train_loss: 0.017806\n",
      "[035/00389] train_loss: 0.017423\n",
      "[035/00439] train_loss: 0.017374\n",
      "[035/00489] train_loss: 0.017949\n",
      "[035/00539] train_loss: 0.017876\n",
      "[035/00589] train_loss: 0.017301\n",
      "[035/00639] train_loss: 0.017345\n",
      "[035/00689] train_loss: 0.016792\n",
      "[035/00739] train_loss: 0.017475\n",
      "[035/00789] train_loss: 0.017539\n",
      "[035/00839] train_loss: 0.017213\n",
      "[035/00889] train_loss: 0.017309\n",
      "[035/00939] train_loss: 0.016446\n",
      "[035/00989] train_loss: 0.016927\n",
      "[035/01039] train_loss: 0.016975\n",
      "[035/01089] train_loss: 0.017225\n",
      "[035/01139] train_loss: 0.017307\n",
      "[035/01189] train_loss: 0.016783\n",
      "[036/00013] train_loss: 0.017587\n",
      "[036/00063] train_loss: 0.019676\n",
      "[036/00113] train_loss: 0.019651\n",
      "[036/00163] train_loss: 0.018375\n",
      "[036/00213] train_loss: 0.017503\n",
      "[036/00263] train_loss: 0.018151\n",
      "[036/00313] train_loss: 0.018302\n",
      "[036/00363] train_loss: 0.017494\n",
      "[036/00413] train_loss: 0.017711\n",
      "[036/00463] train_loss: 0.017485\n",
      "[036/00513] train_loss: 0.016890\n",
      "[036/00563] train_loss: 0.017191\n",
      "[036/00613] train_loss: 0.018199\n",
      "[036/00663] train_loss: 0.016993\n",
      "[036/00713] train_loss: 0.017285\n",
      "[036/00763] train_loss: 0.017162\n",
      "[036/00813] train_loss: 0.016566\n",
      "[036/00863] train_loss: 0.016980\n",
      "[036/00913] train_loss: 0.017426\n",
      "[036/00963] train_loss: 0.018181\n",
      "[036/01013] train_loss: 0.017766\n",
      "[036/01063] train_loss: 0.017495\n",
      "[036/01113] train_loss: 0.017113\n",
      "[036/01163] train_loss: 0.016908\n",
      "[036/01213] train_loss: 0.016834\n",
      "[037/00037] train_loss: 0.019491\n",
      "[037/00087] train_loss: 0.020211\n",
      "[037/00137] train_loss: 0.019350\n",
      "[037/00187] train_loss: 0.019028\n",
      "[037/00237] train_loss: 0.018656\n",
      "[037/00287] train_loss: 0.017764\n",
      "[037/00337] train_loss: 0.018432\n",
      "[037/00387] train_loss: 0.017931\n",
      "[037/00437] train_loss: 0.017735\n",
      "[037/00487] train_loss: 0.017413\n",
      "[037/00537] train_loss: 0.018218\n",
      "[037/00587] train_loss: 0.017019\n",
      "[037/00637] train_loss: 0.018072\n",
      "[037/00687] train_loss: 0.016653\n",
      "[037/00737] train_loss: 0.016994\n",
      "[037/00787] train_loss: 0.016953\n",
      "[037/00837] train_loss: 0.016778\n",
      "[037/00887] train_loss: 0.017779\n",
      "[037/00937] train_loss: 0.017409\n",
      "[037/00987] train_loss: 0.017933\n",
      "[037/01037] train_loss: 0.017126\n",
      "[037/01087] train_loss: 0.017219\n",
      "[037/01137] train_loss: 0.016256\n",
      "[037/01187] train_loss: 0.015876\n",
      "[038/00011] train_loss: 0.017812\n",
      "[038/00061] train_loss: 0.020159\n",
      "[038/00111] train_loss: 0.019728\n",
      "[038/00161] train_loss: 0.019003\n",
      "[038/00211] train_loss: 0.018576\n",
      "[038/00261] train_loss: 0.018099\n",
      "[038/00311] train_loss: 0.017935\n",
      "[038/00361] train_loss: 0.016957\n",
      "[038/00411] train_loss: 0.018331\n",
      "[038/00461] train_loss: 0.017448\n",
      "[038/00511] train_loss: 0.017372\n",
      "[038/00561] train_loss: 0.017814\n",
      "[038/00611] train_loss: 0.016836\n",
      "[038/00661] train_loss: 0.016161\n",
      "[038/00711] train_loss: 0.016631\n",
      "[038/00761] train_loss: 0.017249\n",
      "[038/00811] train_loss: 0.016942\n",
      "[038/00861] train_loss: 0.017151\n",
      "[038/00911] train_loss: 0.016798\n",
      "[038/00961] train_loss: 0.016505\n",
      "[038/01011] train_loss: 0.016761\n",
      "[038/01061] train_loss: 0.016959\n",
      "[038/01111] train_loss: 0.016237\n",
      "[038/01161] train_loss: 0.016911\n",
      "[038/01211] train_loss: 0.016207\n",
      "[039/00035] train_loss: 0.018444\n",
      "[039/00085] train_loss: 0.019861\n",
      "[039/00135] train_loss: 0.018031\n",
      "[039/00185] train_loss: 0.017564\n",
      "[039/00235] train_loss: 0.018407\n",
      "[039/00285] train_loss: 0.017517\n",
      "[039/00335] train_loss: 0.018081\n",
      "[039/00385] train_loss: 0.017876\n",
      "[039/00435] train_loss: 0.017073\n",
      "[039/00485] train_loss: 0.017125\n",
      "[039/00535] train_loss: 0.017213\n",
      "[039/00585] train_loss: 0.016695\n",
      "[039/00635] train_loss: 0.017524\n",
      "[039/00685] train_loss: 0.016861\n",
      "[039/00735] train_loss: 0.016888\n",
      "[039/00785] train_loss: 0.017492\n",
      "[039/00835] train_loss: 0.016891\n",
      "[039/00885] train_loss: 0.017174\n",
      "[039/00935] train_loss: 0.016757\n",
      "[039/00985] train_loss: 0.017947\n",
      "[039/01035] train_loss: 0.016307\n",
      "[039/01085] train_loss: 0.016475\n",
      "[039/01135] train_loss: 0.016624\n",
      "[039/01185] train_loss: 0.017145\n",
      "[040/00009] train_loss: 0.018816\n",
      "[040/00059] train_loss: 0.019977\n",
      "[040/00109] train_loss: 0.019346\n",
      "[040/00159] train_loss: 0.019079\n",
      "[040/00209] train_loss: 0.018166\n",
      "[040/00259] train_loss: 0.017399\n",
      "[040/00309] train_loss: 0.017870\n",
      "[040/00359] train_loss: 0.016760\n",
      "[040/00409] train_loss: 0.017774\n",
      "[040/00459] train_loss: 0.017093\n",
      "[040/00509] train_loss: 0.017389\n",
      "[040/00559] train_loss: 0.017591\n",
      "[040/00609] train_loss: 0.016786\n",
      "[040/00659] train_loss: 0.017192\n",
      "[040/00709] train_loss: 0.016315\n",
      "[040/00759] train_loss: 0.017495\n",
      "[040/00809] train_loss: 0.016901\n",
      "[040/00859] train_loss: 0.016530\n",
      "[040/00909] train_loss: 0.016345\n",
      "[040/00959] train_loss: 0.016676\n",
      "[040/01009] train_loss: 0.017006\n",
      "[040/01059] train_loss: 0.016507\n",
      "[040/01109] train_loss: 0.017443\n",
      "[040/01159] train_loss: 0.016895\n",
      "[040/01209] train_loss: 0.016863\n",
      "[041/00033] train_loss: 0.019148\n",
      "[041/00083] train_loss: 0.018599\n",
      "[041/00133] train_loss: 0.018981\n",
      "[041/00183] train_loss: 0.017914\n",
      "[041/00233] train_loss: 0.018197\n",
      "[041/00283] train_loss: 0.017511\n",
      "[041/00333] train_loss: 0.017549\n",
      "[041/00383] train_loss: 0.017391\n",
      "[041/00433] train_loss: 0.016829\n",
      "[041/00483] train_loss: 0.017649\n",
      "[041/00533] train_loss: 0.016432\n",
      "[041/00583] train_loss: 0.016893\n",
      "[041/00633] train_loss: 0.017052\n",
      "[041/00683] train_loss: 0.016819\n",
      "[041/00733] train_loss: 0.016682\n",
      "[041/00783] train_loss: 0.015927\n",
      "[041/00833] train_loss: 0.017092\n",
      "[041/00883] train_loss: 0.017655\n",
      "[041/00933] train_loss: 0.017356\n",
      "[041/00983] train_loss: 0.017283\n",
      "[041/01033] train_loss: 0.016523\n",
      "[041/01083] train_loss: 0.016811\n",
      "[041/01133] train_loss: 0.016788\n",
      "[041/01183] train_loss: 0.016577\n",
      "[042/00007] train_loss: 0.017809\n",
      "[042/00057] train_loss: 0.018621\n",
      "[042/00107] train_loss: 0.018513\n",
      "[042/00157] train_loss: 0.018266\n",
      "[042/00207] train_loss: 0.017819\n",
      "[042/00257] train_loss: 0.018058\n",
      "[042/00307] train_loss: 0.017360\n",
      "[042/00357] train_loss: 0.017748\n",
      "[042/00407] train_loss: 0.017504\n",
      "[042/00457] train_loss: 0.016497\n",
      "[042/00507] train_loss: 0.017211\n",
      "[042/00557] train_loss: 0.017097\n",
      "[042/00607] train_loss: 0.016265\n",
      "[042/00657] train_loss: 0.016589\n",
      "[042/00707] train_loss: 0.016219\n",
      "[042/00757] train_loss: 0.016457\n",
      "[042/00807] train_loss: 0.016872\n",
      "[042/00857] train_loss: 0.016457\n",
      "[042/00907] train_loss: 0.016717\n",
      "[042/00957] train_loss: 0.016822\n",
      "[042/01007] train_loss: 0.016314\n",
      "[042/01057] train_loss: 0.016140\n",
      "[042/01107] train_loss: 0.016563\n",
      "[042/01157] train_loss: 0.016345\n",
      "[042/01207] train_loss: 0.016807\n",
      "[043/00031] train_loss: 0.018188\n",
      "[043/00081] train_loss: 0.018360\n",
      "[043/00131] train_loss: 0.018049\n",
      "[043/00181] train_loss: 0.018455\n",
      "[043/00231] train_loss: 0.018558\n",
      "[043/00281] train_loss: 0.018029\n",
      "[043/00331] train_loss: 0.016967\n",
      "[043/00381] train_loss: 0.018078\n",
      "[043/00431] train_loss: 0.017097\n",
      "[043/00481] train_loss: 0.017573\n",
      "[043/00531] train_loss: 0.016795\n",
      "[043/00581] train_loss: 0.017429\n",
      "[043/00631] train_loss: 0.016564\n",
      "[043/00681] train_loss: 0.017593\n",
      "[043/00731] train_loss: 0.017156\n",
      "[043/00781] train_loss: 0.017560\n",
      "[043/00831] train_loss: 0.016134\n",
      "[043/00881] train_loss: 0.017395\n",
      "[043/00931] train_loss: 0.016326\n",
      "[043/00981] train_loss: 0.016532\n",
      "[043/01031] train_loss: 0.016208\n",
      "[043/01081] train_loss: 0.016311\n",
      "[043/01131] train_loss: 0.015549\n",
      "[043/01181] train_loss: 0.016327\n",
      "[044/00005] train_loss: 0.016145\n",
      "[044/00055] train_loss: 0.019398\n",
      "[044/00105] train_loss: 0.018777\n",
      "[044/00155] train_loss: 0.016783\n",
      "[044/00205] train_loss: 0.018024\n",
      "[044/00255] train_loss: 0.017345\n",
      "[044/00305] train_loss: 0.017113\n",
      "[044/00355] train_loss: 0.016174\n",
      "[044/00405] train_loss: 0.018327\n",
      "[044/00455] train_loss: 0.016279\n",
      "[044/00505] train_loss: 0.017546\n",
      "[044/00555] train_loss: 0.017159\n",
      "[044/00605] train_loss: 0.016327\n",
      "[044/00655] train_loss: 0.017104\n",
      "[044/00705] train_loss: 0.016633\n",
      "[044/00755] train_loss: 0.017679\n",
      "[044/00805] train_loss: 0.016472\n",
      "[044/00855] train_loss: 0.016737\n",
      "[044/00905] train_loss: 0.016691\n",
      "[044/00955] train_loss: 0.017302\n",
      "[044/01005] train_loss: 0.016374\n",
      "[044/01055] train_loss: 0.016296\n",
      "[044/01105] train_loss: 0.017086\n",
      "[044/01155] train_loss: 0.016751\n",
      "[044/01205] train_loss: 0.016612\n",
      "[045/00029] train_loss: 0.018816\n",
      "[045/00079] train_loss: 0.018785\n",
      "[045/00129] train_loss: 0.017520\n",
      "[045/00179] train_loss: 0.018024\n",
      "[045/00229] train_loss: 0.016798\n",
      "[045/00279] train_loss: 0.017545\n",
      "[045/00329] train_loss: 0.017855\n",
      "[045/00379] train_loss: 0.017242\n",
      "[045/00429] train_loss: 0.016561\n",
      "[045/00479] train_loss: 0.016949\n",
      "[045/00529] train_loss: 0.016943\n",
      "[045/00579] train_loss: 0.017456\n",
      "[045/00629] train_loss: 0.016834\n",
      "[045/00679] train_loss: 0.016422\n",
      "[045/00729] train_loss: 0.016542\n",
      "[045/00779] train_loss: 0.017380\n",
      "[045/00829] train_loss: 0.016575\n",
      "[045/00879] train_loss: 0.016766\n",
      "[045/00929] train_loss: 0.015805\n",
      "[045/00979] train_loss: 0.016082\n",
      "[045/01029] train_loss: 0.016794\n",
      "[045/01079] train_loss: 0.016692\n",
      "[045/01129] train_loss: 0.016917\n",
      "[045/01179] train_loss: 0.016175\n",
      "[046/00003] train_loss: 0.015960\n",
      "[046/00053] train_loss: 0.018592\n",
      "[046/00103] train_loss: 0.017903\n",
      "[046/00153] train_loss: 0.017831\n",
      "[046/00203] train_loss: 0.018257\n",
      "[046/00253] train_loss: 0.016787\n",
      "[046/00303] train_loss: 0.017547\n",
      "[046/00353] train_loss: 0.017524\n",
      "[046/00403] train_loss: 0.016509\n",
      "[046/00453] train_loss: 0.017455\n",
      "[046/00503] train_loss: 0.017725\n",
      "[046/00553] train_loss: 0.016952\n",
      "[046/00603] train_loss: 0.017250\n",
      "[046/00653] train_loss: 0.016534\n",
      "[046/00703] train_loss: 0.017039\n",
      "[046/00753] train_loss: 0.016600\n",
      "[046/00803] train_loss: 0.016750\n",
      "[046/00853] train_loss: 0.016401\n",
      "[046/00903] train_loss: 0.016424\n",
      "[046/00953] train_loss: 0.017023\n",
      "[046/01003] train_loss: 0.016332\n",
      "[046/01053] train_loss: 0.016531\n",
      "[046/01103] train_loss: 0.017726\n",
      "[046/01153] train_loss: 0.016567\n",
      "[046/01203] train_loss: 0.015870\n",
      "[047/00027] train_loss: 0.018150\n",
      "[047/00077] train_loss: 0.018557\n",
      "[047/00127] train_loss: 0.017363\n",
      "[047/00177] train_loss: 0.017689\n",
      "[047/00227] train_loss: 0.017169\n",
      "[047/00277] train_loss: 0.017344\n",
      "[047/00327] train_loss: 0.016488\n",
      "[047/00377] train_loss: 0.017082\n",
      "[047/00427] train_loss: 0.017381\n",
      "[047/00477] train_loss: 0.017373\n",
      "[047/00527] train_loss: 0.017108\n",
      "[047/00577] train_loss: 0.015902\n",
      "[047/00627] train_loss: 0.016516\n",
      "[047/00677] train_loss: 0.016278\n",
      "[047/00727] train_loss: 0.016636\n",
      "[047/00777] train_loss: 0.016496\n",
      "[047/00827] train_loss: 0.015990\n",
      "[047/00877] train_loss: 0.017192\n",
      "[047/00927] train_loss: 0.016888\n",
      "[047/00977] train_loss: 0.016311\n",
      "[047/01027] train_loss: 0.016537\n",
      "[047/01077] train_loss: 0.016679\n",
      "[047/01127] train_loss: 0.016397\n",
      "[047/01177] train_loss: 0.015738\n",
      "[048/00001] train_loss: 0.016336\n",
      "[048/00051] train_loss: 0.018858\n",
      "[048/00101] train_loss: 0.017815\n",
      "[048/00151] train_loss: 0.018436\n",
      "[048/00201] train_loss: 0.016935\n",
      "[048/00251] train_loss: 0.017501\n",
      "[048/00301] train_loss: 0.017224\n",
      "[048/00351] train_loss: 0.017164\n",
      "[048/00401] train_loss: 0.017085\n",
      "[048/00451] train_loss: 0.017616\n",
      "[048/00501] train_loss: 0.016676\n",
      "[048/00551] train_loss: 0.016518\n",
      "[048/00601] train_loss: 0.016850\n",
      "[048/00651] train_loss: 0.017828\n",
      "[048/00701] train_loss: 0.015790\n",
      "[048/00751] train_loss: 0.016670\n",
      "[048/00801] train_loss: 0.016577\n",
      "[048/00851] train_loss: 0.016093\n",
      "[048/00901] train_loss: 0.016082\n",
      "[048/00951] train_loss: 0.016390\n",
      "[048/01001] train_loss: 0.016747\n",
      "[048/01051] train_loss: 0.015218\n",
      "[048/01101] train_loss: 0.015756\n",
      "[048/01151] train_loss: 0.015862\n",
      "[048/01201] train_loss: 0.017024\n",
      "[049/00025] train_loss: 0.017415\n",
      "[049/00075] train_loss: 0.019406\n",
      "[049/00125] train_loss: 0.017991\n",
      "[049/00175] train_loss: 0.017254\n",
      "[049/00225] train_loss: 0.018003\n",
      "[049/00275] train_loss: 0.016787\n",
      "[049/00325] train_loss: 0.016876\n",
      "[049/00375] train_loss: 0.017032\n",
      "[049/00425] train_loss: 0.016639\n",
      "[049/00475] train_loss: 0.016852\n",
      "[049/00525] train_loss: 0.016858\n",
      "[049/00575] train_loss: 0.016664\n",
      "[049/00625] train_loss: 0.016175\n",
      "[049/00675] train_loss: 0.016576\n",
      "[049/00725] train_loss: 0.016672\n",
      "[049/00775] train_loss: 0.017003\n",
      "[049/00825] train_loss: 0.016975\n",
      "[049/00875] train_loss: 0.015961\n",
      "[049/00925] train_loss: 0.015262\n",
      "[049/00975] train_loss: 0.016481\n",
      "[049/01025] train_loss: 0.016655\n",
      "[049/01075] train_loss: 0.015966\n",
      "[049/01125] train_loss: 0.015214\n",
      "[049/01175] train_loss: 0.017009\n",
      "[049/01225] train_loss: 0.016378\n",
      "[050/00049] train_loss: 0.018688\n",
      "[050/00099] train_loss: 0.018211\n",
      "[050/00149] train_loss: 0.018184\n",
      "[050/00199] train_loss: 0.017836\n",
      "[050/00249] train_loss: 0.017861\n",
      "[050/00299] train_loss: 0.017646\n",
      "[050/00349] train_loss: 0.016360\n",
      "[050/00399] train_loss: 0.018203\n",
      "[050/00449] train_loss: 0.016408\n",
      "[050/00499] train_loss: 0.015979\n",
      "[050/00549] train_loss: 0.016596\n",
      "[050/00599] train_loss: 0.016506\n",
      "[050/00649] train_loss: 0.016896\n",
      "[050/00699] train_loss: 0.016356\n",
      "[050/00749] train_loss: 0.015672\n",
      "[050/00799] train_loss: 0.015438\n",
      "[050/00849] train_loss: 0.016074\n",
      "[050/00899] train_loss: 0.015837\n",
      "[050/00949] train_loss: 0.015979\n",
      "[050/00999] train_loss: 0.016489\n",
      "[050/01049] train_loss: 0.015851\n",
      "[050/01099] train_loss: 0.016586\n",
      "[050/01149] train_loss: 0.016427\n",
      "[050/01199] train_loss: 0.015878\n",
      "[051/00023] train_loss: 0.016764\n",
      "[051/00073] train_loss: 0.018633\n",
      "[051/00123] train_loss: 0.017302\n",
      "[051/00173] train_loss: 0.018586\n",
      "[051/00223] train_loss: 0.017158\n",
      "[051/00273] train_loss: 0.016856\n",
      "[051/00323] train_loss: 0.017100\n",
      "[051/00373] train_loss: 0.017334\n",
      "[051/00423] train_loss: 0.016745\n",
      "[051/00473] train_loss: 0.015081\n",
      "[051/00523] train_loss: 0.017469\n",
      "[051/00573] train_loss: 0.016635\n",
      "[051/00623] train_loss: 0.016431\n",
      "[051/00673] train_loss: 0.015828\n",
      "[051/00723] train_loss: 0.016152\n",
      "[051/00773] train_loss: 0.016197\n",
      "[051/00823] train_loss: 0.016379\n",
      "[051/00873] train_loss: 0.016770\n",
      "[051/00923] train_loss: 0.017018\n",
      "[051/00973] train_loss: 0.016517\n",
      "[051/01023] train_loss: 0.016008\n",
      "[051/01073] train_loss: 0.015540\n",
      "[051/01123] train_loss: 0.015794\n",
      "[051/01173] train_loss: 0.015795\n",
      "[051/01223] train_loss: 0.016697\n",
      "[052/00047] train_loss: 0.019046\n",
      "[052/00097] train_loss: 0.017541\n",
      "[052/00147] train_loss: 0.018140\n",
      "[052/00197] train_loss: 0.016315\n",
      "[052/00247] train_loss: 0.016646\n",
      "[052/00297] train_loss: 0.017126\n",
      "[052/00347] train_loss: 0.016551\n",
      "[052/00397] train_loss: 0.015893\n",
      "[052/00447] train_loss: 0.016266\n",
      "[052/00497] train_loss: 0.016113\n",
      "[052/00547] train_loss: 0.016128\n",
      "[052/00597] train_loss: 0.015362\n",
      "[052/00647] train_loss: 0.016151\n",
      "[052/00697] train_loss: 0.016345\n",
      "[052/00747] train_loss: 0.015850\n",
      "[052/00797] train_loss: 0.016314\n",
      "[052/00847] train_loss: 0.016787\n",
      "[052/00897] train_loss: 0.016526\n",
      "[052/00947] train_loss: 0.016563\n",
      "[052/00997] train_loss: 0.015474\n",
      "[052/01047] train_loss: 0.016721\n",
      "[052/01097] train_loss: 0.016309\n",
      "[052/01147] train_loss: 0.016330\n",
      "[052/01197] train_loss: 0.015863\n",
      "[053/00021] train_loss: 0.018398\n",
      "[053/00071] train_loss: 0.018971\n",
      "[053/00121] train_loss: 0.017767\n",
      "[053/00171] train_loss: 0.018539\n",
      "[053/00221] train_loss: 0.016738\n",
      "[053/00271] train_loss: 0.016567\n",
      "[053/00321] train_loss: 0.016657\n",
      "[053/00371] train_loss: 0.016626\n",
      "[053/00421] train_loss: 0.016895\n",
      "[053/00471] train_loss: 0.017241\n",
      "[053/00521] train_loss: 0.016188\n",
      "[053/00571] train_loss: 0.016419\n",
      "[053/00621] train_loss: 0.016421\n",
      "[053/00671] train_loss: 0.016394\n",
      "[053/00721] train_loss: 0.016513\n",
      "[053/00771] train_loss: 0.015601\n",
      "[053/00821] train_loss: 0.016649\n",
      "[053/00871] train_loss: 0.017376\n",
      "[053/00921] train_loss: 0.015528\n",
      "[053/00971] train_loss: 0.016035\n",
      "[053/01021] train_loss: 0.016439\n",
      "[053/01071] train_loss: 0.015952\n",
      "[053/01121] train_loss: 0.016105\n",
      "[053/01171] train_loss: 0.015626\n",
      "[053/01221] train_loss: 0.015694\n",
      "[054/00045] train_loss: 0.018531\n",
      "[054/00095] train_loss: 0.018429\n",
      "[054/00145] train_loss: 0.016732\n",
      "[054/00195] train_loss: 0.017287\n",
      "[054/00245] train_loss: 0.017961\n",
      "[054/00295] train_loss: 0.016301\n",
      "[054/00345] train_loss: 0.016084\n",
      "[054/00395] train_loss: 0.016509\n",
      "[054/00445] train_loss: 0.016457\n",
      "[054/00495] train_loss: 0.015915\n",
      "[054/00545] train_loss: 0.016213\n",
      "[054/00595] train_loss: 0.015590\n",
      "[054/00645] train_loss: 0.015155\n",
      "[054/00695] train_loss: 0.017685\n",
      "[054/00745] train_loss: 0.017112\n",
      "[054/00795] train_loss: 0.016267\n",
      "[054/00845] train_loss: 0.016759\n",
      "[054/00895] train_loss: 0.015693\n",
      "[054/00945] train_loss: 0.015969\n",
      "[054/00995] train_loss: 0.016037\n",
      "[054/01045] train_loss: 0.016384\n",
      "[054/01095] train_loss: 0.015934\n",
      "[054/01145] train_loss: 0.016176\n",
      "[054/01195] train_loss: 0.016256\n",
      "[055/00019] train_loss: 0.017394\n",
      "[055/00069] train_loss: 0.018766\n",
      "[055/00119] train_loss: 0.018098\n",
      "[055/00169] train_loss: 0.017189\n",
      "[055/00219] train_loss: 0.017345\n",
      "[055/00269] train_loss: 0.016656\n",
      "[055/00319] train_loss: 0.016417\n",
      "[055/00369] train_loss: 0.017289\n",
      "[055/00419] train_loss: 0.015727\n",
      "[055/00469] train_loss: 0.016038\n",
      "[055/00519] train_loss: 0.016785\n",
      "[055/00569] train_loss: 0.015739\n",
      "[055/00619] train_loss: 0.015903\n",
      "[055/00669] train_loss: 0.017484\n",
      "[055/00719] train_loss: 0.015892\n",
      "[055/00769] train_loss: 0.016465\n",
      "[055/00819] train_loss: 0.016393\n",
      "[055/00869] train_loss: 0.016074\n",
      "[055/00919] train_loss: 0.016094\n",
      "[055/00969] train_loss: 0.016234\n",
      "[055/01019] train_loss: 0.016780\n",
      "[055/01069] train_loss: 0.015761\n",
      "[055/01119] train_loss: 0.015656\n",
      "[055/01169] train_loss: 0.016629\n",
      "[055/01219] train_loss: 0.015840\n",
      "[056/00043] train_loss: 0.019292\n",
      "[056/00093] train_loss: 0.017783\n",
      "[056/00143] train_loss: 0.017957\n",
      "[056/00193] train_loss: 0.017793\n",
      "[056/00243] train_loss: 0.016743\n",
      "[056/00293] train_loss: 0.016713\n",
      "[056/00343] train_loss: 0.016269\n",
      "[056/00393] train_loss: 0.015514\n",
      "[056/00443] train_loss: 0.016300\n",
      "[056/00493] train_loss: 0.015860\n",
      "[056/00543] train_loss: 0.016694\n",
      "[056/00593] train_loss: 0.016292\n",
      "[056/00643] train_loss: 0.015633\n",
      "[056/00693] train_loss: 0.015602\n",
      "[056/00743] train_loss: 0.015879\n",
      "[056/00793] train_loss: 0.016163\n",
      "[056/00843] train_loss: 0.015652\n",
      "[056/00893] train_loss: 0.016186\n",
      "[056/00943] train_loss: 0.015452\n",
      "[056/00993] train_loss: 0.015387\n",
      "[056/01043] train_loss: 0.015600\n",
      "[056/01093] train_loss: 0.015837\n",
      "[056/01143] train_loss: 0.015684\n",
      "[056/01193] train_loss: 0.016411\n",
      "[057/00017] train_loss: 0.017434\n",
      "[057/00067] train_loss: 0.018752\n",
      "[057/00117] train_loss: 0.018720\n",
      "[057/00167] train_loss: 0.017396\n",
      "[057/00217] train_loss: 0.017615\n",
      "[057/00267] train_loss: 0.017412\n",
      "[057/00317] train_loss: 0.016686\n",
      "[057/00367] train_loss: 0.016659\n",
      "[057/00417] train_loss: 0.016217\n",
      "[057/00467] train_loss: 0.015699\n",
      "[057/00517] train_loss: 0.015311\n",
      "[057/00567] train_loss: 0.015247\n",
      "[057/00617] train_loss: 0.016406\n",
      "[057/00667] train_loss: 0.016710\n",
      "[057/00717] train_loss: 0.016904\n",
      "[057/00767] train_loss: 0.015773\n",
      "[057/00817] train_loss: 0.014975\n",
      "[057/00867] train_loss: 0.015921\n",
      "[057/00917] train_loss: 0.017042\n",
      "[057/00967] train_loss: 0.016162\n",
      "[057/01017] train_loss: 0.015504\n",
      "[057/01067] train_loss: 0.016208\n",
      "[057/01117] train_loss: 0.015876\n",
      "[057/01167] train_loss: 0.015404\n",
      "[057/01217] train_loss: 0.016397\n",
      "[058/00041] train_loss: 0.018362\n",
      "[058/00091] train_loss: 0.017570\n",
      "[058/00141] train_loss: 0.016984\n",
      "[058/00191] train_loss: 0.015944\n",
      "[058/00241] train_loss: 0.017825\n",
      "[058/00291] train_loss: 0.016943\n",
      "[058/00341] train_loss: 0.016058\n",
      "[058/00391] train_loss: 0.017428\n",
      "[058/00441] train_loss: 0.016691\n",
      "[058/00491] train_loss: 0.015878\n",
      "[058/00541] train_loss: 0.015935\n",
      "[058/00591] train_loss: 0.015988\n",
      "[058/00641] train_loss: 0.016024\n",
      "[058/00691] train_loss: 0.015766\n",
      "[058/00741] train_loss: 0.015085\n",
      "[058/00791] train_loss: 0.016769\n",
      "[058/00841] train_loss: 0.015999\n",
      "[058/00891] train_loss: 0.015661\n",
      "[058/00941] train_loss: 0.015669\n",
      "[058/00991] train_loss: 0.016588\n",
      "[058/01041] train_loss: 0.016200\n",
      "[058/01091] train_loss: 0.015610\n",
      "[058/01141] train_loss: 0.015620\n",
      "[058/01191] train_loss: 0.015112\n",
      "[059/00015] train_loss: 0.016175\n",
      "[059/00065] train_loss: 0.018321\n",
      "[059/00115] train_loss: 0.017911\n",
      "[059/00165] train_loss: 0.016919\n",
      "[059/00215] train_loss: 0.017322\n",
      "[059/00265] train_loss: 0.017847\n",
      "[059/00315] train_loss: 0.016169\n",
      "[059/00365] train_loss: 0.016517\n",
      "[059/00415] train_loss: 0.015963\n",
      "[059/00465] train_loss: 0.016427\n",
      "[059/00515] train_loss: 0.016183\n",
      "[059/00565] train_loss: 0.016031\n",
      "[059/00615] train_loss: 0.015270\n",
      "[059/00665] train_loss: 0.015393\n",
      "[059/00715] train_loss: 0.016175\n",
      "[059/00765] train_loss: 0.016097\n",
      "[059/00815] train_loss: 0.016127\n",
      "[059/00865] train_loss: 0.016586\n",
      "[059/00915] train_loss: 0.016407\n",
      "[059/00965] train_loss: 0.016267\n",
      "[059/01015] train_loss: 0.015619\n",
      "[059/01065] train_loss: 0.016531\n",
      "[059/01115] train_loss: 0.015894\n",
      "[059/01165] train_loss: 0.015780\n",
      "[059/01215] train_loss: 0.016362\n",
      "[060/00039] train_loss: 0.017424\n",
      "[060/00089] train_loss: 0.018634\n",
      "[060/00139] train_loss: 0.017552\n",
      "[060/00189] train_loss: 0.017333\n",
      "[060/00239] train_loss: 0.016646\n",
      "[060/00289] train_loss: 0.016618\n",
      "[060/00339] train_loss: 0.016237\n",
      "[060/00389] train_loss: 0.015583\n",
      "[060/00439] train_loss: 0.016310\n",
      "[060/00489] train_loss: 0.015666\n",
      "[060/00539] train_loss: 0.015506\n",
      "[060/00589] train_loss: 0.016202\n",
      "[060/00639] train_loss: 0.016096\n",
      "[060/00689] train_loss: 0.015595\n",
      "[060/00739] train_loss: 0.015506\n",
      "[060/00789] train_loss: 0.014940\n",
      "[060/00839] train_loss: 0.015822\n",
      "[060/00889] train_loss: 0.015881\n",
      "[060/00939] train_loss: 0.015386\n",
      "[060/00989] train_loss: 0.015996\n",
      "[060/01039] train_loss: 0.015924\n",
      "[060/01089] train_loss: 0.016223\n",
      "[060/01139] train_loss: 0.015564\n",
      "[060/01189] train_loss: 0.016260\n",
      "[061/00013] train_loss: 0.016959\n",
      "[061/00063] train_loss: 0.017957\n",
      "[061/00113] train_loss: 0.017831\n",
      "[061/00163] train_loss: 0.017797\n",
      "[061/00213] train_loss: 0.016504\n",
      "[061/00263] train_loss: 0.016700\n",
      "[061/00313] train_loss: 0.017067\n",
      "[061/00363] train_loss: 0.016600\n",
      "[061/00413] train_loss: 0.016243\n",
      "[061/00463] train_loss: 0.015751\n",
      "[061/00513] train_loss: 0.015237\n",
      "[061/00563] train_loss: 0.017207\n",
      "[061/00613] train_loss: 0.015264\n",
      "[061/00663] train_loss: 0.015996\n",
      "[061/00713] train_loss: 0.015887\n",
      "[061/00763] train_loss: 0.016773\n",
      "[061/00813] train_loss: 0.016527\n",
      "[061/00863] train_loss: 0.015839\n",
      "[061/00913] train_loss: 0.015838\n",
      "[061/00963] train_loss: 0.015639\n",
      "[061/01013] train_loss: 0.015268\n",
      "[061/01063] train_loss: 0.014847\n",
      "[061/01113] train_loss: 0.014948\n",
      "[061/01163] train_loss: 0.015697\n",
      "[061/01213] train_loss: 0.015859\n",
      "[062/00037] train_loss: 0.018606\n",
      "[062/00087] train_loss: 0.016763\n",
      "[062/00137] train_loss: 0.016692\n",
      "[062/00187] train_loss: 0.016403\n",
      "[062/00237] train_loss: 0.017479\n",
      "[062/00287] train_loss: 0.017487\n",
      "[062/00337] train_loss: 0.016332\n",
      "[062/00387] train_loss: 0.016475\n",
      "[062/00437] train_loss: 0.015397\n",
      "[062/00487] train_loss: 0.015846\n",
      "[062/00537] train_loss: 0.016139\n",
      "[062/00587] train_loss: 0.015840\n",
      "[062/00637] train_loss: 0.016434\n",
      "[062/00687] train_loss: 0.015734\n",
      "[062/00737] train_loss: 0.016502\n",
      "[062/00787] train_loss: 0.015919\n",
      "[062/00837] train_loss: 0.015345\n",
      "[062/00887] train_loss: 0.015719\n",
      "[062/00937] train_loss: 0.015360\n",
      "[062/00987] train_loss: 0.015479\n",
      "[062/01037] train_loss: 0.015748\n",
      "[062/01087] train_loss: 0.016095\n",
      "[062/01137] train_loss: 0.016170\n",
      "[062/01187] train_loss: 0.015295\n",
      "[063/00011] train_loss: 0.015994\n",
      "[063/00061] train_loss: 0.018437\n",
      "[063/00111] train_loss: 0.017890\n",
      "[063/00161] train_loss: 0.016937\n",
      "[063/00211] train_loss: 0.016570\n",
      "[063/00261] train_loss: 0.017412\n",
      "[063/00311] train_loss: 0.016347\n",
      "[063/00361] train_loss: 0.016236\n",
      "[063/00411] train_loss: 0.016534\n",
      "[063/00461] train_loss: 0.015859\n",
      "[063/00511] train_loss: 0.016135\n",
      "[063/00561] train_loss: 0.016184\n",
      "[063/00611] train_loss: 0.015653\n",
      "[063/00661] train_loss: 0.014967\n",
      "[063/00711] train_loss: 0.016126\n",
      "[063/00761] train_loss: 0.016503\n",
      "[063/00811] train_loss: 0.015731\n",
      "[063/00861] train_loss: 0.015289\n",
      "[063/00911] train_loss: 0.015936\n",
      "[063/00961] train_loss: 0.015541\n",
      "[063/01011] train_loss: 0.015091\n",
      "[063/01061] train_loss: 0.015780\n",
      "[063/01111] train_loss: 0.016230\n",
      "[063/01161] train_loss: 0.016409\n",
      "[063/01211] train_loss: 0.016203\n",
      "[064/00035] train_loss: 0.016975\n",
      "[064/00085] train_loss: 0.018509\n",
      "[064/00135] train_loss: 0.017560\n",
      "[064/00185] train_loss: 0.016303\n",
      "[064/00235] train_loss: 0.016892\n",
      "[064/00285] train_loss: 0.015923\n",
      "[064/00335] train_loss: 0.015800\n",
      "[064/00385] train_loss: 0.016135\n",
      "[064/00435] train_loss: 0.014999\n",
      "[064/00485] train_loss: 0.016670\n",
      "[064/00535] train_loss: 0.014971\n",
      "[064/00585] train_loss: 0.015442\n",
      "[064/00635] train_loss: 0.016013\n",
      "[064/00685] train_loss: 0.015610\n",
      "[064/00735] train_loss: 0.015275\n",
      "[064/00785] train_loss: 0.016517\n",
      "[064/00835] train_loss: 0.015205\n",
      "[064/00885] train_loss: 0.015675\n",
      "[064/00935] train_loss: 0.015129\n",
      "[064/00985] train_loss: 0.015512\n",
      "[064/01035] train_loss: 0.015485\n",
      "[064/01085] train_loss: 0.014872\n",
      "[064/01135] train_loss: 0.016302\n",
      "[064/01185] train_loss: 0.016516\n",
      "[065/00009] train_loss: 0.016242\n",
      "[065/00059] train_loss: 0.018101\n",
      "[065/00109] train_loss: 0.017286\n",
      "[065/00159] train_loss: 0.016329\n",
      "[065/00209] train_loss: 0.016186\n",
      "[065/00259] train_loss: 0.016056\n",
      "[065/00309] train_loss: 0.015735\n",
      "[065/00359] train_loss: 0.016628\n",
      "[065/00409] train_loss: 0.016070\n",
      "[065/00459] train_loss: 0.015156\n",
      "[065/00509] train_loss: 0.015233\n",
      "[065/00559] train_loss: 0.015207\n",
      "[065/00609] train_loss: 0.016049\n",
      "[065/00659] train_loss: 0.015673\n",
      "[065/00709] train_loss: 0.015469\n",
      "[065/00759] train_loss: 0.016351\n",
      "[065/00809] train_loss: 0.015772\n",
      "[065/00859] train_loss: 0.016306\n",
      "[065/00909] train_loss: 0.015758\n",
      "[065/00959] train_loss: 0.015771\n",
      "[065/01009] train_loss: 0.016303\n",
      "[065/01059] train_loss: 0.016024\n",
      "[065/01109] train_loss: 0.014895\n",
      "[065/01159] train_loss: 0.015179\n",
      "[065/01209] train_loss: 0.015050\n",
      "[066/00033] train_loss: 0.016418\n",
      "[066/00083] train_loss: 0.018485\n",
      "[066/00133] train_loss: 0.017357\n",
      "[066/00183] train_loss: 0.016536\n",
      "[066/00233] train_loss: 0.016686\n",
      "[066/00283] train_loss: 0.017275\n",
      "[066/00333] train_loss: 0.016046\n",
      "[066/00383] train_loss: 0.015699\n",
      "[066/00433] train_loss: 0.015813\n",
      "[066/00483] train_loss: 0.015287\n",
      "[066/00533] train_loss: 0.015815\n",
      "[066/00583] train_loss: 0.014949\n",
      "[066/00633] train_loss: 0.015356\n",
      "[066/00683] train_loss: 0.015803\n",
      "[066/00733] train_loss: 0.016451\n",
      "[066/00783] train_loss: 0.015793\n",
      "[066/00833] train_loss: 0.015249\n",
      "[066/00883] train_loss: 0.015954\n",
      "[066/00933] train_loss: 0.016111\n",
      "[066/00983] train_loss: 0.015383\n",
      "[066/01033] train_loss: 0.016019\n",
      "[066/01083] train_loss: 0.015420\n",
      "[066/01133] train_loss: 0.016489\n",
      "[066/01183] train_loss: 0.015993\n",
      "[067/00007] train_loss: 0.016313\n",
      "[067/00057] train_loss: 0.017786\n",
      "[067/00107] train_loss: 0.017180\n",
      "[067/00157] train_loss: 0.017259\n",
      "[067/00207] train_loss: 0.016151\n",
      "[067/00257] train_loss: 0.015674\n",
      "[067/00307] train_loss: 0.016054\n",
      "[067/00357] train_loss: 0.015389\n",
      "[067/00407] train_loss: 0.016551\n",
      "[067/00457] train_loss: 0.015309\n",
      "[067/00507] train_loss: 0.015996\n",
      "[067/00557] train_loss: 0.015909\n",
      "[067/00607] train_loss: 0.016465\n",
      "[067/00657] train_loss: 0.016109\n",
      "[067/00707] train_loss: 0.015869\n",
      "[067/00757] train_loss: 0.014997\n",
      "[067/00807] train_loss: 0.015560\n",
      "[067/00857] train_loss: 0.016289\n",
      "[067/00907] train_loss: 0.016099\n",
      "[067/00957] train_loss: 0.015528\n",
      "[067/01007] train_loss: 0.015757\n",
      "[067/01057] train_loss: 0.015255\n",
      "[067/01107] train_loss: 0.015232\n",
      "[067/01157] train_loss: 0.015466\n",
      "[067/01207] train_loss: 0.016203\n",
      "[068/00031] train_loss: 0.016981\n",
      "[068/00081] train_loss: 0.017930\n",
      "[068/00131] train_loss: 0.017439\n",
      "[068/00181] train_loss: 0.017054\n",
      "[068/00231] train_loss: 0.015836\n",
      "[068/00281] train_loss: 0.016182\n",
      "[068/00331] train_loss: 0.016270\n",
      "[068/00381] train_loss: 0.016459\n",
      "[068/00431] train_loss: 0.015168\n",
      "[068/00481] train_loss: 0.015979\n",
      "[068/00531] train_loss: 0.015979\n",
      "[068/00581] train_loss: 0.015884\n",
      "[068/00631] train_loss: 0.015531\n",
      "[068/00681] train_loss: 0.016042\n",
      "[068/00731] train_loss: 0.014930\n",
      "[068/00781] train_loss: 0.016129\n",
      "[068/00831] train_loss: 0.015356\n",
      "[068/00881] train_loss: 0.015800\n",
      "[068/00931] train_loss: 0.015315\n",
      "[068/00981] train_loss: 0.015496\n",
      "[068/01031] train_loss: 0.014841\n",
      "[068/01081] train_loss: 0.015209\n",
      "[068/01131] train_loss: 0.015795\n",
      "[068/01181] train_loss: 0.015840\n",
      "[069/00005] train_loss: 0.015670\n",
      "[069/00055] train_loss: 0.017551\n",
      "[069/00105] train_loss: 0.017218\n",
      "[069/00155] train_loss: 0.017178\n",
      "[069/00205] train_loss: 0.016832\n",
      "[069/00255] train_loss: 0.016345\n",
      "[069/00305] train_loss: 0.016038\n",
      "[069/00355] train_loss: 0.015476\n",
      "[069/00405] train_loss: 0.015554\n",
      "[069/00455] train_loss: 0.016119\n",
      "[069/00505] train_loss: 0.015578\n",
      "[069/00555] train_loss: 0.016201\n",
      "[069/00605] train_loss: 0.016913\n",
      "[069/00655] train_loss: 0.015854\n",
      "[069/00705] train_loss: 0.014555\n",
      "[069/00755] train_loss: 0.015316\n",
      "[069/00805] train_loss: 0.015979\n",
      "[069/00855] train_loss: 0.015962\n",
      "[069/00905] train_loss: 0.015169\n",
      "[069/00955] train_loss: 0.015153\n",
      "[069/01005] train_loss: 0.015344\n",
      "[069/01055] train_loss: 0.015691\n",
      "[069/01105] train_loss: 0.015782\n",
      "[069/01155] train_loss: 0.015232\n",
      "[069/01205] train_loss: 0.015899\n",
      "[070/00029] train_loss: 0.016488\n",
      "[070/00079] train_loss: 0.017360\n",
      "[070/00129] train_loss: 0.017158\n",
      "[070/00179] train_loss: 0.016726\n",
      "[070/00229] train_loss: 0.016041\n",
      "[070/00279] train_loss: 0.015430\n",
      "[070/00329] train_loss: 0.016057\n",
      "[070/00379] train_loss: 0.016532\n",
      "[070/00429] train_loss: 0.016031\n",
      "[070/00479] train_loss: 0.015556\n",
      "[070/00529] train_loss: 0.016189\n",
      "[070/00579] train_loss: 0.015599\n",
      "[070/00629] train_loss: 0.015173\n",
      "[070/00679] train_loss: 0.016090\n",
      "[070/00729] train_loss: 0.015355\n",
      "[070/00779] train_loss: 0.016165\n",
      "[070/00829] train_loss: 0.015219\n",
      "[070/00879] train_loss: 0.015303\n",
      "[070/00929] train_loss: 0.015093\n",
      "[070/00979] train_loss: 0.016249\n",
      "[070/01029] train_loss: 0.014730\n",
      "[070/01079] train_loss: 0.015104\n",
      "[070/01129] train_loss: 0.015051\n",
      "[070/01179] train_loss: 0.015331\n",
      "[071/00003] train_loss: 0.015105\n",
      "[071/00053] train_loss: 0.017224\n",
      "[071/00103] train_loss: 0.016791\n",
      "[071/00153] train_loss: 0.016590\n",
      "[071/00203] train_loss: 0.017065\n",
      "[071/00253] train_loss: 0.016637\n",
      "[071/00303] train_loss: 0.015976\n",
      "[071/00353] train_loss: 0.016778\n",
      "[071/00403] train_loss: 0.015974\n",
      "[071/00453] train_loss: 0.015515\n",
      "[071/00503] train_loss: 0.015646\n",
      "[071/00553] train_loss: 0.015483\n",
      "[071/00603] train_loss: 0.016434\n",
      "[071/00653] train_loss: 0.015780\n",
      "[071/00703] train_loss: 0.014990\n",
      "[071/00753] train_loss: 0.015071\n",
      "[071/00803] train_loss: 0.016192\n",
      "[071/00853] train_loss: 0.014788\n",
      "[071/00903] train_loss: 0.015011\n",
      "[071/00953] train_loss: 0.014702\n",
      "[071/01003] train_loss: 0.015813\n",
      "[071/01053] train_loss: 0.014976\n",
      "[071/01103] train_loss: 0.015332\n",
      "[071/01153] train_loss: 0.015111\n",
      "[071/01203] train_loss: 0.016121\n",
      "[072/00027] train_loss: 0.016830\n",
      "[072/00077] train_loss: 0.017060\n",
      "[072/00127] train_loss: 0.016090\n",
      "[072/00177] train_loss: 0.016558\n",
      "[072/00227] train_loss: 0.016415\n",
      "[072/00277] train_loss: 0.015376\n",
      "[072/00327] train_loss: 0.016971\n",
      "[072/00377] train_loss: 0.016010\n",
      "[072/00427] train_loss: 0.016055\n",
      "[072/00477] train_loss: 0.015095\n",
      "[072/00527] train_loss: 0.016031\n",
      "[072/00577] train_loss: 0.015487\n",
      "[072/00627] train_loss: 0.014170\n",
      "[072/00677] train_loss: 0.015457\n",
      "[072/00727] train_loss: 0.015118\n",
      "[072/00777] train_loss: 0.015763\n",
      "[072/00827] train_loss: 0.015527\n",
      "[072/00877] train_loss: 0.015266\n",
      "[072/00927] train_loss: 0.016162\n",
      "[072/00977] train_loss: 0.015972\n",
      "[072/01027] train_loss: 0.015183\n",
      "[072/01077] train_loss: 0.015157\n",
      "[072/01127] train_loss: 0.014872\n",
      "[072/01177] train_loss: 0.015270\n",
      "[073/00001] train_loss: 0.015539\n",
      "[073/00051] train_loss: 0.017727\n",
      "[073/00101] train_loss: 0.016978\n",
      "[073/00151] train_loss: 0.016323\n",
      "[073/00201] train_loss: 0.016655\n",
      "[073/00251] train_loss: 0.015596\n",
      "[073/00301] train_loss: 0.017029\n",
      "[073/00351] train_loss: 0.015745\n",
      "[073/00401] train_loss: 0.016164\n",
      "[073/00451] train_loss: 0.015277\n",
      "[073/00501] train_loss: 0.015397\n",
      "[073/00551] train_loss: 0.014797\n",
      "[073/00601] train_loss: 0.016028\n",
      "[073/00651] train_loss: 0.015688\n",
      "[073/00701] train_loss: 0.014933\n",
      "[073/00751] train_loss: 0.015504\n",
      "[073/00801] train_loss: 0.015263\n",
      "[073/00851] train_loss: 0.014803\n",
      "[073/00901] train_loss: 0.014955\n",
      "[073/00951] train_loss: 0.014856\n",
      "[073/01001] train_loss: 0.015334\n",
      "[073/01051] train_loss: 0.015532\n",
      "[073/01101] train_loss: 0.014969\n",
      "[073/01151] train_loss: 0.015921\n",
      "[073/01201] train_loss: 0.015168\n",
      "[074/00025] train_loss: 0.017157\n",
      "[074/00075] train_loss: 0.017203\n",
      "[074/00125] train_loss: 0.016617\n",
      "[074/00175] train_loss: 0.016802\n",
      "[074/00225] train_loss: 0.016436\n",
      "[074/00275] train_loss: 0.016238\n",
      "[074/00325] train_loss: 0.016718\n",
      "[074/00375] train_loss: 0.016261\n",
      "[074/00425] train_loss: 0.016164\n",
      "[074/00475] train_loss: 0.014997\n",
      "[074/00525] train_loss: 0.015075\n",
      "[074/00575] train_loss: 0.015893\n",
      "[074/00625] train_loss: 0.015366\n",
      "[074/00675] train_loss: 0.014988\n",
      "[074/00725] train_loss: 0.015185\n",
      "[074/00775] train_loss: 0.015300\n",
      "[074/00825] train_loss: 0.014807\n",
      "[074/00875] train_loss: 0.014760\n",
      "[074/00925] train_loss: 0.014928\n",
      "[074/00975] train_loss: 0.015920\n",
      "[074/01025] train_loss: 0.015157\n",
      "[074/01075] train_loss: 0.015357\n",
      "[074/01125] train_loss: 0.015776\n",
      "[074/01175] train_loss: 0.014628\n",
      "[074/01225] train_loss: 0.014910\n",
      "[075/00049] train_loss: 0.016858\n",
      "[075/00099] train_loss: 0.017554\n",
      "[075/00149] train_loss: 0.017570\n",
      "[075/00199] train_loss: 0.016054\n",
      "[075/00249] train_loss: 0.016599\n",
      "[075/00299] train_loss: 0.014884\n",
      "[075/00349] train_loss: 0.015619\n",
      "[075/00399] train_loss: 0.015149\n",
      "[075/00449] train_loss: 0.015158\n",
      "[075/00499] train_loss: 0.015103\n",
      "[075/00549] train_loss: 0.015965\n",
      "[075/00599] train_loss: 0.015364\n",
      "[075/00649] train_loss: 0.015233\n",
      "[075/00699] train_loss: 0.015674\n",
      "[075/00749] train_loss: 0.015688\n",
      "[075/00799] train_loss: 0.014481\n",
      "[075/00849] train_loss: 0.015182\n",
      "[075/00899] train_loss: 0.015675\n",
      "[075/00949] train_loss: 0.015625\n",
      "[075/00999] train_loss: 0.015150\n",
      "[075/01049] train_loss: 0.014724\n",
      "[075/01099] train_loss: 0.014522\n",
      "[075/01149] train_loss: 0.015607\n",
      "[075/01199] train_loss: 0.014957\n",
      "[076/00023] train_loss: 0.015805\n",
      "[076/00073] train_loss: 0.016312\n",
      "[076/00123] train_loss: 0.016629\n",
      "[076/00173] train_loss: 0.016296\n",
      "[076/00223] train_loss: 0.016596\n",
      "[076/00273] train_loss: 0.016382\n",
      "[076/00323] train_loss: 0.016160\n",
      "[076/00373] train_loss: 0.015122\n",
      "[076/00423] train_loss: 0.015848\n",
      "[076/00473] train_loss: 0.015400\n",
      "[076/00523] train_loss: 0.015755\n",
      "[076/00573] train_loss: 0.015701\n",
      "[076/00623] train_loss: 0.016056\n",
      "[076/00673] train_loss: 0.015006\n",
      "[076/00723] train_loss: 0.014633\n",
      "[076/00773] train_loss: 0.015668\n",
      "[076/00823] train_loss: 0.015315\n",
      "[076/00873] train_loss: 0.014921\n",
      "[076/00923] train_loss: 0.016092\n",
      "[076/00973] train_loss: 0.015769\n",
      "[076/01023] train_loss: 0.015342\n",
      "[076/01073] train_loss: 0.014341\n",
      "[076/01123] train_loss: 0.013761\n",
      "[076/01173] train_loss: 0.014937\n",
      "[076/01223] train_loss: 0.015642\n",
      "[077/00047] train_loss: 0.016927\n",
      "[077/00097] train_loss: 0.017320\n",
      "[077/00147] train_loss: 0.016233\n",
      "[077/00197] train_loss: 0.015688\n",
      "[077/00247] train_loss: 0.015466\n",
      "[077/00297] train_loss: 0.015903\n",
      "[077/00347] train_loss: 0.015819\n",
      "[077/00397] train_loss: 0.015641\n",
      "[077/00447] train_loss: 0.016083\n",
      "[077/00497] train_loss: 0.015977\n",
      "[077/00547] train_loss: 0.015817\n",
      "[077/00597] train_loss: 0.015483\n",
      "[077/00647] train_loss: 0.014289\n",
      "[077/00697] train_loss: 0.015560\n",
      "[077/00747] train_loss: 0.015027\n",
      "[077/00797] train_loss: 0.015421\n",
      "[077/00847] train_loss: 0.015931\n",
      "[077/00897] train_loss: 0.016545\n",
      "[077/00947] train_loss: 0.014549\n",
      "[077/00997] train_loss: 0.014897\n",
      "[077/01047] train_loss: 0.016023\n",
      "[077/01097] train_loss: 0.015806\n",
      "[077/01147] train_loss: 0.014740\n",
      "[077/01197] train_loss: 0.015401\n",
      "[078/00021] train_loss: 0.016123\n",
      "[078/00071] train_loss: 0.016764\n",
      "[078/00121] train_loss: 0.016986\n",
      "[078/00171] train_loss: 0.016570\n",
      "[078/00221] train_loss: 0.016274\n",
      "[078/00271] train_loss: 0.016038\n",
      "[078/00321] train_loss: 0.015824\n",
      "[078/00371] train_loss: 0.016116\n",
      "[078/00421] train_loss: 0.015533\n",
      "[078/00471] train_loss: 0.014784\n",
      "[078/00521] train_loss: 0.014985\n",
      "[078/00571] train_loss: 0.015648\n",
      "[078/00621] train_loss: 0.015042\n",
      "[078/00671] train_loss: 0.015718\n",
      "[078/00721] train_loss: 0.015215\n",
      "[078/00771] train_loss: 0.015546\n",
      "[078/00821] train_loss: 0.014429\n",
      "[078/00871] train_loss: 0.015751\n",
      "[078/00921] train_loss: 0.014726\n",
      "[078/00971] train_loss: 0.014781\n",
      "[078/01021] train_loss: 0.014325\n",
      "[078/01071] train_loss: 0.015699\n",
      "[078/01121] train_loss: 0.014436\n",
      "[078/01171] train_loss: 0.015309\n",
      "[078/01221] train_loss: 0.014891\n",
      "[079/00045] train_loss: 0.016945\n",
      "[079/00095] train_loss: 0.017214\n",
      "[079/00145] train_loss: 0.015988\n",
      "[079/00195] train_loss: 0.016966\n",
      "[079/00245] train_loss: 0.015830\n",
      "[079/00295] train_loss: 0.015975\n",
      "[079/00345] train_loss: 0.015117\n",
      "[079/00395] train_loss: 0.015488\n",
      "[079/00445] train_loss: 0.016014\n",
      "[079/00495] train_loss: 0.015669\n",
      "[079/00545] train_loss: 0.015644\n",
      "[079/00595] train_loss: 0.015067\n",
      "[079/00645] train_loss: 0.015067\n",
      "[079/00695] train_loss: 0.015039\n",
      "[079/00745] train_loss: 0.015246\n",
      "[079/00795] train_loss: 0.015719\n",
      "[079/00845] train_loss: 0.015104\n",
      "[079/00895] train_loss: 0.015834\n",
      "[079/00945] train_loss: 0.014756\n",
      "[079/00995] train_loss: 0.014853\n",
      "[079/01045] train_loss: 0.015207\n",
      "[079/01095] train_loss: 0.015641\n",
      "[079/01145] train_loss: 0.014921\n",
      "[079/01195] train_loss: 0.014271\n",
      "[080/00019] train_loss: 0.015058\n",
      "[080/00069] train_loss: 0.016112\n",
      "[080/00119] train_loss: 0.015587\n",
      "[080/00169] train_loss: 0.015551\n",
      "[080/00219] train_loss: 0.016527\n",
      "[080/00269] train_loss: 0.016255\n",
      "[080/00319] train_loss: 0.015206\n",
      "[080/00369] train_loss: 0.016257\n",
      "[080/00419] train_loss: 0.016076\n",
      "[080/00469] train_loss: 0.014857\n",
      "[080/00519] train_loss: 0.015147\n",
      "[080/00569] train_loss: 0.015615\n",
      "[080/00619] train_loss: 0.015768\n",
      "[080/00669] train_loss: 0.015446\n",
      "[080/00719] train_loss: 0.014879\n",
      "[080/00769] train_loss: 0.015819\n",
      "[080/00819] train_loss: 0.014817\n",
      "[080/00869] train_loss: 0.015224\n",
      "[080/00919] train_loss: 0.015303\n",
      "[080/00969] train_loss: 0.015289\n",
      "[080/01019] train_loss: 0.014856\n",
      "[080/01069] train_loss: 0.015294\n",
      "[080/01119] train_loss: 0.014525\n",
      "[080/01169] train_loss: 0.015494\n",
      "[080/01219] train_loss: 0.014914\n",
      "[081/00043] train_loss: 0.017981\n",
      "[081/00093] train_loss: 0.015542\n",
      "[081/00143] train_loss: 0.017569\n",
      "[081/00193] train_loss: 0.016339\n",
      "[081/00243] train_loss: 0.015169\n",
      "[081/00293] train_loss: 0.015408\n",
      "[081/00343] train_loss: 0.015689\n",
      "[081/00393] train_loss: 0.015637\n",
      "[081/00443] train_loss: 0.015588\n",
      "[081/00493] train_loss: 0.015771\n",
      "[081/00543] train_loss: 0.014654\n",
      "[081/00593] train_loss: 0.015419\n",
      "[081/00643] train_loss: 0.015293\n",
      "[081/00693] train_loss: 0.015220\n",
      "[081/00743] train_loss: 0.016126\n",
      "[081/00793] train_loss: 0.015098\n",
      "[081/00843] train_loss: 0.014991\n",
      "[081/00893] train_loss: 0.015446\n",
      "[081/00943] train_loss: 0.015796\n",
      "[081/00993] train_loss: 0.014519\n",
      "[081/01043] train_loss: 0.014430\n",
      "[081/01093] train_loss: 0.015347\n",
      "[081/01143] train_loss: 0.015558\n",
      "[081/01193] train_loss: 0.014957\n",
      "[082/00017] train_loss: 0.015500\n",
      "[082/00067] train_loss: 0.016498\n",
      "[082/00117] train_loss: 0.015802\n",
      "[082/00167] train_loss: 0.015628\n",
      "[082/00217] train_loss: 0.015324\n",
      "[082/00267] train_loss: 0.015674\n",
      "[082/00317] train_loss: 0.015896\n",
      "[082/00367] train_loss: 0.014958\n",
      "[082/00417] train_loss: 0.015535\n",
      "[082/00467] train_loss: 0.015383\n",
      "[082/00517] train_loss: 0.015605\n",
      "[082/00567] train_loss: 0.015326\n",
      "[082/00617] train_loss: 0.014881\n",
      "[082/00667] train_loss: 0.014361\n",
      "[082/00717] train_loss: 0.015418\n",
      "[082/00767] train_loss: 0.013883\n",
      "[082/00817] train_loss: 0.015445\n",
      "[082/00867] train_loss: 0.015220\n",
      "[082/00917] train_loss: 0.014974\n",
      "[082/00967] train_loss: 0.016363\n",
      "[082/01017] train_loss: 0.015384\n",
      "[082/01067] train_loss: 0.014809\n",
      "[082/01117] train_loss: 0.015006\n",
      "[082/01167] train_loss: 0.015159\n",
      "[082/01217] train_loss: 0.014828\n",
      "[083/00041] train_loss: 0.017527\n",
      "[083/00091] train_loss: 0.017114\n",
      "[083/00141] train_loss: 0.015729\n",
      "[083/00191] train_loss: 0.016182\n",
      "[083/00241] train_loss: 0.016181\n",
      "[083/00291] train_loss: 0.016051\n",
      "[083/00341] train_loss: 0.016645\n",
      "[083/00391] train_loss: 0.015144\n",
      "[083/00441] train_loss: 0.015721\n",
      "[083/00491] train_loss: 0.015089\n",
      "[083/00541] train_loss: 0.015238\n",
      "[083/00591] train_loss: 0.014759\n",
      "[083/00641] train_loss: 0.014813\n",
      "[083/00691] train_loss: 0.015637\n",
      "[083/00741] train_loss: 0.014602\n",
      "[083/00791] train_loss: 0.014161\n",
      "[083/00841] train_loss: 0.015157\n",
      "[083/00891] train_loss: 0.014728\n",
      "[083/00941] train_loss: 0.015709\n",
      "[083/00991] train_loss: 0.015564\n",
      "[083/01041] train_loss: 0.014673\n",
      "[083/01091] train_loss: 0.015295\n",
      "[083/01141] train_loss: 0.014348\n",
      "[083/01191] train_loss: 0.014717\n",
      "[084/00015] train_loss: 0.016294\n",
      "[084/00065] train_loss: 0.017583\n",
      "[084/00115] train_loss: 0.016330\n",
      "[084/00165] train_loss: 0.016296\n",
      "[084/00215] train_loss: 0.015597\n",
      "[084/00265] train_loss: 0.015797\n",
      "[084/00315] train_loss: 0.015199\n",
      "[084/00365] train_loss: 0.014749\n",
      "[084/00415] train_loss: 0.015007\n",
      "[084/00465] train_loss: 0.015197\n",
      "[084/00515] train_loss: 0.015061\n",
      "[084/00565] train_loss: 0.014296\n",
      "[084/00615] train_loss: 0.016119\n",
      "[084/00665] train_loss: 0.015156\n",
      "[084/00715] train_loss: 0.014439\n",
      "[084/00765] train_loss: 0.015803\n",
      "[084/00815] train_loss: 0.014347\n",
      "[084/00865] train_loss: 0.015279\n",
      "[084/00915] train_loss: 0.014271\n",
      "[084/00965] train_loss: 0.015462\n",
      "[084/01015] train_loss: 0.015304\n",
      "[084/01065] train_loss: 0.014170\n",
      "[084/01115] train_loss: 0.014867\n",
      "[084/01165] train_loss: 0.014658\n",
      "[084/01215] train_loss: 0.015693\n",
      "[085/00039] train_loss: 0.016767\n",
      "[085/00089] train_loss: 0.016374\n",
      "[085/00139] train_loss: 0.017040\n",
      "[085/00189] train_loss: 0.015808\n",
      "[085/00239] train_loss: 0.015960\n",
      "[085/00289] train_loss: 0.016287\n",
      "[085/00339] train_loss: 0.015164\n",
      "[085/00389] train_loss: 0.014892\n",
      "[085/00439] train_loss: 0.015418\n",
      "[085/00489] train_loss: 0.015448\n",
      "[085/00539] train_loss: 0.015531\n",
      "[085/00589] train_loss: 0.016717\n",
      "[085/00639] train_loss: 0.015613\n",
      "[085/00689] train_loss: 0.014878\n",
      "[085/00739] train_loss: 0.015760\n",
      "[085/00789] train_loss: 0.015423\n",
      "[085/00839] train_loss: 0.014391\n",
      "[085/00889] train_loss: 0.015625\n",
      "[085/00939] train_loss: 0.015009\n",
      "[085/00989] train_loss: 0.014870\n",
      "[085/01039] train_loss: 0.014375\n",
      "[085/01089] train_loss: 0.015298\n",
      "[085/01139] train_loss: 0.015198\n",
      "[085/01189] train_loss: 0.014519\n",
      "[086/00013] train_loss: 0.015275\n",
      "[086/00063] train_loss: 0.016940\n",
      "[086/00113] train_loss: 0.015963\n",
      "[086/00163] train_loss: 0.015241\n",
      "[086/00213] train_loss: 0.016068\n",
      "[086/00263] train_loss: 0.016723\n",
      "[086/00313] train_loss: 0.016065\n",
      "[086/00363] train_loss: 0.014674\n",
      "[086/00413] train_loss: 0.015404\n",
      "[086/00463] train_loss: 0.015252\n",
      "[086/00513] train_loss: 0.014702\n",
      "[086/00563] train_loss: 0.015429\n",
      "[086/00613] train_loss: 0.014762\n",
      "[086/00663] train_loss: 0.014704\n",
      "[086/00713] train_loss: 0.014840\n",
      "[086/00763] train_loss: 0.014852\n",
      "[086/00813] train_loss: 0.015691\n",
      "[086/00863] train_loss: 0.015319\n",
      "[086/00913] train_loss: 0.014917\n",
      "[086/00963] train_loss: 0.014432\n",
      "[086/01013] train_loss: 0.014675\n",
      "[086/01063] train_loss: 0.015120\n",
      "[086/01113] train_loss: 0.015074\n",
      "[086/01163] train_loss: 0.015493\n",
      "[086/01213] train_loss: 0.014911\n",
      "[087/00037] train_loss: 0.016557\n",
      "[087/00087] train_loss: 0.017547\n",
      "[087/00137] train_loss: 0.015710\n",
      "[087/00187] train_loss: 0.015932\n",
      "[087/00237] train_loss: 0.016154\n",
      "[087/00287] train_loss: 0.015286\n",
      "[087/00337] train_loss: 0.015018\n",
      "[087/00387] train_loss: 0.015316\n",
      "[087/00437] train_loss: 0.015369\n",
      "[087/00487] train_loss: 0.015443\n",
      "[087/00537] train_loss: 0.014792\n",
      "[087/00587] train_loss: 0.014910\n",
      "[087/00637] train_loss: 0.014010\n",
      "[087/00687] train_loss: 0.014955\n",
      "[087/00737] train_loss: 0.015384\n",
      "[087/00787] train_loss: 0.016171\n",
      "[087/00837] train_loss: 0.014568\n",
      "[087/00887] train_loss: 0.014774\n",
      "[087/00937] train_loss: 0.014860\n",
      "[087/00987] train_loss: 0.015813\n",
      "[087/01037] train_loss: 0.014766\n",
      "[087/01087] train_loss: 0.015024\n",
      "[087/01137] train_loss: 0.015913\n",
      "[087/01187] train_loss: 0.014102\n",
      "[088/00011] train_loss: 0.015157\n",
      "[088/00061] train_loss: 0.016312\n",
      "[088/00111] train_loss: 0.015606\n",
      "[088/00161] train_loss: 0.016016\n",
      "[088/00211] train_loss: 0.016027\n",
      "[088/00261] train_loss: 0.015840\n",
      "[088/00311] train_loss: 0.015578\n",
      "[088/00361] train_loss: 0.015579\n",
      "[088/00411] train_loss: 0.015466\n",
      "[088/00461] train_loss: 0.015179\n",
      "[088/00511] train_loss: 0.014597\n",
      "[088/00561] train_loss: 0.015115\n",
      "[088/00611] train_loss: 0.015097\n",
      "[088/00661] train_loss: 0.014227\n",
      "[088/00711] train_loss: 0.015029\n",
      "[088/00761] train_loss: 0.015090\n",
      "[088/00811] train_loss: 0.015128\n",
      "[088/00861] train_loss: 0.014531\n",
      "[088/00911] train_loss: 0.015409\n",
      "[088/00961] train_loss: 0.014319\n",
      "[088/01011] train_loss: 0.014745\n",
      "[088/01061] train_loss: 0.014669\n",
      "[088/01111] train_loss: 0.014613\n",
      "[088/01161] train_loss: 0.016127\n",
      "[088/01211] train_loss: 0.015158\n",
      "[089/00035] train_loss: 0.016579\n",
      "[089/00085] train_loss: 0.016366\n",
      "[089/00135] train_loss: 0.016636\n",
      "[089/00185] train_loss: 0.015883\n",
      "[089/00235] train_loss: 0.015733\n",
      "[089/00285] train_loss: 0.015476\n",
      "[089/00335] train_loss: 0.014420\n",
      "[089/00385] train_loss: 0.014936\n",
      "[089/00435] train_loss: 0.014743\n",
      "[089/00485] train_loss: 0.014889\n",
      "[089/00535] train_loss: 0.014254\n",
      "[089/00585] train_loss: 0.014568\n",
      "[089/00635] train_loss: 0.014572\n",
      "[089/00685] train_loss: 0.015578\n",
      "[089/00735] train_loss: 0.014061\n",
      "[089/00785] train_loss: 0.016202\n",
      "[089/00835] train_loss: 0.015330\n",
      "[089/00885] train_loss: 0.014564\n",
      "[089/00935] train_loss: 0.015452\n",
      "[089/00985] train_loss: 0.014688\n",
      "[089/01035] train_loss: 0.015191\n",
      "[089/01085] train_loss: 0.015376\n",
      "[089/01135] train_loss: 0.014236\n",
      "[089/01185] train_loss: 0.015262\n",
      "[090/00009] train_loss: 0.016356\n",
      "[090/00059] train_loss: 0.016993\n",
      "[090/00109] train_loss: 0.016328\n",
      "[090/00159] train_loss: 0.015168\n",
      "[090/00209] train_loss: 0.016304\n",
      "[090/00259] train_loss: 0.015338\n",
      "[090/00309] train_loss: 0.015362\n",
      "[090/00359] train_loss: 0.014937\n",
      "[090/00409] train_loss: 0.016432\n",
      "[090/00459] train_loss: 0.014803\n",
      "[090/00509] train_loss: 0.015665\n",
      "[090/00559] train_loss: 0.014503\n",
      "[090/00609] train_loss: 0.015247\n",
      "[090/00659] train_loss: 0.014933\n",
      "[090/00709] train_loss: 0.014788\n",
      "[090/00759] train_loss: 0.014880\n",
      "[090/00809] train_loss: 0.015192\n",
      "[090/00859] train_loss: 0.014253\n",
      "[090/00909] train_loss: 0.015047\n",
      "[090/00959] train_loss: 0.014454\n",
      "[090/01009] train_loss: 0.013833\n",
      "[090/01059] train_loss: 0.014768\n",
      "[090/01109] train_loss: 0.015073\n",
      "[090/01159] train_loss: 0.016169\n",
      "[090/01209] train_loss: 0.015155\n",
      "[091/00033] train_loss: 0.017007\n",
      "[091/00083] train_loss: 0.016299\n",
      "[091/00133] train_loss: 0.016219\n",
      "[091/00183] train_loss: 0.015745\n",
      "[091/00233] train_loss: 0.015506\n",
      "[091/00283] train_loss: 0.015447\n",
      "[091/00333] train_loss: 0.015683\n",
      "[091/00383] train_loss: 0.014595\n",
      "[091/00433] train_loss: 0.015893\n",
      "[091/00483] train_loss: 0.015420\n",
      "[091/00533] train_loss: 0.015583\n",
      "[091/00583] train_loss: 0.014274\n",
      "[091/00633] train_loss: 0.014986\n",
      "[091/00683] train_loss: 0.015005\n",
      "[091/00733] train_loss: 0.014907\n",
      "[091/00783] train_loss: 0.014818\n",
      "[091/00833] train_loss: 0.014752\n",
      "[091/00883] train_loss: 0.013541\n",
      "[091/00933] train_loss: 0.014655\n",
      "[091/00983] train_loss: 0.014907\n",
      "[091/01033] train_loss: 0.013915\n",
      "[091/01083] train_loss: 0.014388\n",
      "[091/01133] train_loss: 0.014531\n",
      "[091/01183] train_loss: 0.014126\n",
      "[092/00007] train_loss: 0.015023\n",
      "[092/00057] train_loss: 0.017125\n",
      "[092/00107] train_loss: 0.015231\n",
      "[092/00157] train_loss: 0.016102\n",
      "[092/00207] train_loss: 0.015882\n",
      "[092/00257] train_loss: 0.014573\n",
      "[092/00307] train_loss: 0.015043\n",
      "[092/00357] train_loss: 0.015828\n",
      "[092/00407] train_loss: 0.015457\n",
      "[092/00457] train_loss: 0.014646\n",
      "[092/00507] train_loss: 0.014333\n",
      "[092/00557] train_loss: 0.015718\n",
      "[092/00607] train_loss: 0.015154\n",
      "[092/00657] train_loss: 0.014471\n",
      "[092/00707] train_loss: 0.014484\n",
      "[092/00757] train_loss: 0.014277\n",
      "[092/00807] train_loss: 0.014788\n",
      "[092/00857] train_loss: 0.014891\n",
      "[092/00907] train_loss: 0.014675\n",
      "[092/00957] train_loss: 0.015584\n",
      "[092/01007] train_loss: 0.014069\n",
      "[092/01057] train_loss: 0.014634\n",
      "[092/01107] train_loss: 0.015057\n",
      "[092/01157] train_loss: 0.014941\n",
      "[092/01207] train_loss: 0.014176\n",
      "[093/00031] train_loss: 0.016355\n",
      "[093/00081] train_loss: 0.016029\n",
      "[093/00131] train_loss: 0.016515\n",
      "[093/00181] train_loss: 0.016189\n",
      "[093/00231] train_loss: 0.014807\n",
      "[093/00281] train_loss: 0.014778\n",
      "[093/00331] train_loss: 0.015217\n",
      "[093/00381] train_loss: 0.015761\n",
      "[093/00431] train_loss: 0.015580\n",
      "[093/00481] train_loss: 0.015719\n",
      "[093/00531] train_loss: 0.014827\n",
      "[093/00581] train_loss: 0.014814\n",
      "[093/00631] train_loss: 0.014548\n",
      "[093/00681] train_loss: 0.014916\n",
      "[093/00731] train_loss: 0.014819\n",
      "[093/00781] train_loss: 0.014915\n",
      "[093/00831] train_loss: 0.014873\n",
      "[093/00881] train_loss: 0.015115\n",
      "[093/00931] train_loss: 0.014836\n",
      "[093/00981] train_loss: 0.014592\n",
      "[093/01031] train_loss: 0.015055\n",
      "[093/01081] train_loss: 0.014584\n",
      "[093/01131] train_loss: 0.014914\n",
      "[093/01181] train_loss: 0.015646\n",
      "[094/00005] train_loss: 0.015258\n",
      "[094/00055] train_loss: 0.017159\n",
      "[094/00105] train_loss: 0.015872\n",
      "[094/00155] train_loss: 0.015851\n",
      "[094/00205] train_loss: 0.015501\n",
      "[094/00255] train_loss: 0.015311\n",
      "[094/00305] train_loss: 0.015162\n",
      "[094/00355] train_loss: 0.014979\n",
      "[094/00405] train_loss: 0.015184\n",
      "[094/00455] train_loss: 0.015303\n",
      "[094/00505] train_loss: 0.014707\n",
      "[094/00555] train_loss: 0.014778\n",
      "[094/00605] train_loss: 0.015038\n",
      "[094/00655] train_loss: 0.014764\n",
      "[094/00705] train_loss: 0.014389\n",
      "[094/00755] train_loss: 0.015033\n",
      "[094/00805] train_loss: 0.015319\n",
      "[094/00855] train_loss: 0.014525\n",
      "[094/00905] train_loss: 0.014858\n",
      "[094/00955] train_loss: 0.015161\n",
      "[094/01005] train_loss: 0.014957\n",
      "[094/01055] train_loss: 0.015174\n",
      "[094/01105] train_loss: 0.015212\n",
      "[094/01155] train_loss: 0.014343\n",
      "[094/01205] train_loss: 0.015669\n",
      "[095/00029] train_loss: 0.016007\n",
      "[095/00079] train_loss: 0.016564\n",
      "[095/00129] train_loss: 0.015515\n",
      "[095/00179] train_loss: 0.016290\n",
      "[095/00229] train_loss: 0.015563\n",
      "[095/00279] train_loss: 0.015639\n",
      "[095/00329] train_loss: 0.014726\n",
      "[095/00379] train_loss: 0.015886\n",
      "[095/00429] train_loss: 0.015440\n",
      "[095/00479] train_loss: 0.015044\n",
      "[095/00529] train_loss: 0.015314\n",
      "[095/00579] train_loss: 0.016038\n",
      "[095/00629] train_loss: 0.014861\n",
      "[095/00679] train_loss: 0.014935\n",
      "[095/00729] train_loss: 0.014012\n",
      "[095/00779] train_loss: 0.015205\n",
      "[095/00829] train_loss: 0.014622\n",
      "[095/00879] train_loss: 0.014969\n",
      "[095/00929] train_loss: 0.014977\n",
      "[095/00979] train_loss: 0.014766\n",
      "[095/01029] train_loss: 0.015259\n",
      "[095/01079] train_loss: 0.014372\n",
      "[095/01129] train_loss: 0.015069\n",
      "[095/01179] train_loss: 0.014494\n",
      "[096/00003] train_loss: 0.014085\n",
      "[096/00053] train_loss: 0.016506\n",
      "[096/00103] train_loss: 0.016415\n",
      "[096/00153] train_loss: 0.015885\n",
      "[096/00203] train_loss: 0.015865\n",
      "[096/00253] train_loss: 0.015772\n",
      "[096/00303] train_loss: 0.015450\n",
      "[096/00353] train_loss: 0.015360\n",
      "[096/00403] train_loss: 0.014667\n",
      "[096/00453] train_loss: 0.014667\n",
      "[096/00503] train_loss: 0.014795\n",
      "[096/00553] train_loss: 0.014178\n",
      "[096/00603] train_loss: 0.014845\n",
      "[096/00653] train_loss: 0.015541\n",
      "[096/00703] train_loss: 0.014579\n",
      "[096/00753] train_loss: 0.014727\n",
      "[096/00803] train_loss: 0.014171\n",
      "[096/00853] train_loss: 0.014722\n",
      "[096/00903] train_loss: 0.014750\n",
      "[096/00953] train_loss: 0.014735\n",
      "[096/01003] train_loss: 0.014216\n",
      "[096/01053] train_loss: 0.014478\n",
      "[096/01103] train_loss: 0.014554\n",
      "[096/01153] train_loss: 0.014954\n",
      "[096/01203] train_loss: 0.014415\n",
      "[097/00027] train_loss: 0.016667\n",
      "[097/00077] train_loss: 0.016360\n",
      "[097/00127] train_loss: 0.016039\n",
      "[097/00177] train_loss: 0.015121\n",
      "[097/00227] train_loss: 0.015442\n",
      "[097/00277] train_loss: 0.015609\n",
      "[097/00327] train_loss: 0.015327\n",
      "[097/00377] train_loss: 0.014755\n",
      "[097/00427] train_loss: 0.015268\n",
      "[097/00477] train_loss: 0.015179\n",
      "[097/00527] train_loss: 0.015057\n",
      "[097/00577] train_loss: 0.014899\n",
      "[097/00627] train_loss: 0.015380\n",
      "[097/00677] train_loss: 0.014543\n",
      "[097/00727] train_loss: 0.015034\n",
      "[097/00777] train_loss: 0.015107\n",
      "[097/00827] train_loss: 0.014332\n",
      "[097/00877] train_loss: 0.014239\n",
      "[097/00927] train_loss: 0.014208\n",
      "[097/00977] train_loss: 0.014525\n",
      "[097/01027] train_loss: 0.015034\n",
      "[097/01077] train_loss: 0.014827\n",
      "[097/01127] train_loss: 0.015059\n",
      "[097/01177] train_loss: 0.014555\n",
      "[098/00001] train_loss: 0.014847\n",
      "[098/00051] train_loss: 0.016430\n",
      "[098/00101] train_loss: 0.016655\n",
      "[098/00151] train_loss: 0.014905\n",
      "[098/00201] train_loss: 0.016675\n",
      "[098/00251] train_loss: 0.015588\n",
      "[098/00301] train_loss: 0.015097\n",
      "[098/00351] train_loss: 0.014761\n",
      "[098/00401] train_loss: 0.014866\n",
      "[098/00451] train_loss: 0.015322\n",
      "[098/00501] train_loss: 0.015120\n",
      "[098/00551] train_loss: 0.014689\n",
      "[098/00601] train_loss: 0.015127\n",
      "[098/00651] train_loss: 0.015511\n",
      "[098/00701] train_loss: 0.015384\n",
      "[098/00751] train_loss: 0.014264\n",
      "[098/00801] train_loss: 0.015275\n",
      "[098/00851] train_loss: 0.015050\n",
      "[098/00901] train_loss: 0.013752\n",
      "[098/00951] train_loss: 0.014876\n",
      "[098/01001] train_loss: 0.013932\n",
      "[098/01051] train_loss: 0.014522\n",
      "[098/01101] train_loss: 0.014740\n",
      "[098/01151] train_loss: 0.014272\n",
      "[098/01201] train_loss: 0.014399\n",
      "[099/00025] train_loss: 0.015690\n",
      "[099/00075] train_loss: 0.016794\n",
      "[099/00125] train_loss: 0.016123\n",
      "[099/00175] train_loss: 0.015387\n",
      "[099/00225] train_loss: 0.015176\n",
      "[099/00275] train_loss: 0.015373\n",
      "[099/00325] train_loss: 0.014136\n",
      "[099/00375] train_loss: 0.014776\n",
      "[099/00425] train_loss: 0.015465\n",
      "[099/00475] train_loss: 0.015116\n",
      "[099/00525] train_loss: 0.014502\n",
      "[099/00575] train_loss: 0.014388\n",
      "[099/00625] train_loss: 0.014293\n",
      "[099/00675] train_loss: 0.014764\n",
      "[099/00725] train_loss: 0.014429\n",
      "[099/00775] train_loss: 0.014516\n",
      "[099/00825] train_loss: 0.014792\n",
      "[099/00875] train_loss: 0.015197\n",
      "[099/00925] train_loss: 0.014304\n",
      "[099/00975] train_loss: 0.015268\n",
      "[099/01025] train_loss: 0.014415\n",
      "[099/01075] train_loss: 0.014792\n",
      "[099/01125] train_loss: 0.014852\n",
      "[099/01175] train_loss: 0.014453\n",
      "[099/01225] train_loss: 0.014734\n",
      "[100/00049] train_loss: 0.016091\n",
      "[100/00099] train_loss: 0.016912\n",
      "[100/00149] train_loss: 0.014840\n",
      "[100/00199] train_loss: 0.015604\n",
      "[100/00249] train_loss: 0.014964\n",
      "[100/00299] train_loss: 0.016371\n",
      "[100/00349] train_loss: 0.014869\n",
      "[100/00399] train_loss: 0.014958\n",
      "[100/00449] train_loss: 0.014793\n",
      "[100/00499] train_loss: 0.015212\n",
      "[100/00549] train_loss: 0.014532\n",
      "[100/00599] train_loss: 0.014742\n",
      "[100/00649] train_loss: 0.013985\n",
      "[100/00699] train_loss: 0.014915\n",
      "[100/00749] train_loss: 0.015076\n",
      "[100/00799] train_loss: 0.015054\n",
      "[100/00849] train_loss: 0.015168\n",
      "[100/00899] train_loss: 0.014503\n",
      "[100/00949] train_loss: 0.015602\n",
      "[100/00999] train_loss: 0.014390\n",
      "[100/01049] train_loss: 0.013904\n",
      "[100/01099] train_loss: 0.014918\n",
      "[100/01149] train_loss: 0.014822\n",
      "[100/01199] train_loss: 0.014360\n",
      "[101/00023] train_loss: 0.015551\n",
      "[101/00073] train_loss: 0.015962\n",
      "[101/00123] train_loss: 0.016053\n",
      "[101/00173] train_loss: 0.016286\n",
      "[101/00223] train_loss: 0.016022\n",
      "[101/00273] train_loss: 0.015060\n",
      "[101/00323] train_loss: 0.015433\n",
      "[101/00373] train_loss: 0.015108\n",
      "[101/00423] train_loss: 0.015079\n",
      "[101/00473] train_loss: 0.014260\n",
      "[101/00523] train_loss: 0.014525\n",
      "[101/00573] train_loss: 0.015229\n",
      "[101/00623] train_loss: 0.014842\n",
      "[101/00673] train_loss: 0.014507\n",
      "[101/00723] train_loss: 0.014379\n",
      "[101/00773] train_loss: 0.014941\n",
      "[101/00823] train_loss: 0.014325\n",
      "[101/00873] train_loss: 0.015203\n",
      "[101/00923] train_loss: 0.014589\n",
      "[101/00973] train_loss: 0.015089\n",
      "[101/01023] train_loss: 0.014619\n",
      "[101/01073] train_loss: 0.014037\n",
      "[101/01123] train_loss: 0.014074\n",
      "[101/01173] train_loss: 0.014389\n",
      "[101/01223] train_loss: 0.015033\n",
      "[102/00047] train_loss: 0.017012\n",
      "[102/00097] train_loss: 0.016665\n",
      "[102/00147] train_loss: 0.015686\n",
      "[102/00197] train_loss: 0.016600\n",
      "[102/00247] train_loss: 0.015818\n",
      "[102/00297] train_loss: 0.015796\n",
      "[102/00347] train_loss: 0.015374\n",
      "[102/00397] train_loss: 0.014755\n",
      "[102/00447] train_loss: 0.015301\n",
      "[102/00497] train_loss: 0.014555\n",
      "[102/00547] train_loss: 0.015208\n",
      "[102/00597] train_loss: 0.014276\n",
      "[102/00647] train_loss: 0.013962\n",
      "[102/00697] train_loss: 0.014543\n",
      "[102/00747] train_loss: 0.014131\n",
      "[102/00797] train_loss: 0.014851\n",
      "[102/00847] train_loss: 0.014562\n",
      "[102/00897] train_loss: 0.015416\n",
      "[102/00947] train_loss: 0.014997\n",
      "[102/00997] train_loss: 0.014548\n",
      "[102/01047] train_loss: 0.015118\n",
      "[102/01097] train_loss: 0.014843\n",
      "[102/01147] train_loss: 0.015539\n",
      "[102/01197] train_loss: 0.015186\n",
      "[103/00021] train_loss: 0.015595\n",
      "[103/00071] train_loss: 0.015853\n",
      "[103/00121] train_loss: 0.015586\n",
      "[103/00171] train_loss: 0.016311\n",
      "[103/00221] train_loss: 0.015271\n",
      "[103/00271] train_loss: 0.015534\n",
      "[103/00321] train_loss: 0.015504\n",
      "[103/00371] train_loss: 0.015041\n",
      "[103/00421] train_loss: 0.015580\n",
      "[103/00471] train_loss: 0.014415\n",
      "[103/00521] train_loss: 0.014646\n",
      "[103/00571] train_loss: 0.014555\n",
      "[103/00621] train_loss: 0.015234\n",
      "[103/00671] train_loss: 0.014657\n",
      "[103/00721] train_loss: 0.014645\n",
      "[103/00771] train_loss: 0.015125\n",
      "[103/00821] train_loss: 0.014325\n",
      "[103/00871] train_loss: 0.014612\n",
      "[103/00921] train_loss: 0.014669\n",
      "[103/00971] train_loss: 0.014585\n",
      "[103/01021] train_loss: 0.014517\n",
      "[103/01071] train_loss: 0.014927\n",
      "[103/01121] train_loss: 0.015648\n",
      "[103/01171] train_loss: 0.014024\n",
      "[103/01221] train_loss: 0.014219\n",
      "[104/00045] train_loss: 0.015850\n",
      "[104/00095] train_loss: 0.016399\n",
      "[104/00145] train_loss: 0.015884\n",
      "[104/00195] train_loss: 0.015563\n",
      "[104/00245] train_loss: 0.015763\n",
      "[104/00295] train_loss: 0.015885\n",
      "[104/00345] train_loss: 0.015377\n",
      "[104/00395] train_loss: 0.015164\n",
      "[104/00445] train_loss: 0.015226\n",
      "[104/00495] train_loss: 0.014934\n",
      "[104/00545] train_loss: 0.014955\n",
      "[104/00595] train_loss: 0.014889\n",
      "[104/00645] train_loss: 0.014665\n",
      "[104/00695] train_loss: 0.014698\n",
      "[104/00745] train_loss: 0.014570\n",
      "[104/00795] train_loss: 0.015332\n",
      "[104/00845] train_loss: 0.014121\n",
      "[104/00895] train_loss: 0.015363\n",
      "[104/00945] train_loss: 0.014486\n",
      "[104/00995] train_loss: 0.014161\n",
      "[104/01045] train_loss: 0.014399\n",
      "[104/01095] train_loss: 0.014775\n",
      "[104/01145] train_loss: 0.014107\n",
      "[104/01195] train_loss: 0.014421\n",
      "[105/00019] train_loss: 0.014948\n",
      "[105/00069] train_loss: 0.016505\n",
      "[105/00119] train_loss: 0.015497\n",
      "[105/00169] train_loss: 0.015393\n",
      "[105/00219] train_loss: 0.015994\n",
      "[105/00269] train_loss: 0.015700\n",
      "[105/00319] train_loss: 0.015462\n",
      "[105/00369] train_loss: 0.014978\n",
      "[105/00419] train_loss: 0.015729\n",
      "[105/00469] train_loss: 0.014692\n",
      "[105/00519] train_loss: 0.014930\n",
      "[105/00569] train_loss: 0.015749\n",
      "[105/00619] train_loss: 0.014913\n",
      "[105/00669] train_loss: 0.014553\n",
      "[105/00719] train_loss: 0.014777\n",
      "[105/00769] train_loss: 0.014392\n",
      "[105/00819] train_loss: 0.014875\n",
      "[105/00869] train_loss: 0.014753\n",
      "[105/00919] train_loss: 0.014064\n",
      "[105/00969] train_loss: 0.014707\n",
      "[105/01019] train_loss: 0.014116\n",
      "[105/01069] train_loss: 0.014820\n",
      "[105/01119] train_loss: 0.014622\n",
      "[105/01169] train_loss: 0.014773\n",
      "[105/01219] train_loss: 0.014950\n",
      "[106/00043] train_loss: 0.015520\n",
      "[106/00093] train_loss: 0.016366\n",
      "[106/00143] train_loss: 0.015773\n",
      "[106/00193] train_loss: 0.015615\n",
      "[106/00243] train_loss: 0.015824\n",
      "[106/00293] train_loss: 0.014896\n",
      "[106/00343] train_loss: 0.014865\n",
      "[106/00393] train_loss: 0.015771\n",
      "[106/00443] train_loss: 0.015176\n",
      "[106/00493] train_loss: 0.013930\n",
      "[106/00543] train_loss: 0.015040\n",
      "[106/00593] train_loss: 0.014348\n",
      "[106/00643] train_loss: 0.014817\n",
      "[106/00693] train_loss: 0.014238\n",
      "[106/00743] train_loss: 0.014919\n",
      "[106/00793] train_loss: 0.014307\n",
      "[106/00843] train_loss: 0.014549\n",
      "[106/00893] train_loss: 0.014802\n",
      "[106/00943] train_loss: 0.014256\n",
      "[106/00993] train_loss: 0.014462\n",
      "[106/01043] train_loss: 0.014971\n",
      "[106/01093] train_loss: 0.013206\n",
      "[106/01143] train_loss: 0.014649\n",
      "[106/01193] train_loss: 0.014734\n",
      "[107/00017] train_loss: 0.015114\n",
      "[107/00067] train_loss: 0.016482\n",
      "[107/00117] train_loss: 0.016334\n",
      "[107/00167] train_loss: 0.015657\n",
      "[107/00217] train_loss: 0.015336\n",
      "[107/00267] train_loss: 0.015093\n",
      "[107/00317] train_loss: 0.015527\n",
      "[107/00367] train_loss: 0.014914\n",
      "[107/00417] train_loss: 0.015576\n",
      "[107/00467] train_loss: 0.015535\n",
      "[107/00517] train_loss: 0.015149\n",
      "[107/00567] train_loss: 0.014493\n",
      "[107/00617] train_loss: 0.014663\n",
      "[107/00667] train_loss: 0.014225\n",
      "[107/00717] train_loss: 0.013852\n",
      "[107/00767] train_loss: 0.014860\n",
      "[107/00817] train_loss: 0.014661\n",
      "[107/00867] train_loss: 0.014954\n",
      "[107/00917] train_loss: 0.015179\n",
      "[107/00967] train_loss: 0.013934\n",
      "[107/01017] train_loss: 0.013786\n",
      "[107/01067] train_loss: 0.013785\n",
      "[107/01117] train_loss: 0.014413\n",
      "[107/01167] train_loss: 0.014841\n",
      "[107/01217] train_loss: 0.014042\n",
      "[108/00041] train_loss: 0.015783\n",
      "[108/00091] train_loss: 0.016519\n",
      "[108/00141] train_loss: 0.015488\n",
      "[108/00191] train_loss: 0.015441\n",
      "[108/00241] train_loss: 0.015333\n",
      "[108/00291] train_loss: 0.014808\n",
      "[108/00341] train_loss: 0.015442\n",
      "[108/00391] train_loss: 0.014583\n",
      "[108/00441] train_loss: 0.015540\n",
      "[108/00491] train_loss: 0.015231\n",
      "[108/00541] train_loss: 0.014696\n",
      "[108/00591] train_loss: 0.014450\n",
      "[108/00641] train_loss: 0.014624\n",
      "[108/00691] train_loss: 0.013694\n",
      "[108/00741] train_loss: 0.014883\n",
      "[108/00791] train_loss: 0.014092\n",
      "[108/00841] train_loss: 0.014622\n",
      "[108/00891] train_loss: 0.014254\n",
      "[108/00941] train_loss: 0.013739\n",
      "[108/00991] train_loss: 0.014008\n",
      "[108/01041] train_loss: 0.014112\n",
      "[108/01091] train_loss: 0.015091\n",
      "[108/01141] train_loss: 0.014920\n",
      "[108/01191] train_loss: 0.014174\n",
      "[109/00015] train_loss: 0.014951\n",
      "[109/00065] train_loss: 0.016503\n",
      "[109/00115] train_loss: 0.015298\n",
      "[109/00165] train_loss: 0.015985\n",
      "[109/00215] train_loss: 0.016122\n",
      "[109/00265] train_loss: 0.015225\n",
      "[109/00315] train_loss: 0.013869\n",
      "[109/00365] train_loss: 0.014615\n",
      "[109/00415] train_loss: 0.015190\n",
      "[109/00465] train_loss: 0.014946\n",
      "[109/00515] train_loss: 0.014536\n",
      "[109/00565] train_loss: 0.014084\n",
      "[109/00615] train_loss: 0.015461\n",
      "[109/00665] train_loss: 0.014736\n",
      "[109/00715] train_loss: 0.014352\n",
      "[109/00765] train_loss: 0.015459\n",
      "[109/00815] train_loss: 0.014043\n",
      "[109/00865] train_loss: 0.014709\n",
      "[109/00915] train_loss: 0.013930\n",
      "[109/00965] train_loss: 0.015613\n",
      "[109/01015] train_loss: 0.014596\n",
      "[109/01065] train_loss: 0.014782\n",
      "[109/01115] train_loss: 0.013896\n",
      "[109/01165] train_loss: 0.014677\n",
      "[109/01215] train_loss: 0.014131\n",
      "[110/00039] train_loss: 0.015748\n",
      "[110/00089] train_loss: 0.015954\n",
      "[110/00139] train_loss: 0.015586\n",
      "[110/00189] train_loss: 0.015358\n",
      "[110/00239] train_loss: 0.015521\n",
      "[110/00289] train_loss: 0.015368\n",
      "[110/00339] train_loss: 0.014747\n",
      "[110/00389] train_loss: 0.013968\n",
      "[110/00439] train_loss: 0.015405\n",
      "[110/00489] train_loss: 0.014763\n",
      "[110/00539] train_loss: 0.014183\n",
      "[110/00589] train_loss: 0.014180\n",
      "[110/00639] train_loss: 0.014454\n",
      "[110/00689] train_loss: 0.014825\n",
      "[110/00739] train_loss: 0.014561\n",
      "[110/00789] train_loss: 0.013658\n",
      "[110/00839] train_loss: 0.014348\n",
      "[110/00889] train_loss: 0.015152\n",
      "[110/00939] train_loss: 0.014910\n",
      "[110/00989] train_loss: 0.014651\n",
      "[110/01039] train_loss: 0.014004\n",
      "[110/01089] train_loss: 0.014131\n",
      "[110/01139] train_loss: 0.015267\n",
      "[110/01189] train_loss: 0.014447\n",
      "[111/00013] train_loss: 0.014878\n",
      "[111/00063] train_loss: 0.015845\n",
      "[111/00113] train_loss: 0.016413\n",
      "[111/00163] train_loss: 0.015631\n",
      "[111/00213] train_loss: 0.015337\n",
      "[111/00263] train_loss: 0.015011\n",
      "[111/00313] train_loss: 0.014844\n",
      "[111/00363] train_loss: 0.014730\n",
      "[111/00413] train_loss: 0.014833\n",
      "[111/00463] train_loss: 0.015212\n",
      "[111/00513] train_loss: 0.014237\n",
      "[111/00563] train_loss: 0.015185\n",
      "[111/00613] train_loss: 0.015222\n",
      "[111/00663] train_loss: 0.014990\n",
      "[111/00713] train_loss: 0.015282\n",
      "[111/00763] train_loss: 0.014330\n",
      "[111/00813] train_loss: 0.013746\n",
      "[111/00863] train_loss: 0.014873\n",
      "[111/00913] train_loss: 0.014556\n",
      "[111/00963] train_loss: 0.014587\n",
      "[111/01013] train_loss: 0.013817\n",
      "[111/01063] train_loss: 0.014825\n",
      "[111/01113] train_loss: 0.014388\n",
      "[111/01163] train_loss: 0.013404\n",
      "[111/01213] train_loss: 0.014343\n",
      "[112/00037] train_loss: 0.015378\n",
      "[112/00087] train_loss: 0.016116\n",
      "[112/00137] train_loss: 0.015892\n",
      "[112/00187] train_loss: 0.015173\n",
      "[112/00237] train_loss: 0.015615\n",
      "[112/00287] train_loss: 0.015510\n",
      "[112/00337] train_loss: 0.015459\n",
      "[112/00387] train_loss: 0.015060\n",
      "[112/00437] train_loss: 0.014490\n",
      "[112/00487] train_loss: 0.014911\n",
      "[112/00537] train_loss: 0.014354\n",
      "[112/00587] train_loss: 0.014309\n",
      "[112/00637] train_loss: 0.014053\n",
      "[112/00687] train_loss: 0.014569\n",
      "[112/00737] train_loss: 0.013861\n",
      "[112/00787] train_loss: 0.014986\n",
      "[112/00837] train_loss: 0.014875\n",
      "[112/00887] train_loss: 0.014646\n",
      "[112/00937] train_loss: 0.014689\n",
      "[112/00987] train_loss: 0.014256\n",
      "[112/01037] train_loss: 0.014459\n",
      "[112/01087] train_loss: 0.014192\n",
      "[112/01137] train_loss: 0.014353\n",
      "[112/01187] train_loss: 0.014374\n",
      "[113/00011] train_loss: 0.015625\n",
      "[113/00061] train_loss: 0.015248\n",
      "[113/00111] train_loss: 0.015936\n",
      "[113/00161] train_loss: 0.016347\n",
      "[113/00211] train_loss: 0.015316\n",
      "[113/00261] train_loss: 0.015072\n",
      "[113/00311] train_loss: 0.014845\n",
      "[113/00361] train_loss: 0.014884\n",
      "[113/00411] train_loss: 0.014529\n",
      "[113/00461] train_loss: 0.014427\n",
      "[113/00511] train_loss: 0.014390\n",
      "[113/00561] train_loss: 0.014724\n",
      "[113/00611] train_loss: 0.014370\n",
      "[113/00661] train_loss: 0.014910\n",
      "[113/00711] train_loss: 0.013968\n",
      "[113/00761] train_loss: 0.014463\n",
      "[113/00811] train_loss: 0.014543\n",
      "[113/00861] train_loss: 0.015267\n",
      "[113/00911] train_loss: 0.014705\n",
      "[113/00961] train_loss: 0.014299\n",
      "[113/01011] train_loss: 0.013852\n",
      "[113/01061] train_loss: 0.014628\n",
      "[113/01111] train_loss: 0.014373\n",
      "[113/01161] train_loss: 0.014081\n",
      "[113/01211] train_loss: 0.014464\n",
      "[114/00035] train_loss: 0.015819\n",
      "[114/00085] train_loss: 0.015645\n",
      "[114/00135] train_loss: 0.014759\n",
      "[114/00185] train_loss: 0.016242\n",
      "[114/00235] train_loss: 0.015101\n",
      "[114/00285] train_loss: 0.015295\n",
      "[114/00335] train_loss: 0.014852\n",
      "[114/00385] train_loss: 0.014717\n",
      "[114/00435] train_loss: 0.014942\n",
      "[114/00485] train_loss: 0.014352\n",
      "[114/00535] train_loss: 0.014330\n",
      "[114/00585] train_loss: 0.014490\n",
      "[114/00635] train_loss: 0.014454\n",
      "[114/00685] train_loss: 0.013698\n",
      "[114/00735] train_loss: 0.014593\n",
      "[114/00785] train_loss: 0.015279\n",
      "[114/00835] train_loss: 0.013478\n",
      "[114/00885] train_loss: 0.015169\n",
      "[114/00935] train_loss: 0.013871\n",
      "[114/00985] train_loss: 0.014545\n",
      "[114/01035] train_loss: 0.014290\n",
      "[114/01085] train_loss: 0.015413\n",
      "[114/01135] train_loss: 0.013847\n",
      "[114/01185] train_loss: 0.014095\n",
      "[115/00009] train_loss: 0.015101\n",
      "[115/00059] train_loss: 0.016535\n",
      "[115/00109] train_loss: 0.016012\n",
      "[115/00159] train_loss: 0.015252\n",
      "[115/00209] train_loss: 0.015089\n",
      "[115/00259] train_loss: 0.014096\n",
      "[115/00309] train_loss: 0.014622\n",
      "[115/00359] train_loss: 0.014467\n",
      "[115/00409] train_loss: 0.014633\n",
      "[115/00459] train_loss: 0.014433\n",
      "[115/00509] train_loss: 0.014852\n",
      "[115/00559] train_loss: 0.014358\n",
      "[115/00609] train_loss: 0.014928\n",
      "[115/00659] train_loss: 0.014287\n",
      "[115/00709] train_loss: 0.014443\n",
      "[115/00759] train_loss: 0.014684\n",
      "[115/00809] train_loss: 0.015520\n",
      "[115/00859] train_loss: 0.014740\n",
      "[115/00909] train_loss: 0.014572\n",
      "[115/00959] train_loss: 0.014406\n",
      "[115/01009] train_loss: 0.014122\n",
      "[115/01059] train_loss: 0.014085\n",
      "[115/01109] train_loss: 0.013412\n",
      "[115/01159] train_loss: 0.014801\n",
      "[115/01209] train_loss: 0.013860\n",
      "[116/00033] train_loss: 0.015965\n",
      "[116/00083] train_loss: 0.015800\n",
      "[116/00133] train_loss: 0.015655\n",
      "[116/00183] train_loss: 0.015362\n",
      "[116/00233] train_loss: 0.015354\n",
      "[116/00283] train_loss: 0.015174\n",
      "[116/00333] train_loss: 0.015356\n",
      "[116/00383] train_loss: 0.015374\n",
      "[116/00433] train_loss: 0.014573\n",
      "[116/00483] train_loss: 0.014827\n",
      "[116/00533] train_loss: 0.014181\n",
      "[116/00583] train_loss: 0.013833\n",
      "[116/00633] train_loss: 0.014312\n",
      "[116/00683] train_loss: 0.014102\n",
      "[116/00733] train_loss: 0.015040\n",
      "[116/00783] train_loss: 0.014328\n",
      "[116/00833] train_loss: 0.014080\n",
      "[116/00883] train_loss: 0.014294\n",
      "[116/00933] train_loss: 0.014980\n",
      "[116/00983] train_loss: 0.014112\n",
      "[116/01033] train_loss: 0.014337\n",
      "[116/01083] train_loss: 0.014849\n",
      "[116/01133] train_loss: 0.014292\n",
      "[116/01183] train_loss: 0.014874\n",
      "[117/00007] train_loss: 0.014853\n",
      "[117/00057] train_loss: 0.017275\n",
      "[117/00107] train_loss: 0.016223\n",
      "[117/00157] train_loss: 0.015559\n",
      "[117/00207] train_loss: 0.015029\n",
      "[117/00257] train_loss: 0.015601\n",
      "[117/00307] train_loss: 0.014947\n",
      "[117/00357] train_loss: 0.014541\n",
      "[117/00407] train_loss: 0.016025\n",
      "[117/00457] train_loss: 0.014535\n",
      "[117/00507] train_loss: 0.014025\n",
      "[117/00557] train_loss: 0.013889\n",
      "[117/00607] train_loss: 0.015219\n",
      "[117/00657] train_loss: 0.014731\n",
      "[117/00707] train_loss: 0.013823\n",
      "[117/00757] train_loss: 0.014099\n",
      "[117/00807] train_loss: 0.013688\n",
      "[117/00857] train_loss: 0.014721\n",
      "[117/00907] train_loss: 0.014015\n",
      "[117/00957] train_loss: 0.013252\n",
      "[117/01007] train_loss: 0.013946\n",
      "[117/01057] train_loss: 0.013843\n",
      "[117/01107] train_loss: 0.014316\n",
      "[117/01157] train_loss: 0.014730\n",
      "[117/01207] train_loss: 0.013879\n",
      "[118/00031] train_loss: 0.015954\n",
      "[118/00081] train_loss: 0.016114\n",
      "[118/00131] train_loss: 0.015336\n",
      "[118/00181] train_loss: 0.015152\n",
      "[118/00231] train_loss: 0.015415\n",
      "[118/00281] train_loss: 0.014644\n",
      "[118/00331] train_loss: 0.014624\n",
      "[118/00381] train_loss: 0.014188\n",
      "[118/00431] train_loss: 0.014265\n",
      "[118/00481] train_loss: 0.014308\n",
      "[118/00531] train_loss: 0.014648\n",
      "[118/00581] train_loss: 0.015263\n",
      "[118/00631] train_loss: 0.014312\n",
      "[118/00681] train_loss: 0.014210\n",
      "[118/00731] train_loss: 0.014440\n",
      "[118/00781] train_loss: 0.014225\n",
      "[118/00831] train_loss: 0.013955\n",
      "[118/00881] train_loss: 0.013935\n",
      "[118/00931] train_loss: 0.014361\n",
      "[118/00981] train_loss: 0.015423\n",
      "[118/01031] train_loss: 0.014756\n",
      "[118/01081] train_loss: 0.013695\n",
      "[118/01131] train_loss: 0.014022\n",
      "[118/01181] train_loss: 0.014430\n",
      "[119/00005] train_loss: 0.015164\n",
      "[119/00055] train_loss: 0.016111\n",
      "[119/00105] train_loss: 0.014833\n",
      "[119/00155] train_loss: 0.015345\n",
      "[119/00205] train_loss: 0.015016\n",
      "[119/00255] train_loss: 0.014296\n",
      "[119/00305] train_loss: 0.014975\n",
      "[119/00355] train_loss: 0.014081\n",
      "[119/00405] train_loss: 0.014748\n",
      "[119/00455] train_loss: 0.014131\n",
      "[119/00505] train_loss: 0.013905\n",
      "[119/00555] train_loss: 0.013893\n",
      "[119/00605] train_loss: 0.014735\n",
      "[119/00655] train_loss: 0.014183\n",
      "[119/00705] train_loss: 0.013797\n",
      "[119/00755] train_loss: 0.014735\n",
      "[119/00805] train_loss: 0.014142\n",
      "[119/00855] train_loss: 0.014269\n",
      "[119/00905] train_loss: 0.014644\n",
      "[119/00955] train_loss: 0.015015\n",
      "[119/01005] train_loss: 0.015038\n",
      "[119/01055] train_loss: 0.014629\n",
      "[119/01105] train_loss: 0.014010\n",
      "[119/01155] train_loss: 0.014516\n",
      "[119/01205] train_loss: 0.016397\n",
      "[120/00029] train_loss: 0.016051\n",
      "[120/00079] train_loss: 0.015554\n",
      "[120/00129] train_loss: 0.015770\n",
      "[120/00179] train_loss: 0.014586\n",
      "[120/00229] train_loss: 0.015756\n",
      "[120/00279] train_loss: 0.014897\n",
      "[120/00329] train_loss: 0.015061\n",
      "[120/00379] train_loss: 0.014534\n",
      "[120/00429] train_loss: 0.015349\n",
      "[120/00479] train_loss: 0.014439\n",
      "[120/00529] train_loss: 0.015188\n",
      "[120/00579] train_loss: 0.013900\n",
      "[120/00629] train_loss: 0.014716\n",
      "[120/00679] train_loss: 0.014135\n",
      "[120/00729] train_loss: 0.015162\n",
      "[120/00779] train_loss: 0.013727\n",
      "[120/00829] train_loss: 0.013901\n",
      "[120/00879] train_loss: 0.014492\n",
      "[120/00929] train_loss: 0.013510\n",
      "[120/00979] train_loss: 0.014143\n",
      "[120/01029] train_loss: 0.014365\n",
      "[120/01079] train_loss: 0.014201\n",
      "[120/01129] train_loss: 0.014385\n",
      "[120/01179] train_loss: 0.014764\n",
      "[121/00003] train_loss: 0.014345\n",
      "[121/00053] train_loss: 0.015905\n",
      "[121/00103] train_loss: 0.015328\n",
      "[121/00153] train_loss: 0.015125\n",
      "[121/00203] train_loss: 0.015793\n",
      "[121/00253] train_loss: 0.014841\n",
      "[121/00303] train_loss: 0.014700\n",
      "[121/00353] train_loss: 0.014870\n",
      "[121/00403] train_loss: 0.015687\n",
      "[121/00453] train_loss: 0.014812\n",
      "[121/00503] train_loss: 0.013450\n",
      "[121/00553] train_loss: 0.014664\n",
      "[121/00603] train_loss: 0.014359\n",
      "[121/00653] train_loss: 0.015264\n",
      "[121/00703] train_loss: 0.014118\n",
      "[121/00753] train_loss: 0.013329\n",
      "[121/00803] train_loss: 0.013742\n",
      "[121/00853] train_loss: 0.013563\n",
      "[121/00903] train_loss: 0.014293\n",
      "[121/00953] train_loss: 0.014127\n",
      "[121/01003] train_loss: 0.013837\n",
      "[121/01053] train_loss: 0.014077\n",
      "[121/01103] train_loss: 0.014916\n",
      "[121/01153] train_loss: 0.014137\n",
      "[121/01203] train_loss: 0.014354\n",
      "[122/00027] train_loss: 0.015453\n",
      "[122/00077] train_loss: 0.015241\n",
      "[122/00127] train_loss: 0.015377\n",
      "[122/00177] train_loss: 0.015163\n",
      "[122/00227] train_loss: 0.015181\n",
      "[122/00277] train_loss: 0.014556\n",
      "[122/00327] train_loss: 0.015525\n",
      "[122/00377] train_loss: 0.014344\n",
      "[122/00427] train_loss: 0.014384\n",
      "[122/00477] train_loss: 0.014767\n",
      "[122/00527] train_loss: 0.014799\n",
      "[122/00577] train_loss: 0.014664\n",
      "[122/00627] train_loss: 0.015143\n",
      "[122/00677] train_loss: 0.014133\n",
      "[122/00727] train_loss: 0.014701\n",
      "[122/00777] train_loss: 0.014400\n",
      "[122/00827] train_loss: 0.014071\n",
      "[122/00877] train_loss: 0.014712\n",
      "[122/00927] train_loss: 0.014028\n",
      "[122/00977] train_loss: 0.014481\n",
      "[122/01027] train_loss: 0.013692\n",
      "[122/01077] train_loss: 0.014126\n",
      "[122/01127] train_loss: 0.014180\n",
      "[122/01177] train_loss: 0.013771\n",
      "[123/00001] train_loss: 0.014552\n",
      "[123/00051] train_loss: 0.016751\n",
      "[123/00101] train_loss: 0.015373\n",
      "[123/00151] train_loss: 0.016301\n",
      "[123/00201] train_loss: 0.014811\n",
      "[123/00251] train_loss: 0.014929\n",
      "[123/00301] train_loss: 0.014943\n",
      "[123/00351] train_loss: 0.014925\n",
      "[123/00401] train_loss: 0.015212\n",
      "[123/00451] train_loss: 0.013920\n",
      "[123/00501] train_loss: 0.014789\n",
      "[123/00551] train_loss: 0.014973\n",
      "[123/00601] train_loss: 0.014954\n",
      "[123/00651] train_loss: 0.013753\n",
      "[123/00701] train_loss: 0.014453\n",
      "[123/00751] train_loss: 0.014168\n",
      "[123/00801] train_loss: 0.014488\n",
      "[123/00851] train_loss: 0.014194\n",
      "[123/00901] train_loss: 0.013805\n",
      "[123/00951] train_loss: 0.014982\n",
      "[123/01001] train_loss: 0.014932\n",
      "[123/01051] train_loss: 0.014532\n",
      "[123/01101] train_loss: 0.013940\n",
      "[123/01151] train_loss: 0.014611\n",
      "[123/01201] train_loss: 0.014319\n",
      "[124/00025] train_loss: 0.014881\n",
      "[124/00075] train_loss: 0.015872\n",
      "[124/00125] train_loss: 0.015573\n",
      "[124/00175] train_loss: 0.015401\n",
      "[124/00225] train_loss: 0.014404\n",
      "[124/00275] train_loss: 0.015313\n",
      "[124/00325] train_loss: 0.014821\n",
      "[124/00375] train_loss: 0.014431\n",
      "[124/00425] train_loss: 0.014343\n",
      "[124/00475] train_loss: 0.014484\n",
      "[124/00525] train_loss: 0.014396\n",
      "[124/00575] train_loss: 0.013892\n",
      "[124/00625] train_loss: 0.015436\n",
      "[124/00675] train_loss: 0.014556\n",
      "[124/00725] train_loss: 0.014264\n",
      "[124/00775] train_loss: 0.014589\n",
      "[124/00825] train_loss: 0.013370\n",
      "[124/00875] train_loss: 0.013588\n",
      "[124/00925] train_loss: 0.014625\n",
      "[124/00975] train_loss: 0.013440\n",
      "[124/01025] train_loss: 0.013931\n",
      "[124/01075] train_loss: 0.013662\n",
      "[124/01125] train_loss: 0.014330\n",
      "[124/01175] train_loss: 0.014471\n",
      "[124/01225] train_loss: 0.014314\n",
      "[125/00049] train_loss: 0.016150\n",
      "[125/00099] train_loss: 0.015328\n",
      "[125/00149] train_loss: 0.014985\n",
      "[125/00199] train_loss: 0.014608\n",
      "[125/00249] train_loss: 0.015362\n",
      "[125/00299] train_loss: 0.014823\n",
      "[125/00349] train_loss: 0.015293\n",
      "[125/00399] train_loss: 0.014495\n",
      "[125/00449] train_loss: 0.015045\n",
      "[125/00499] train_loss: 0.014754\n",
      "[125/00549] train_loss: 0.014394\n",
      "[125/00599] train_loss: 0.014337\n",
      "[125/00649] train_loss: 0.013838\n",
      "[125/00699] train_loss: 0.014852\n",
      "[125/00749] train_loss: 0.013646\n",
      "[125/00799] train_loss: 0.014150\n",
      "[125/00849] train_loss: 0.013849\n",
      "[125/00899] train_loss: 0.014827\n",
      "[125/00949] train_loss: 0.014313\n",
      "[125/00999] train_loss: 0.014826\n",
      "[125/01049] train_loss: 0.014160\n",
      "[125/01099] train_loss: 0.014405\n",
      "[125/01149] train_loss: 0.013935\n",
      "[125/01199] train_loss: 0.014430\n",
      "[126/00023] train_loss: 0.015364\n",
      "[126/00073] train_loss: 0.015900\n",
      "[126/00123] train_loss: 0.015508\n",
      "[126/00173] train_loss: 0.015452\n",
      "[126/00223] train_loss: 0.014356\n",
      "[126/00273] train_loss: 0.015661\n",
      "[126/00323] train_loss: 0.014714\n",
      "[126/00373] train_loss: 0.015103\n",
      "[126/00423] train_loss: 0.014092\n",
      "[126/00473] train_loss: 0.014362\n",
      "[126/00523] train_loss: 0.014616\n",
      "[126/00573] train_loss: 0.013991\n",
      "[126/00623] train_loss: 0.015049\n",
      "[126/00673] train_loss: 0.014228\n",
      "[126/00723] train_loss: 0.014842\n",
      "[126/00773] train_loss: 0.014320\n",
      "[126/00823] train_loss: 0.013659\n",
      "[126/00873] train_loss: 0.014652\n",
      "[126/00923] train_loss: 0.014420\n",
      "[126/00973] train_loss: 0.013246\n",
      "[126/01023] train_loss: 0.013938\n",
      "[126/01073] train_loss: 0.014659\n",
      "[126/01123] train_loss: 0.013649\n",
      "[126/01173] train_loss: 0.014409\n",
      "[126/01223] train_loss: 0.014367\n",
      "[127/00047] train_loss: 0.015497\n",
      "[127/00097] train_loss: 0.015397\n",
      "[127/00147] train_loss: 0.014970\n",
      "[127/00197] train_loss: 0.015285\n",
      "[127/00247] train_loss: 0.014534\n",
      "[127/00297] train_loss: 0.014833\n",
      "[127/00347] train_loss: 0.015262\n",
      "[127/00397] train_loss: 0.014763\n",
      "[127/00447] train_loss: 0.014349\n",
      "[127/00497] train_loss: 0.014822\n",
      "[127/00547] train_loss: 0.014233\n",
      "[127/00597] train_loss: 0.014608\n",
      "[127/00647] train_loss: 0.014494\n",
      "[127/00697] train_loss: 0.014065\n",
      "[127/00747] train_loss: 0.013964\n",
      "[127/00797] train_loss: 0.013961\n",
      "[127/00847] train_loss: 0.013522\n",
      "[127/00897] train_loss: 0.014438\n",
      "[127/00947] train_loss: 0.013744\n",
      "[127/00997] train_loss: 0.014364\n",
      "[127/01047] train_loss: 0.014069\n",
      "[127/01097] train_loss: 0.013900\n",
      "[127/01147] train_loss: 0.014801\n",
      "[127/01197] train_loss: 0.014283\n",
      "[128/00021] train_loss: 0.015759\n",
      "[128/00071] train_loss: 0.015817\n",
      "[128/00121] train_loss: 0.015357\n",
      "[128/00171] train_loss: 0.015567\n",
      "[128/00221] train_loss: 0.015362\n",
      "[128/00271] train_loss: 0.014254\n",
      "[128/00321] train_loss: 0.014201\n",
      "[128/00371] train_loss: 0.014991\n",
      "[128/00421] train_loss: 0.014008\n",
      "[128/00471] train_loss: 0.014120\n",
      "[128/00521] train_loss: 0.014341\n",
      "[128/00571] train_loss: 0.014428\n",
      "[128/00621] train_loss: 0.014307\n",
      "[128/00671] train_loss: 0.014481\n",
      "[128/00721] train_loss: 0.014531\n",
      "[128/00771] train_loss: 0.014098\n",
      "[128/00821] train_loss: 0.013429\n",
      "[128/00871] train_loss: 0.014058\n",
      "[128/00921] train_loss: 0.014478\n",
      "[128/00971] train_loss: 0.014481\n",
      "[128/01021] train_loss: 0.014373\n",
      "[128/01071] train_loss: 0.014018\n",
      "[128/01121] train_loss: 0.014652\n",
      "[128/01171] train_loss: 0.013649\n",
      "[128/01221] train_loss: 0.013840\n",
      "[129/00045] train_loss: 0.015736\n",
      "[129/00095] train_loss: 0.015713\n",
      "[129/00145] train_loss: 0.015973\n",
      "[129/00195] train_loss: 0.013619\n",
      "[129/00245] train_loss: 0.014339\n",
      "[129/00295] train_loss: 0.014388\n",
      "[129/00345] train_loss: 0.014234\n",
      "[129/00395] train_loss: 0.013832\n",
      "[129/00445] train_loss: 0.015173\n",
      "[129/00495] train_loss: 0.014867\n",
      "[129/00545] train_loss: 0.014435\n",
      "[129/00595] train_loss: 0.014645\n",
      "[129/00645] train_loss: 0.015576\n",
      "[129/00695] train_loss: 0.014072\n",
      "[129/00745] train_loss: 0.014796\n",
      "[129/00795] train_loss: 0.014113\n",
      "[129/00845] train_loss: 0.014099\n",
      "[129/00895] train_loss: 0.014573\n",
      "[129/00945] train_loss: 0.014367\n",
      "[129/00995] train_loss: 0.013517\n",
      "[129/01045] train_loss: 0.014470\n",
      "[129/01095] train_loss: 0.014410\n",
      "[129/01145] train_loss: 0.013483\n",
      "[129/01195] train_loss: 0.015080\n",
      "[130/00019] train_loss: 0.014908\n",
      "[130/00069] train_loss: 0.015910\n",
      "[130/00119] train_loss: 0.015687\n",
      "[130/00169] train_loss: 0.015003\n",
      "[130/00219] train_loss: 0.015164\n",
      "[130/00269] train_loss: 0.014913\n",
      "[130/00319] train_loss: 0.014820\n",
      "[130/00369] train_loss: 0.014556\n",
      "[130/00419] train_loss: 0.014692\n",
      "[130/00469] train_loss: 0.014464\n",
      "[130/00519] train_loss: 0.014405\n",
      "[130/00569] train_loss: 0.014637\n",
      "[130/00619] train_loss: 0.014823\n",
      "[130/00669] train_loss: 0.013943\n",
      "[130/00719] train_loss: 0.014226\n",
      "[130/00769] train_loss: 0.014222\n",
      "[130/00819] train_loss: 0.014324\n",
      "[130/00869] train_loss: 0.013494\n",
      "[130/00919] train_loss: 0.014429\n",
      "[130/00969] train_loss: 0.013873\n",
      "[130/01019] train_loss: 0.014416\n",
      "[130/01069] train_loss: 0.013348\n",
      "[130/01119] train_loss: 0.013460\n",
      "[130/01169] train_loss: 0.013726\n",
      "[130/01219] train_loss: 0.013531\n",
      "[131/00043] train_loss: 0.015768\n",
      "[131/00093] train_loss: 0.015272\n",
      "[131/00143] train_loss: 0.014507\n",
      "[131/00193] train_loss: 0.014307\n",
      "[131/00243] train_loss: 0.014423\n",
      "[131/00293] train_loss: 0.013908\n",
      "[131/00343] train_loss: 0.014125\n",
      "[131/00393] train_loss: 0.013314\n",
      "[131/00443] train_loss: 0.014659\n",
      "[131/00493] train_loss: 0.014566\n",
      "[131/00543] train_loss: 0.014690\n",
      "[131/00593] train_loss: 0.014163\n",
      "[131/00643] train_loss: 0.013643\n",
      "[131/00693] train_loss: 0.014499\n",
      "[131/00743] train_loss: 0.014190\n",
      "[131/00793] train_loss: 0.014009\n",
      "[131/00843] train_loss: 0.014715\n",
      "[131/00893] train_loss: 0.014331\n",
      "[131/00943] train_loss: 0.014350\n",
      "[131/00993] train_loss: 0.015487\n",
      "[131/01043] train_loss: 0.014372\n",
      "[131/01093] train_loss: 0.013752\n",
      "[131/01143] train_loss: 0.013539\n",
      "[131/01193] train_loss: 0.014293\n",
      "[132/00017] train_loss: 0.014526\n",
      "[132/00067] train_loss: 0.015355\n",
      "[132/00117] train_loss: 0.015045\n",
      "[132/00167] train_loss: 0.014378\n",
      "[132/00217] train_loss: 0.014566\n",
      "[132/00267] train_loss: 0.014762\n",
      "[132/00317] train_loss: 0.014304\n",
      "[132/00367] train_loss: 0.013686\n",
      "[132/00417] train_loss: 0.014813\n",
      "[132/00467] train_loss: 0.013784\n",
      "[132/00517] train_loss: 0.014240\n",
      "[132/00567] train_loss: 0.014690\n",
      "[132/00617] train_loss: 0.014974\n",
      "[132/00667] train_loss: 0.014845\n",
      "[132/00717] train_loss: 0.015030\n",
      "[132/00767] train_loss: 0.014570\n",
      "[132/00817] train_loss: 0.014285\n",
      "[132/00867] train_loss: 0.014273\n",
      "[132/00917] train_loss: 0.013935\n",
      "[132/00967] train_loss: 0.014526\n",
      "[132/01017] train_loss: 0.014029\n",
      "[132/01067] train_loss: 0.013944\n",
      "[132/01117] train_loss: 0.014184\n",
      "[132/01167] train_loss: 0.013408\n",
      "[132/01217] train_loss: 0.014021\n",
      "[133/00041] train_loss: 0.015406\n",
      "[133/00091] train_loss: 0.015565\n",
      "[133/00141] train_loss: 0.015205\n",
      "[133/00191] train_loss: 0.014906\n",
      "[133/00241] train_loss: 0.014750\n",
      "[133/00291] train_loss: 0.013996\n",
      "[133/00341] train_loss: 0.014724\n",
      "[133/00391] train_loss: 0.014211\n",
      "[133/00441] train_loss: 0.013820\n",
      "[133/00491] train_loss: 0.014903\n",
      "[133/00541] train_loss: 0.014144\n",
      "[133/00591] train_loss: 0.013113\n",
      "[133/00641] train_loss: 0.014754\n",
      "[133/00691] train_loss: 0.013823\n",
      "[133/00741] train_loss: 0.014177\n",
      "[133/00791] train_loss: 0.014693\n",
      "[133/00841] train_loss: 0.015049\n",
      "[133/00891] train_loss: 0.014006\n",
      "[133/00941] train_loss: 0.014468\n",
      "[133/00991] train_loss: 0.014244\n",
      "[133/01041] train_loss: 0.014271\n",
      "[133/01091] train_loss: 0.013448\n",
      "[133/01141] train_loss: 0.013632\n",
      "[133/01191] train_loss: 0.013704\n",
      "[134/00015] train_loss: 0.014017\n",
      "[134/00065] train_loss: 0.015640\n",
      "[134/00115] train_loss: 0.015442\n",
      "[134/00165] train_loss: 0.014908\n",
      "[134/00215] train_loss: 0.014406\n",
      "[134/00265] train_loss: 0.013731\n",
      "[134/00315] train_loss: 0.014867\n",
      "[134/00365] train_loss: 0.014239\n",
      "[134/00415] train_loss: 0.014443\n",
      "[134/00465] train_loss: 0.013946\n",
      "[134/00515] train_loss: 0.014271\n",
      "[134/00565] train_loss: 0.014619\n",
      "[134/00615] train_loss: 0.013702\n",
      "[134/00665] train_loss: 0.014506\n",
      "[134/00715] train_loss: 0.014288\n",
      "[134/00765] train_loss: 0.013909\n",
      "[134/00815] train_loss: 0.014857\n",
      "[134/00865] train_loss: 0.014790\n",
      "[134/00915] train_loss: 0.014522\n",
      "[134/00965] train_loss: 0.014339\n",
      "[134/01015] train_loss: 0.014016\n",
      "[134/01065] train_loss: 0.014805\n",
      "[134/01115] train_loss: 0.013936\n",
      "[134/01165] train_loss: 0.014025\n",
      "[134/01215] train_loss: 0.013487\n",
      "[135/00039] train_loss: 0.015819\n",
      "[135/00089] train_loss: 0.015674\n",
      "[135/00139] train_loss: 0.015151\n",
      "[135/00189] train_loss: 0.015734\n",
      "[135/00239] train_loss: 0.014991\n",
      "[135/00289] train_loss: 0.015125\n",
      "[135/00339] train_loss: 0.013951\n",
      "[135/00389] train_loss: 0.014149\n",
      "[135/00439] train_loss: 0.013914\n",
      "[135/00489] train_loss: 0.014325\n",
      "[135/00539] train_loss: 0.014228\n",
      "[135/00589] train_loss: 0.013867\n",
      "[135/00639] train_loss: 0.014583\n",
      "[135/00689] train_loss: 0.014396\n",
      "[135/00739] train_loss: 0.014020\n",
      "[135/00789] train_loss: 0.014308\n",
      "[135/00839] train_loss: 0.013739\n",
      "[135/00889] train_loss: 0.013778\n",
      "[135/00939] train_loss: 0.014172\n",
      "[135/00989] train_loss: 0.013572\n",
      "[135/01039] train_loss: 0.013630\n",
      "[135/01089] train_loss: 0.014022\n",
      "[135/01139] train_loss: 0.014500\n",
      "[135/01189] train_loss: 0.014303\n",
      "[136/00013] train_loss: 0.014814\n",
      "[136/00063] train_loss: 0.015848\n",
      "[136/00113] train_loss: 0.014928\n",
      "[136/00163] train_loss: 0.015956\n",
      "[136/00213] train_loss: 0.014415\n",
      "[136/00263] train_loss: 0.014167\n",
      "[136/00313] train_loss: 0.014843\n",
      "[136/00363] train_loss: 0.013965\n",
      "[136/00413] train_loss: 0.014533\n",
      "[136/00463] train_loss: 0.014559\n",
      "[136/00513] train_loss: 0.014628\n",
      "[136/00563] train_loss: 0.014637\n",
      "[136/00613] train_loss: 0.014238\n",
      "[136/00663] train_loss: 0.014031\n",
      "[136/00713] train_loss: 0.013799\n",
      "[136/00763] train_loss: 0.013842\n",
      "[136/00813] train_loss: 0.014036\n",
      "[136/00863] train_loss: 0.014422\n",
      "[136/00913] train_loss: 0.013744\n",
      "[136/00963] train_loss: 0.014154\n",
      "[136/01013] train_loss: 0.014496\n",
      "[136/01063] train_loss: 0.013125\n",
      "[136/01113] train_loss: 0.014073\n",
      "[136/01163] train_loss: 0.013400\n",
      "[136/01213] train_loss: 0.014223\n",
      "[137/00037] train_loss: 0.015389\n",
      "[137/00087] train_loss: 0.015449\n",
      "[137/00137] train_loss: 0.015365\n",
      "[137/00187] train_loss: 0.014960\n",
      "[137/00237] train_loss: 0.014261\n",
      "[137/00287] train_loss: 0.015135\n",
      "[137/00337] train_loss: 0.013511\n",
      "[137/00387] train_loss: 0.013503\n",
      "[137/00437] train_loss: 0.014541\n",
      "[137/00487] train_loss: 0.014400\n",
      "[137/00537] train_loss: 0.014172\n",
      "[137/00587] train_loss: 0.014836\n",
      "[137/00637] train_loss: 0.013894\n",
      "[137/00687] train_loss: 0.014239\n",
      "[137/00737] train_loss: 0.013306\n",
      "[137/00787] train_loss: 0.013446\n",
      "[137/00837] train_loss: 0.014554\n",
      "[137/00887] train_loss: 0.013767\n",
      "[137/00937] train_loss: 0.014638\n",
      "[137/00987] train_loss: 0.014379\n",
      "[137/01037] train_loss: 0.013284\n",
      "[137/01087] train_loss: 0.013284\n",
      "[137/01137] train_loss: 0.014400\n",
      "[137/01187] train_loss: 0.014317\n",
      "[138/00011] train_loss: 0.015302\n",
      "[138/00061] train_loss: 0.015243\n",
      "[138/00111] train_loss: 0.015290\n",
      "[138/00161] train_loss: 0.014826\n",
      "[138/00211] train_loss: 0.015011\n",
      "[138/00261] train_loss: 0.014590\n",
      "[138/00311] train_loss: 0.013983\n",
      "[138/00361] train_loss: 0.014568\n",
      "[138/00411] train_loss: 0.013802\n",
      "[138/00461] train_loss: 0.013943\n",
      "[138/00511] train_loss: 0.013679\n",
      "[138/00561] train_loss: 0.013412\n",
      "[138/00611] train_loss: 0.014354\n",
      "[138/00661] train_loss: 0.014285\n",
      "[138/00711] train_loss: 0.013830\n",
      "[138/00761] train_loss: 0.015287\n",
      "[138/00811] train_loss: 0.014727\n",
      "[138/00861] train_loss: 0.014378\n",
      "[138/00911] train_loss: 0.014344\n",
      "[138/00961] train_loss: 0.014069\n",
      "[138/01011] train_loss: 0.014003\n",
      "[138/01061] train_loss: 0.013975\n",
      "[138/01111] train_loss: 0.013816\n",
      "[138/01161] train_loss: 0.013734\n",
      "[138/01211] train_loss: 0.014844\n",
      "[139/00035] train_loss: 0.014687\n",
      "[139/00085] train_loss: 0.015099\n",
      "[139/00135] train_loss: 0.014832\n",
      "[139/00185] train_loss: 0.014809\n",
      "[139/00235] train_loss: 0.014449\n",
      "[139/00285] train_loss: 0.013766\n",
      "[139/00335] train_loss: 0.014968\n",
      "[139/00385] train_loss: 0.014433\n",
      "[139/00435] train_loss: 0.014627\n",
      "[139/00485] train_loss: 0.014298\n",
      "[139/00535] train_loss: 0.013888\n",
      "[139/00585] train_loss: 0.014679\n",
      "[139/00635] train_loss: 0.014455\n",
      "[139/00685] train_loss: 0.014015\n",
      "[139/00735] train_loss: 0.014269\n",
      "[139/00785] train_loss: 0.014594\n",
      "[139/00835] train_loss: 0.013962\n",
      "[139/00885] train_loss: 0.014172\n",
      "[139/00935] train_loss: 0.014201\n",
      "[139/00985] train_loss: 0.013390\n",
      "[139/01035] train_loss: 0.014023\n",
      "[139/01085] train_loss: 0.013403\n",
      "[139/01135] train_loss: 0.014552\n",
      "[139/01185] train_loss: 0.014086\n",
      "[140/00009] train_loss: 0.014648\n",
      "[140/00059] train_loss: 0.015565\n",
      "[140/00109] train_loss: 0.015171\n",
      "[140/00159] train_loss: 0.015767\n",
      "[140/00209] train_loss: 0.014448\n",
      "[140/00259] train_loss: 0.014882\n",
      "[140/00309] train_loss: 0.014955\n",
      "[140/00359] train_loss: 0.014075\n",
      "[140/00409] train_loss: 0.014879\n",
      "[140/00459] train_loss: 0.013826\n",
      "[140/00509] train_loss: 0.015399\n",
      "[140/00559] train_loss: 0.014432\n",
      "[140/00609] train_loss: 0.013735\n",
      "[140/00659] train_loss: 0.014468\n",
      "[140/00709] train_loss: 0.013887\n",
      "[140/00759] train_loss: 0.013937\n",
      "[140/00809] train_loss: 0.014263\n",
      "[140/00859] train_loss: 0.013206\n",
      "[140/00909] train_loss: 0.014136\n",
      "[140/00959] train_loss: 0.012651\n",
      "[140/01009] train_loss: 0.013574\n",
      "[140/01059] train_loss: 0.014531\n",
      "[140/01109] train_loss: 0.013371\n",
      "[140/01159] train_loss: 0.013657\n",
      "[140/01209] train_loss: 0.014063\n",
      "[141/00033] train_loss: 0.015688\n",
      "[141/00083] train_loss: 0.015021\n",
      "[141/00133] train_loss: 0.014548\n",
      "[141/00183] train_loss: 0.015173\n",
      "[141/00233] train_loss: 0.014935\n",
      "[141/00283] train_loss: 0.014591\n",
      "[141/00333] train_loss: 0.013892\n",
      "[141/00383] train_loss: 0.013525\n",
      "[141/00433] train_loss: 0.014287\n",
      "[141/00483] train_loss: 0.013733\n",
      "[141/00533] train_loss: 0.013370\n",
      "[141/00583] train_loss: 0.014751\n",
      "[141/00633] train_loss: 0.014586\n",
      "[141/00683] train_loss: 0.013782\n",
      "[141/00733] train_loss: 0.014289\n",
      "[141/00783] train_loss: 0.014785\n",
      "[141/00833] train_loss: 0.014230\n",
      "[141/00883] train_loss: 0.014753\n",
      "[141/00933] train_loss: 0.014300\n",
      "[141/00983] train_loss: 0.013568\n",
      "[141/01033] train_loss: 0.014462\n",
      "[141/01083] train_loss: 0.013575\n",
      "[141/01133] train_loss: 0.013730\n",
      "[141/01183] train_loss: 0.014022\n",
      "[142/00007] train_loss: 0.013873\n",
      "[142/00057] train_loss: 0.015091\n",
      "[142/00107] train_loss: 0.015175\n",
      "[142/00157] train_loss: 0.015195\n",
      "[142/00207] train_loss: 0.015563\n",
      "[142/00257] train_loss: 0.014527\n",
      "[142/00307] train_loss: 0.015008\n",
      "[142/00357] train_loss: 0.014200\n",
      "[142/00407] train_loss: 0.014778\n",
      "[142/00457] train_loss: 0.013799\n",
      "[142/00507] train_loss: 0.014317\n",
      "[142/00557] train_loss: 0.014129\n",
      "[142/00607] train_loss: 0.014188\n",
      "[142/00657] train_loss: 0.013884\n",
      "[142/00707] train_loss: 0.013950\n",
      "[142/00757] train_loss: 0.014158\n",
      "[142/00807] train_loss: 0.013874\n",
      "[142/00857] train_loss: 0.014011\n",
      "[142/00907] train_loss: 0.013363\n",
      "[142/00957] train_loss: 0.012994\n",
      "[142/01007] train_loss: 0.014453\n",
      "[142/01057] train_loss: 0.013201\n",
      "[142/01107] train_loss: 0.014013\n",
      "[142/01157] train_loss: 0.013825\n",
      "[142/01207] train_loss: 0.013637\n",
      "[143/00031] train_loss: 0.015167\n",
      "[143/00081] train_loss: 0.015501\n",
      "[143/00131] train_loss: 0.015630\n",
      "[143/00181] train_loss: 0.015565\n",
      "[143/00231] train_loss: 0.015517\n",
      "[143/00281] train_loss: 0.015080\n",
      "[143/00331] train_loss: 0.014469\n",
      "[143/00381] train_loss: 0.013692\n",
      "[143/00431] train_loss: 0.013905\n",
      "[143/00481] train_loss: 0.013796\n",
      "[143/00531] train_loss: 0.014947\n",
      "[143/00581] train_loss: 0.014555\n",
      "[143/00631] train_loss: 0.014355\n",
      "[143/00681] train_loss: 0.013732\n",
      "[143/00731] train_loss: 0.014675\n",
      "[143/00781] train_loss: 0.013811\n",
      "[143/00831] train_loss: 0.013808\n",
      "[143/00881] train_loss: 0.014223\n",
      "[143/00931] train_loss: 0.013596\n",
      "[143/00981] train_loss: 0.013826\n",
      "[143/01031] train_loss: 0.013443\n",
      "[143/01081] train_loss: 0.013644\n",
      "[143/01131] train_loss: 0.013629\n",
      "[143/01181] train_loss: 0.013302\n",
      "[144/00005] train_loss: 0.014100\n",
      "[144/00055] train_loss: 0.016164\n",
      "[144/00105] train_loss: 0.016131\n",
      "[144/00155] train_loss: 0.014380\n",
      "[144/00205] train_loss: 0.015162\n",
      "[144/00255] train_loss: 0.014571\n",
      "[144/00305] train_loss: 0.014473\n",
      "[144/00355] train_loss: 0.013604\n",
      "[144/00405] train_loss: 0.014386\n",
      "[144/00455] train_loss: 0.014035\n",
      "[144/00505] train_loss: 0.014555\n",
      "[144/00555] train_loss: 0.013922\n",
      "[144/00605] train_loss: 0.014344\n",
      "[144/00655] train_loss: 0.013400\n",
      "[144/00705] train_loss: 0.013830\n",
      "[144/00755] train_loss: 0.013923\n",
      "[144/00805] train_loss: 0.013520\n",
      "[144/00855] train_loss: 0.013971\n",
      "[144/00905] train_loss: 0.014944\n",
      "[144/00955] train_loss: 0.013786\n",
      "[144/01005] train_loss: 0.013603\n",
      "[144/01055] train_loss: 0.014022\n",
      "[144/01105] train_loss: 0.015018\n",
      "[144/01155] train_loss: 0.012866\n",
      "[144/01205] train_loss: 0.013808\n",
      "[145/00029] train_loss: 0.014814\n",
      "[145/00079] train_loss: 0.015161\n",
      "[145/00129] train_loss: 0.014190\n",
      "[145/00179] train_loss: 0.015006\n",
      "[145/00229] train_loss: 0.014588\n",
      "[145/00279] train_loss: 0.013997\n",
      "[145/00329] train_loss: 0.014034\n",
      "[145/00379] train_loss: 0.014029\n",
      "[145/00429] train_loss: 0.013837\n",
      "[145/00479] train_loss: 0.014166\n",
      "[145/00529] train_loss: 0.013776\n",
      "[145/00579] train_loss: 0.013778\n",
      "[145/00629] train_loss: 0.013237\n",
      "[145/00679] train_loss: 0.014177\n",
      "[145/00729] train_loss: 0.014588\n",
      "[145/00779] train_loss: 0.014737\n",
      "[145/00829] train_loss: 0.013732\n",
      "[145/00879] train_loss: 0.014303\n",
      "[145/00929] train_loss: 0.014637\n",
      "[145/00979] train_loss: 0.014625\n",
      "[145/01029] train_loss: 0.014092\n",
      "[145/01079] train_loss: 0.014250\n",
      "[145/01129] train_loss: 0.013355\n",
      "[145/01179] train_loss: 0.014108\n",
      "[146/00003] train_loss: 0.014278\n",
      "[146/00053] train_loss: 0.015379\n",
      "[146/00103] train_loss: 0.016005\n",
      "[146/00153] train_loss: 0.015336\n",
      "[146/00203] train_loss: 0.015286\n",
      "[146/00253] train_loss: 0.014808\n",
      "[146/00303] train_loss: 0.014679\n",
      "[146/00353] train_loss: 0.013815\n",
      "[146/00403] train_loss: 0.014632\n",
      "[146/00453] train_loss: 0.014896\n",
      "[146/00503] train_loss: 0.014522\n",
      "[146/00553] train_loss: 0.014446\n",
      "[146/00603] train_loss: 0.014009\n",
      "[146/00653] train_loss: 0.013681\n",
      "[146/00703] train_loss: 0.013133\n",
      "[146/00753] train_loss: 0.014327\n",
      "[146/00803] train_loss: 0.013499\n",
      "[146/00853] train_loss: 0.013448\n",
      "[146/00903] train_loss: 0.014323\n",
      "[146/00953] train_loss: 0.014196\n",
      "[146/01003] train_loss: 0.013592\n",
      "[146/01053] train_loss: 0.013669\n",
      "[146/01103] train_loss: 0.013351\n",
      "[146/01153] train_loss: 0.013561\n",
      "[146/01203] train_loss: 0.014407\n",
      "[147/00027] train_loss: 0.015224\n",
      "[147/00077] train_loss: 0.014712\n",
      "[147/00127] train_loss: 0.015272\n",
      "[147/00177] train_loss: 0.013843\n",
      "[147/00227] train_loss: 0.014105\n",
      "[147/00277] train_loss: 0.014140\n",
      "[147/00327] train_loss: 0.015235\n",
      "[147/00377] train_loss: 0.014782\n",
      "[147/00427] train_loss: 0.014332\n",
      "[147/00477] train_loss: 0.014059\n",
      "[147/00527] train_loss: 0.014167\n",
      "[147/00577] train_loss: 0.013782\n",
      "[147/00627] train_loss: 0.013589\n",
      "[147/00677] train_loss: 0.014006\n",
      "[147/00727] train_loss: 0.014554\n",
      "[147/00777] train_loss: 0.013464\n",
      "[147/00827] train_loss: 0.013518\n",
      "[147/00877] train_loss: 0.013717\n",
      "[147/00927] train_loss: 0.014773\n",
      "[147/00977] train_loss: 0.013162\n",
      "[147/01027] train_loss: 0.013732\n",
      "[147/01077] train_loss: 0.013804\n",
      "[147/01127] train_loss: 0.013467\n",
      "[147/01177] train_loss: 0.013932\n",
      "[148/00001] train_loss: 0.014833\n",
      "[148/00051] train_loss: 0.015983\n",
      "[148/00101] train_loss: 0.015062\n",
      "[148/00151] train_loss: 0.014467\n",
      "[148/00201] train_loss: 0.014459\n",
      "[148/00251] train_loss: 0.014283\n",
      "[148/00301] train_loss: 0.014295\n",
      "[148/00351] train_loss: 0.014458\n",
      "[148/00401] train_loss: 0.013537\n",
      "[148/00451] train_loss: 0.014047\n",
      "[148/00501] train_loss: 0.014009\n",
      "[148/00551] train_loss: 0.014627\n",
      "[148/00601] train_loss: 0.014018\n",
      "[148/00651] train_loss: 0.013127\n",
      "[148/00701] train_loss: 0.013558\n",
      "[148/00751] train_loss: 0.013577\n",
      "[148/00801] train_loss: 0.013121\n",
      "[148/00851] train_loss: 0.014519\n",
      "[148/00901] train_loss: 0.014089\n",
      "[148/00951] train_loss: 0.014136\n",
      "[148/01001] train_loss: 0.013982\n",
      "[148/01051] train_loss: 0.013568\n",
      "[148/01101] train_loss: 0.013434\n",
      "[148/01151] train_loss: 0.013585\n",
      "[148/01201] train_loss: 0.015011\n",
      "[149/00025] train_loss: 0.014307\n",
      "[149/00075] train_loss: 0.014490\n",
      "[149/00125] train_loss: 0.014902\n",
      "[149/00175] train_loss: 0.014912\n",
      "[149/00225] train_loss: 0.014264\n",
      "[149/00275] train_loss: 0.015033\n",
      "[149/00325] train_loss: 0.013917\n",
      "[149/00375] train_loss: 0.013875\n",
      "[149/00425] train_loss: 0.015122\n",
      "[149/00475] train_loss: 0.013681\n",
      "[149/00525] train_loss: 0.013568\n",
      "[149/00575] train_loss: 0.014162\n",
      "[149/00625] train_loss: 0.014092\n",
      "[149/00675] train_loss: 0.014219\n",
      "[149/00725] train_loss: 0.013685\n",
      "[149/00775] train_loss: 0.013779\n",
      "[149/00825] train_loss: 0.013400\n",
      "[149/00875] train_loss: 0.013833\n",
      "[149/00925] train_loss: 0.014724\n",
      "[149/00975] train_loss: 0.014314\n",
      "[149/01025] train_loss: 0.014078\n",
      "[149/01075] train_loss: 0.014121\n",
      "[149/01125] train_loss: 0.014100\n",
      "[149/01175] train_loss: 0.013843\n",
      "[149/01225] train_loss: 0.014424\n",
      "[150/00049] train_loss: 0.015816\n",
      "[150/00099] train_loss: 0.014608\n",
      "[150/00149] train_loss: 0.014216\n",
      "[150/00199] train_loss: 0.015242\n",
      "[150/00249] train_loss: 0.013534\n",
      "[150/00299] train_loss: 0.014208\n",
      "[150/00349] train_loss: 0.013728\n",
      "[150/00399] train_loss: 0.013699\n",
      "[150/00449] train_loss: 0.014542\n",
      "[150/00499] train_loss: 0.015523\n",
      "[150/00549] train_loss: 0.013337\n",
      "[150/00599] train_loss: 0.013670\n",
      "[150/00649] train_loss: 0.013124\n",
      "[150/00699] train_loss: 0.014140\n",
      "[150/00749] train_loss: 0.014369\n",
      "[150/00799] train_loss: 0.013539\n",
      "[150/00849] train_loss: 0.013501\n",
      "[150/00899] train_loss: 0.013661\n",
      "[150/00949] train_loss: 0.013880\n",
      "[150/00999] train_loss: 0.013333\n",
      "[150/01049] train_loss: 0.013918\n",
      "[150/01099] train_loss: 0.013924\n",
      "[150/01149] train_loss: 0.013648\n",
      "[150/01199] train_loss: 0.014891\n",
      "[151/00023] train_loss: 0.015273\n",
      "[151/00073] train_loss: 0.015675\n",
      "[151/00123] train_loss: 0.015182\n",
      "[151/00173] train_loss: 0.015281\n",
      "[151/00223] train_loss: 0.014252\n",
      "[151/00273] train_loss: 0.014230\n",
      "[151/00323] train_loss: 0.014427\n",
      "[151/00373] train_loss: 0.013643\n",
      "[151/00423] train_loss: 0.013809\n",
      "[151/00473] train_loss: 0.014118\n",
      "[151/00523] train_loss: 0.014231\n",
      "[151/00573] train_loss: 0.013715\n",
      "[151/00623] train_loss: 0.014022\n",
      "[151/00673] train_loss: 0.014464\n",
      "[151/00723] train_loss: 0.013569\n",
      "[151/00773] train_loss: 0.014156\n",
      "[151/00823] train_loss: 0.014280\n",
      "[151/00873] train_loss: 0.013374\n",
      "[151/00923] train_loss: 0.014051\n",
      "[151/00973] train_loss: 0.013884\n",
      "[151/01023] train_loss: 0.014819\n",
      "[151/01073] train_loss: 0.013331\n",
      "[151/01123] train_loss: 0.014480\n",
      "[151/01173] train_loss: 0.013358\n",
      "[151/01223] train_loss: 0.013744\n",
      "[152/00047] train_loss: 0.015851\n",
      "[152/00097] train_loss: 0.015921\n",
      "[152/00147] train_loss: 0.014905\n",
      "[152/00197] train_loss: 0.014068\n",
      "[152/00247] train_loss: 0.013655\n",
      "[152/00297] train_loss: 0.014050\n",
      "[152/00347] train_loss: 0.014310\n",
      "[152/00397] train_loss: 0.014232\n",
      "[152/00447] train_loss: 0.013984\n",
      "[152/00497] train_loss: 0.013718\n",
      "[152/00547] train_loss: 0.014082\n",
      "[152/00597] train_loss: 0.013997\n",
      "[152/00647] train_loss: 0.013884\n",
      "[152/00697] train_loss: 0.014097\n",
      "[152/00747] train_loss: 0.014012\n",
      "[152/00797] train_loss: 0.014295\n",
      "[152/00847] train_loss: 0.014030\n",
      "[152/00897] train_loss: 0.013657\n",
      "[152/00947] train_loss: 0.013676\n",
      "[152/00997] train_loss: 0.014089\n",
      "[152/01047] train_loss: 0.013604\n",
      "[152/01097] train_loss: 0.014319\n",
      "[152/01147] train_loss: 0.013399\n",
      "[152/01197] train_loss: 0.013910\n",
      "[153/00021] train_loss: 0.014862\n",
      "[153/00071] train_loss: 0.016468\n",
      "[153/00121] train_loss: 0.014968\n",
      "[153/00171] train_loss: 0.014568\n",
      "[153/00221] train_loss: 0.014377\n",
      "[153/00271] train_loss: 0.013503\n",
      "[153/00321] train_loss: 0.014625\n",
      "[153/00371] train_loss: 0.014240\n",
      "[153/00421] train_loss: 0.013226\n",
      "[153/00471] train_loss: 0.014300\n",
      "[153/00521] train_loss: 0.013752\n",
      "[153/00571] train_loss: 0.014017\n",
      "[153/00621] train_loss: 0.014294\n",
      "[153/00671] train_loss: 0.013322\n",
      "[153/00721] train_loss: 0.013445\n",
      "[153/00771] train_loss: 0.014560\n",
      "[153/00821] train_loss: 0.013774\n",
      "[153/00871] train_loss: 0.013782\n",
      "[153/00921] train_loss: 0.013451\n",
      "[153/00971] train_loss: 0.014103\n",
      "[153/01021] train_loss: 0.013776\n",
      "[153/01071] train_loss: 0.014149\n",
      "[153/01121] train_loss: 0.013801\n",
      "[153/01171] train_loss: 0.012889\n",
      "[153/01221] train_loss: 0.013736\n",
      "[154/00045] train_loss: 0.015776\n",
      "[154/00095] train_loss: 0.015462\n",
      "[154/00145] train_loss: 0.014932\n",
      "[154/00195] train_loss: 0.014472\n",
      "[154/00245] train_loss: 0.014291\n",
      "[154/00295] train_loss: 0.015523\n",
      "[154/00345] train_loss: 0.014664\n",
      "[154/00395] train_loss: 0.014178\n",
      "[154/00445] train_loss: 0.013780\n",
      "[154/00495] train_loss: 0.014440\n",
      "[154/00545] train_loss: 0.015058\n",
      "[154/00595] train_loss: 0.013911\n",
      "[154/00645] train_loss: 0.014305\n",
      "[154/00695] train_loss: 0.013295\n",
      "[154/00745] train_loss: 0.013580\n",
      "[154/00795] train_loss: 0.013361\n",
      "[154/00845] train_loss: 0.013944\n",
      "[154/00895] train_loss: 0.013541\n",
      "[154/00945] train_loss: 0.013975\n",
      "[154/00995] train_loss: 0.013900\n",
      "[154/01045] train_loss: 0.014584\n",
      "[154/01095] train_loss: 0.013294\n",
      "[154/01145] train_loss: 0.013308\n",
      "[154/01195] train_loss: 0.014139\n",
      "[155/00019] train_loss: 0.014605\n",
      "[155/00069] train_loss: 0.015442\n",
      "[155/00119] train_loss: 0.014820\n",
      "[155/00169] train_loss: 0.014137\n",
      "[155/00219] train_loss: 0.013759\n",
      "[155/00269] train_loss: 0.014053\n",
      "[155/00319] train_loss: 0.014267\n",
      "[155/00369] train_loss: 0.013404\n",
      "[155/00419] train_loss: 0.013232\n",
      "[155/00469] train_loss: 0.014238\n",
      "[155/00519] train_loss: 0.014121\n",
      "[155/00569] train_loss: 0.013746\n",
      "[155/00619] train_loss: 0.014723\n",
      "[155/00669] train_loss: 0.013993\n",
      "[155/00719] train_loss: 0.013905\n",
      "[155/00769] train_loss: 0.013853\n",
      "[155/00819] train_loss: 0.014376\n",
      "[155/00869] train_loss: 0.014971\n",
      "[155/00919] train_loss: 0.013639\n",
      "[155/00969] train_loss: 0.013968\n",
      "[155/01019] train_loss: 0.013635\n",
      "[155/01069] train_loss: 0.014118\n",
      "[155/01119] train_loss: 0.013627\n",
      "[155/01169] train_loss: 0.013181\n",
      "[155/01219] train_loss: 0.014554\n",
      "[156/00043] train_loss: 0.014896\n",
      "[156/00093] train_loss: 0.015045\n",
      "[156/00143] train_loss: 0.014654\n",
      "[156/00193] train_loss: 0.015993\n",
      "[156/00243] train_loss: 0.013972\n",
      "[156/00293] train_loss: 0.014042\n",
      "[156/00343] train_loss: 0.013529\n",
      "[156/00393] train_loss: 0.014092\n",
      "[156/00443] train_loss: 0.014444\n",
      "[156/00493] train_loss: 0.015122\n",
      "[156/00543] train_loss: 0.013122\n",
      "[156/00593] train_loss: 0.013639\n",
      "[156/00643] train_loss: 0.013494\n",
      "[156/00693] train_loss: 0.014034\n",
      "[156/00743] train_loss: 0.013430\n",
      "[156/00793] train_loss: 0.014122\n",
      "[156/00843] train_loss: 0.014712\n",
      "[156/00893] train_loss: 0.013344\n",
      "[156/00943] train_loss: 0.013862\n",
      "[156/00993] train_loss: 0.014170\n",
      "[156/01043] train_loss: 0.014290\n",
      "[156/01093] train_loss: 0.013104\n",
      "[156/01143] train_loss: 0.013551\n",
      "[156/01193] train_loss: 0.014044\n",
      "[157/00017] train_loss: 0.014289\n",
      "[157/00067] train_loss: 0.014944\n",
      "[157/00117] train_loss: 0.014724\n",
      "[157/00167] train_loss: 0.014300\n",
      "[157/00217] train_loss: 0.014988\n",
      "[157/00267] train_loss: 0.014020\n",
      "[157/00317] train_loss: 0.014094\n",
      "[157/00367] train_loss: 0.013663\n",
      "[157/00417] train_loss: 0.013648\n",
      "[157/00467] train_loss: 0.014040\n",
      "[157/00517] train_loss: 0.014896\n",
      "[157/00567] train_loss: 0.013905\n",
      "[157/00617] train_loss: 0.013720\n",
      "[157/00667] train_loss: 0.014067\n",
      "[157/00717] train_loss: 0.013528\n",
      "[157/00767] train_loss: 0.013725\n",
      "[157/00817] train_loss: 0.013756\n",
      "[157/00867] train_loss: 0.013623\n",
      "[157/00917] train_loss: 0.014230\n",
      "[157/00967] train_loss: 0.013164\n",
      "[157/01017] train_loss: 0.013695\n",
      "[157/01067] train_loss: 0.013127\n",
      "[157/01117] train_loss: 0.013545\n",
      "[157/01167] train_loss: 0.014657\n",
      "[157/01217] train_loss: 0.014514\n",
      "[158/00041] train_loss: 0.014469\n",
      "[158/00091] train_loss: 0.014429\n",
      "[158/00141] train_loss: 0.016065\n",
      "[158/00191] train_loss: 0.014604\n",
      "[158/00241] train_loss: 0.014258\n",
      "[158/00291] train_loss: 0.014906\n",
      "[158/00341] train_loss: 0.013747\n",
      "[158/00391] train_loss: 0.013805\n",
      "[158/00441] train_loss: 0.014214\n",
      "[158/00491] train_loss: 0.014248\n",
      "[158/00541] train_loss: 0.014523\n",
      "[158/00591] train_loss: 0.013306\n",
      "[158/00641] train_loss: 0.014151\n",
      "[158/00691] train_loss: 0.013540\n",
      "[158/00741] train_loss: 0.014244\n",
      "[158/00791] train_loss: 0.013212\n",
      "[158/00841] train_loss: 0.014433\n",
      "[158/00891] train_loss: 0.013730\n",
      "[158/00941] train_loss: 0.013680\n",
      "[158/00991] train_loss: 0.013671\n",
      "[158/01041] train_loss: 0.014001\n",
      "[158/01091] train_loss: 0.013941\n",
      "[158/01141] train_loss: 0.013568\n",
      "[158/01191] train_loss: 0.013605\n",
      "[159/00015] train_loss: 0.014316\n",
      "[159/00065] train_loss: 0.015629\n",
      "[159/00115] train_loss: 0.013764\n",
      "[159/00165] train_loss: 0.015336\n",
      "[159/00215] train_loss: 0.014184\n",
      "[159/00265] train_loss: 0.014936\n",
      "[159/00315] train_loss: 0.014997\n",
      "[159/00365] train_loss: 0.013763\n",
      "[159/00415] train_loss: 0.014452\n",
      "[159/00465] train_loss: 0.014299\n",
      "[159/00515] train_loss: 0.013909\n",
      "[159/00565] train_loss: 0.014529\n",
      "[159/00615] train_loss: 0.013704\n",
      "[159/00665] train_loss: 0.013504\n",
      "[159/00715] train_loss: 0.013343\n",
      "[159/00765] train_loss: 0.014398\n",
      "[159/00815] train_loss: 0.013798\n",
      "[159/00865] train_loss: 0.013741\n",
      "[159/00915] train_loss: 0.013620\n",
      "[159/00965] train_loss: 0.012584\n",
      "[159/01015] train_loss: 0.014043\n",
      "[159/01065] train_loss: 0.014274\n",
      "[159/01115] train_loss: 0.014253\n",
      "[159/01165] train_loss: 0.013402\n",
      "[159/01215] train_loss: 0.013281\n",
      "[160/00039] train_loss: 0.015418\n",
      "[160/00089] train_loss: 0.015326\n",
      "[160/00139] train_loss: 0.015431\n",
      "[160/00189] train_loss: 0.014908\n",
      "[160/00239] train_loss: 0.014445\n",
      "[160/00289] train_loss: 0.014848\n",
      "[160/00339] train_loss: 0.014104\n",
      "[160/00389] train_loss: 0.013741\n",
      "[160/00439] train_loss: 0.013490\n",
      "[160/00489] train_loss: 0.014085\n",
      "[160/00539] train_loss: 0.013689\n",
      "[160/00589] train_loss: 0.014157\n",
      "[160/00639] train_loss: 0.013502\n",
      "[160/00689] train_loss: 0.013946\n",
      "[160/00739] train_loss: 0.013896\n",
      "[160/00789] train_loss: 0.014407\n",
      "[160/00839] train_loss: 0.013859\n",
      "[160/00889] train_loss: 0.012843\n",
      "[160/00939] train_loss: 0.013424\n",
      "[160/00989] train_loss: 0.013878\n",
      "[160/01039] train_loss: 0.013878\n",
      "[160/01089] train_loss: 0.013667\n",
      "[160/01139] train_loss: 0.013009\n",
      "[160/01189] train_loss: 0.013003\n",
      "[161/00013] train_loss: 0.013421\n",
      "[161/00063] train_loss: 0.015117\n",
      "[161/00113] train_loss: 0.014525\n",
      "[161/00163] train_loss: 0.014717\n",
      "[161/00213] train_loss: 0.014187\n",
      "[161/00263] train_loss: 0.014714\n",
      "[161/00313] train_loss: 0.013626\n",
      "[161/00363] train_loss: 0.014701\n",
      "[161/00413] train_loss: 0.013756\n",
      "[161/00463] train_loss: 0.014044\n",
      "[161/00513] train_loss: 0.013681\n",
      "[161/00563] train_loss: 0.013654\n",
      "[161/00613] train_loss: 0.014375\n",
      "[161/00663] train_loss: 0.014585\n",
      "[161/00713] train_loss: 0.013118\n",
      "[161/00763] train_loss: 0.014548\n",
      "[161/00813] train_loss: 0.012989\n",
      "[161/00863] train_loss: 0.013942\n",
      "[161/00913] train_loss: 0.013852\n",
      "[161/00963] train_loss: 0.013027\n",
      "[161/01013] train_loss: 0.013580\n",
      "[161/01063] train_loss: 0.013652\n",
      "[161/01113] train_loss: 0.013945\n",
      "[161/01163] train_loss: 0.014050\n",
      "[161/01213] train_loss: 0.014671\n",
      "[162/00037] train_loss: 0.014512\n",
      "[162/00087] train_loss: 0.014858\n",
      "[162/00137] train_loss: 0.013989\n",
      "[162/00187] train_loss: 0.014826\n",
      "[162/00237] train_loss: 0.014647\n",
      "[162/00287] train_loss: 0.014659\n",
      "[162/00337] train_loss: 0.014130\n",
      "[162/00387] train_loss: 0.012982\n",
      "[162/00437] train_loss: 0.014218\n",
      "[162/00487] train_loss: 0.013345\n",
      "[162/00537] train_loss: 0.014954\n",
      "[162/00587] train_loss: 0.014631\n",
      "[162/00637] train_loss: 0.014412\n",
      "[162/00687] train_loss: 0.013846\n",
      "[162/00737] train_loss: 0.014314\n",
      "[162/00787] train_loss: 0.013418\n",
      "[162/00837] train_loss: 0.013453\n",
      "[162/00887] train_loss: 0.013607\n",
      "[162/00937] train_loss: 0.012924\n",
      "[162/00987] train_loss: 0.012931\n",
      "[162/01037] train_loss: 0.013258\n",
      "[162/01087] train_loss: 0.013373\n",
      "[162/01137] train_loss: 0.013611\n",
      "[162/01187] train_loss: 0.013760\n",
      "[163/00011] train_loss: 0.014514\n",
      "[163/00061] train_loss: 0.015155\n",
      "[163/00111] train_loss: 0.014855\n",
      "[163/00161] train_loss: 0.014781\n",
      "[163/00211] train_loss: 0.013613\n",
      "[163/00261] train_loss: 0.014382\n",
      "[163/00311] train_loss: 0.014861\n",
      "[163/00361] train_loss: 0.013751\n",
      "[163/00411] train_loss: 0.014786\n",
      "[163/00461] train_loss: 0.015178\n",
      "[163/00511] train_loss: 0.013515\n",
      "[163/00561] train_loss: 0.013706\n",
      "[163/00611] train_loss: 0.013124\n",
      "[163/00661] train_loss: 0.013477\n",
      "[163/00711] train_loss: 0.013025\n",
      "[163/00761] train_loss: 0.013362\n",
      "[163/00811] train_loss: 0.013773\n",
      "[163/00861] train_loss: 0.014047\n",
      "[163/00911] train_loss: 0.014816\n",
      "[163/00961] train_loss: 0.013409\n",
      "[163/01011] train_loss: 0.014112\n",
      "[163/01061] train_loss: 0.013676\n",
      "[163/01111] train_loss: 0.013393\n",
      "[163/01161] train_loss: 0.013241\n",
      "[163/01211] train_loss: 0.014003\n",
      "[164/00035] train_loss: 0.015176\n",
      "[164/00085] train_loss: 0.014057\n",
      "[164/00135] train_loss: 0.014668\n",
      "[164/00185] train_loss: 0.014848\n",
      "[164/00235] train_loss: 0.013907\n",
      "[164/00285] train_loss: 0.014403\n",
      "[164/00335] train_loss: 0.013821\n",
      "[164/00385] train_loss: 0.013499\n",
      "[164/00435] train_loss: 0.013975\n",
      "[164/00485] train_loss: 0.013978\n",
      "[164/00535] train_loss: 0.013843\n",
      "[164/00585] train_loss: 0.014457\n",
      "[164/00635] train_loss: 0.014208\n",
      "[164/00685] train_loss: 0.013874\n",
      "[164/00735] train_loss: 0.013891\n",
      "[164/00785] train_loss: 0.014110\n",
      "[164/00835] train_loss: 0.013169\n",
      "[164/00885] train_loss: 0.013756\n",
      "[164/00935] train_loss: 0.013815\n",
      "[164/00985] train_loss: 0.013518\n",
      "[164/01035] train_loss: 0.013700\n",
      "[164/01085] train_loss: 0.012920\n",
      "[164/01135] train_loss: 0.013371\n",
      "[164/01185] train_loss: 0.013375\n",
      "[165/00009] train_loss: 0.013691\n",
      "[165/00059] train_loss: 0.014982\n",
      "[165/00109] train_loss: 0.014669\n",
      "[165/00159] train_loss: 0.013640\n",
      "[165/00209] train_loss: 0.014290\n",
      "[165/00259] train_loss: 0.014765\n",
      "[165/00309] train_loss: 0.014051\n",
      "[165/00359] train_loss: 0.013428\n",
      "[165/00409] train_loss: 0.014015\n",
      "[165/00459] train_loss: 0.014234\n",
      "[165/00509] train_loss: 0.014192\n",
      "[165/00559] train_loss: 0.013403\n",
      "[165/00609] train_loss: 0.013882\n",
      "[165/00659] train_loss: 0.014103\n",
      "[165/00709] train_loss: 0.013931\n",
      "[165/00759] train_loss: 0.013324\n",
      "[165/00809] train_loss: 0.013621\n",
      "[165/00859] train_loss: 0.013409\n",
      "[165/00909] train_loss: 0.014348\n",
      "[165/00959] train_loss: 0.013636\n",
      "[165/01009] train_loss: 0.014041\n",
      "[165/01059] train_loss: 0.013496\n",
      "[165/01109] train_loss: 0.012878\n",
      "[165/01159] train_loss: 0.013349\n",
      "[165/01209] train_loss: 0.013742\n",
      "[166/00033] train_loss: 0.014318\n",
      "[166/00083] train_loss: 0.015648\n",
      "[166/00133] train_loss: 0.013537\n",
      "[166/00183] train_loss: 0.014250\n",
      "[166/00233] train_loss: 0.013577\n",
      "[166/00283] train_loss: 0.014658\n",
      "[166/00333] train_loss: 0.014391\n",
      "[166/00383] train_loss: 0.014766\n",
      "[166/00433] train_loss: 0.014058\n",
      "[166/00483] train_loss: 0.014191\n",
      "[166/00533] train_loss: 0.013012\n",
      "[166/00583] train_loss: 0.013719\n",
      "[166/00633] train_loss: 0.013004\n",
      "[166/00683] train_loss: 0.013798\n",
      "[166/00733] train_loss: 0.013944\n",
      "[166/00783] train_loss: 0.013825\n",
      "[166/00833] train_loss: 0.013865\n",
      "[166/00883] train_loss: 0.013941\n",
      "[166/00933] train_loss: 0.013203\n",
      "[166/00983] train_loss: 0.013539\n",
      "[166/01033] train_loss: 0.013906\n",
      "[166/01083] train_loss: 0.013699\n",
      "[166/01133] train_loss: 0.013605\n",
      "[166/01183] train_loss: 0.013677\n",
      "[167/00007] train_loss: 0.013265\n",
      "[167/00057] train_loss: 0.015147\n",
      "[167/00107] train_loss: 0.014856\n",
      "[167/00157] train_loss: 0.013877\n",
      "[167/00207] train_loss: 0.014526\n",
      "[167/00257] train_loss: 0.014400\n",
      "[167/00307] train_loss: 0.013569\n",
      "[167/00357] train_loss: 0.013398\n",
      "[167/00407] train_loss: 0.014512\n",
      "[167/00457] train_loss: 0.013243\n",
      "[167/00507] train_loss: 0.014110\n",
      "[167/00557] train_loss: 0.013759\n",
      "[167/00607] train_loss: 0.012859\n",
      "[167/00657] train_loss: 0.013487\n",
      "[167/00707] train_loss: 0.014735\n",
      "[167/00757] train_loss: 0.013799\n",
      "[167/00807] train_loss: 0.014371\n",
      "[167/00857] train_loss: 0.013859\n",
      "[167/00907] train_loss: 0.013565\n",
      "[167/00957] train_loss: 0.014160\n",
      "[167/01007] train_loss: 0.013677\n",
      "[167/01057] train_loss: 0.013165\n",
      "[167/01107] train_loss: 0.013873\n",
      "[167/01157] train_loss: 0.013915\n",
      "[167/01207] train_loss: 0.013591\n",
      "[168/00031] train_loss: 0.015219\n",
      "[168/00081] train_loss: 0.015297\n",
      "[168/00131] train_loss: 0.014543\n",
      "[168/00181] train_loss: 0.014678\n",
      "[168/00231] train_loss: 0.014213\n",
      "[168/00281] train_loss: 0.013979\n",
      "[168/00331] train_loss: 0.014876\n",
      "[168/00381] train_loss: 0.014737\n",
      "[168/00431] train_loss: 0.014173\n",
      "[168/00481] train_loss: 0.013679\n",
      "[168/00531] train_loss: 0.014082\n",
      "[168/00581] train_loss: 0.013921\n",
      "[168/00631] train_loss: 0.013314\n",
      "[168/00681] train_loss: 0.014263\n",
      "[168/00731] train_loss: 0.012621\n",
      "[168/00781] train_loss: 0.013793\n",
      "[168/00831] train_loss: 0.013162\n",
      "[168/00881] train_loss: 0.014349\n",
      "[168/00931] train_loss: 0.013714\n",
      "[168/00981] train_loss: 0.013635\n",
      "[168/01031] train_loss: 0.013422\n",
      "[168/01081] train_loss: 0.012994\n",
      "[168/01131] train_loss: 0.012900\n",
      "[168/01181] train_loss: 0.014397\n",
      "[169/00005] train_loss: 0.013903\n",
      "[169/00055] train_loss: 0.014707\n",
      "[169/00105] train_loss: 0.013347\n",
      "[169/00155] train_loss: 0.013695\n",
      "[169/00205] train_loss: 0.013713\n",
      "[169/00255] train_loss: 0.013957\n",
      "[169/00305] train_loss: 0.014362\n",
      "[169/00355] train_loss: 0.013622\n",
      "[169/00405] train_loss: 0.014146\n",
      "[169/00455] train_loss: 0.014148\n",
      "[169/00505] train_loss: 0.013999\n",
      "[169/00555] train_loss: 0.013723\n",
      "[169/00605] train_loss: 0.013350\n",
      "[169/00655] train_loss: 0.013849\n",
      "[169/00705] train_loss: 0.013345\n",
      "[169/00755] train_loss: 0.013511\n",
      "[169/00805] train_loss: 0.013612\n",
      "[169/00855] train_loss: 0.013108\n",
      "[169/00905] train_loss: 0.014366\n",
      "[169/00955] train_loss: 0.014488\n",
      "[169/01005] train_loss: 0.013893\n",
      "[169/01055] train_loss: 0.013723\n",
      "[169/01105] train_loss: 0.013020\n",
      "[169/01155] train_loss: 0.013744\n",
      "[169/01205] train_loss: 0.013967\n",
      "[170/00029] train_loss: 0.014254\n",
      "[170/00079] train_loss: 0.015428\n",
      "[170/00129] train_loss: 0.015455\n",
      "[170/00179] train_loss: 0.014352\n",
      "[170/00229] train_loss: 0.014509\n",
      "[170/00279] train_loss: 0.014715\n",
      "[170/00329] train_loss: 0.013844\n",
      "[170/00379] train_loss: 0.013894\n",
      "[170/00429] train_loss: 0.013348\n",
      "[170/00479] train_loss: 0.013102\n",
      "[170/00529] train_loss: 0.013363\n",
      "[170/00579] train_loss: 0.013376\n",
      "[170/00629] train_loss: 0.015047\n",
      "[170/00679] train_loss: 0.013101\n",
      "[170/00729] train_loss: 0.013573\n",
      "[170/00779] train_loss: 0.013456\n",
      "[170/00829] train_loss: 0.013981\n",
      "[170/00879] train_loss: 0.013533\n",
      "[170/00929] train_loss: 0.013047\n",
      "[170/00979] train_loss: 0.013419\n",
      "[170/01029] train_loss: 0.012883\n",
      "[170/01079] train_loss: 0.013641\n",
      "[170/01129] train_loss: 0.013277\n",
      "[170/01179] train_loss: 0.013824\n",
      "[171/00003] train_loss: 0.013995\n",
      "[171/00053] train_loss: 0.015303\n",
      "[171/00103] train_loss: 0.014487\n",
      "[171/00153] train_loss: 0.014141\n",
      "[171/00203] train_loss: 0.014606\n",
      "[171/00253] train_loss: 0.014698\n",
      "[171/00303] train_loss: 0.014285\n",
      "[171/00353] train_loss: 0.012983\n",
      "[171/00403] train_loss: 0.013182\n",
      "[171/00453] train_loss: 0.013962\n",
      "[171/00503] train_loss: 0.013884\n",
      "[171/00553] train_loss: 0.014862\n",
      "[171/00603] train_loss: 0.014209\n",
      "[171/00653] train_loss: 0.014671\n",
      "[171/00703] train_loss: 0.013783\n",
      "[171/00753] train_loss: 0.012876\n",
      "[171/00803] train_loss: 0.014293\n",
      "[171/00853] train_loss: 0.014409\n",
      "[171/00903] train_loss: 0.012684\n",
      "[171/00953] train_loss: 0.014208\n",
      "[171/01003] train_loss: 0.013480\n",
      "[171/01053] train_loss: 0.013134\n",
      "[171/01103] train_loss: 0.013438\n",
      "[171/01153] train_loss: 0.013609\n",
      "[171/01203] train_loss: 0.013713\n",
      "[172/00027] train_loss: 0.014321\n",
      "[172/00077] train_loss: 0.014911\n",
      "[172/00127] train_loss: 0.013978\n",
      "[172/00177] train_loss: 0.014837\n",
      "[172/00227] train_loss: 0.014319\n",
      "[172/00277] train_loss: 0.013766\n",
      "[172/00327] train_loss: 0.013846\n",
      "[172/00377] train_loss: 0.013911\n",
      "[172/00427] train_loss: 0.013226\n",
      "[172/00477] train_loss: 0.013595\n",
      "[172/00527] train_loss: 0.013156\n",
      "[172/00577] train_loss: 0.013550\n",
      "[172/00627] train_loss: 0.014300\n",
      "[172/00677] train_loss: 0.013606\n",
      "[172/00727] train_loss: 0.015740\n",
      "[172/00777] train_loss: 0.013762\n",
      "[172/00827] train_loss: 0.013050\n",
      "[172/00877] train_loss: 0.013225\n",
      "[172/00927] train_loss: 0.012852\n",
      "[172/00977] train_loss: 0.014105\n",
      "[172/01027] train_loss: 0.013788\n",
      "[172/01077] train_loss: 0.013019\n",
      "[172/01127] train_loss: 0.014054\n",
      "[172/01177] train_loss: 0.013975\n",
      "[173/00001] train_loss: 0.013231\n",
      "[173/00051] train_loss: 0.015372\n",
      "[173/00101] train_loss: 0.014867\n",
      "[173/00151] train_loss: 0.014274\n",
      "[173/00201] train_loss: 0.014110\n",
      "[173/00251] train_loss: 0.014659\n",
      "[173/00301] train_loss: 0.013628\n",
      "[173/00351] train_loss: 0.014739\n",
      "[173/00401] train_loss: 0.013957\n",
      "[173/00451] train_loss: 0.013829\n",
      "[173/00501] train_loss: 0.013324\n",
      "[173/00551] train_loss: 0.013114\n",
      "[173/00601] train_loss: 0.013666\n",
      "[173/00651] train_loss: 0.014056\n",
      "[173/00701] train_loss: 0.014800\n",
      "[173/00751] train_loss: 0.013532\n",
      "[173/00801] train_loss: 0.012792\n",
      "[173/00851] train_loss: 0.013753\n",
      "[173/00901] train_loss: 0.013171\n",
      "[173/00951] train_loss: 0.013471\n",
      "[173/01001] train_loss: 0.013688\n",
      "[173/01051] train_loss: 0.013406\n",
      "[173/01101] train_loss: 0.013340\n",
      "[173/01151] train_loss: 0.014319\n",
      "[173/01201] train_loss: 0.013308\n",
      "[174/00025] train_loss: 0.014491\n",
      "[174/00075] train_loss: 0.015188\n",
      "[174/00125] train_loss: 0.014928\n",
      "[174/00175] train_loss: 0.015030\n",
      "[174/00225] train_loss: 0.014509\n",
      "[174/00275] train_loss: 0.014341\n",
      "[174/00325] train_loss: 0.013879\n",
      "[174/00375] train_loss: 0.013472\n",
      "[174/00425] train_loss: 0.014504\n",
      "[174/00475] train_loss: 0.013184\n",
      "[174/00525] train_loss: 0.013515\n",
      "[174/00575] train_loss: 0.013231\n",
      "[174/00625] train_loss: 0.013720\n",
      "[174/00675] train_loss: 0.013691\n",
      "[174/00725] train_loss: 0.013217\n",
      "[174/00775] train_loss: 0.013914\n",
      "[174/00825] train_loss: 0.014236\n",
      "[174/00875] train_loss: 0.013270\n",
      "[174/00925] train_loss: 0.013871\n",
      "[174/00975] train_loss: 0.013354\n",
      "[174/01025] train_loss: 0.013012\n",
      "[174/01075] train_loss: 0.013619\n",
      "[174/01125] train_loss: 0.013635\n",
      "[174/01175] train_loss: 0.013574\n",
      "[174/01225] train_loss: 0.012960\n",
      "[175/00049] train_loss: 0.014952\n",
      "[175/00099] train_loss: 0.014178\n",
      "[175/00149] train_loss: 0.014190\n",
      "[175/00199] train_loss: 0.015124\n",
      "[175/00249] train_loss: 0.014350\n",
      "[175/00299] train_loss: 0.014243\n",
      "[175/00349] train_loss: 0.013817\n",
      "[175/00399] train_loss: 0.013681\n",
      "[175/00449] train_loss: 0.013252\n",
      "[175/00499] train_loss: 0.013488\n",
      "[175/00549] train_loss: 0.012704\n",
      "[175/00599] train_loss: 0.013241\n",
      "[175/00649] train_loss: 0.014179\n",
      "[175/00699] train_loss: 0.013803\n",
      "[175/00749] train_loss: 0.013597\n",
      "[175/00799] train_loss: 0.013905\n",
      "[175/00849] train_loss: 0.013679\n",
      "[175/00899] train_loss: 0.014214\n",
      "[175/00949] train_loss: 0.014113\n",
      "[175/00999] train_loss: 0.013651\n",
      "[175/01049] train_loss: 0.013923\n",
      "[175/01099] train_loss: 0.014197\n",
      "[175/01149] train_loss: 0.012404\n",
      "[175/01199] train_loss: 0.013955\n",
      "[176/00023] train_loss: 0.014930\n",
      "[176/00073] train_loss: 0.014656\n",
      "[176/00123] train_loss: 0.015961\n",
      "[176/00173] train_loss: 0.014332\n",
      "[176/00223] train_loss: 0.013783\n",
      "[176/00273] train_loss: 0.014098\n",
      "[176/00323] train_loss: 0.014005\n",
      "[176/00373] train_loss: 0.013855\n",
      "[176/00423] train_loss: 0.013970\n",
      "[176/00473] train_loss: 0.013871\n",
      "[176/00523] train_loss: 0.013965\n",
      "[176/00573] train_loss: 0.013707\n",
      "[176/00623] train_loss: 0.013146\n",
      "[176/00673] train_loss: 0.013722\n",
      "[176/00723] train_loss: 0.013101\n",
      "[176/00773] train_loss: 0.013960\n",
      "[176/00823] train_loss: 0.013905\n",
      "[176/00873] train_loss: 0.013269\n",
      "[176/00923] train_loss: 0.013462\n",
      "[176/00973] train_loss: 0.013510\n",
      "[176/01023] train_loss: 0.013370\n",
      "[176/01073] train_loss: 0.013718\n",
      "[176/01123] train_loss: 0.012834\n",
      "[176/01173] train_loss: 0.013546\n",
      "[176/01223] train_loss: 0.013481\n",
      "[177/00047] train_loss: 0.014314\n",
      "[177/00097] train_loss: 0.015047\n",
      "[177/00147] train_loss: 0.014300\n",
      "[177/00197] train_loss: 0.013996\n",
      "[177/00247] train_loss: 0.014410\n",
      "[177/00297] train_loss: 0.013478\n",
      "[177/00347] train_loss: 0.012848\n",
      "[177/00397] train_loss: 0.014754\n",
      "[177/00447] train_loss: 0.013874\n",
      "[177/00497] train_loss: 0.013998\n",
      "[177/00547] train_loss: 0.013609\n",
      "[177/00597] train_loss: 0.012987\n",
      "[177/00647] train_loss: 0.013812\n",
      "[177/00697] train_loss: 0.013795\n",
      "[177/00747] train_loss: 0.014019\n",
      "[177/00797] train_loss: 0.013887\n",
      "[177/00847] train_loss: 0.014527\n",
      "[177/00897] train_loss: 0.013669\n",
      "[177/00947] train_loss: 0.013450\n",
      "[177/00997] train_loss: 0.013789\n",
      "[177/01047] train_loss: 0.013616\n",
      "[177/01097] train_loss: 0.013287\n",
      "[177/01147] train_loss: 0.013141\n",
      "[177/01197] train_loss: 0.013618\n",
      "[178/00021] train_loss: 0.014108\n",
      "[178/00071] train_loss: 0.014714\n",
      "[178/00121] train_loss: 0.014215\n",
      "[178/00171] train_loss: 0.013754\n",
      "[178/00221] train_loss: 0.014092\n",
      "[178/00271] train_loss: 0.014028\n",
      "[178/00321] train_loss: 0.014111\n",
      "[178/00371] train_loss: 0.014073\n",
      "[178/00421] train_loss: 0.012924\n",
      "[178/00471] train_loss: 0.013251\n",
      "[178/00521] train_loss: 0.013530\n",
      "[178/00571] train_loss: 0.014272\n",
      "[178/00621] train_loss: 0.014048\n",
      "[178/00671] train_loss: 0.014320\n",
      "[178/00721] train_loss: 0.013683\n",
      "[178/00771] train_loss: 0.013846\n",
      "[178/00821] train_loss: 0.014181\n",
      "[178/00871] train_loss: 0.013603\n",
      "[178/00921] train_loss: 0.013510\n",
      "[178/00971] train_loss: 0.013491\n",
      "[178/01021] train_loss: 0.013652\n",
      "[178/01071] train_loss: 0.012993\n",
      "[178/01121] train_loss: 0.013733\n",
      "[178/01171] train_loss: 0.013487\n",
      "[178/01221] train_loss: 0.012981\n",
      "[179/00045] train_loss: 0.015137\n",
      "[179/00095] train_loss: 0.015285\n",
      "[179/00145] train_loss: 0.014008\n",
      "[179/00195] train_loss: 0.014181\n",
      "[179/00245] train_loss: 0.014288\n",
      "[179/00295] train_loss: 0.013800\n",
      "[179/00345] train_loss: 0.013768\n",
      "[179/00395] train_loss: 0.013260\n",
      "[179/00445] train_loss: 0.014209\n",
      "[179/00495] train_loss: 0.013622\n",
      "[179/00545] train_loss: 0.013516\n",
      "[179/00595] train_loss: 0.013785\n",
      "[179/00645] train_loss: 0.013026\n",
      "[179/00695] train_loss: 0.014388\n",
      "[179/00745] train_loss: 0.013829\n",
      "[179/00795] train_loss: 0.013439\n",
      "[179/00845] train_loss: 0.012938\n",
      "[179/00895] train_loss: 0.013195\n",
      "[179/00945] train_loss: 0.013379\n",
      "[179/00995] train_loss: 0.013535\n",
      "[179/01045] train_loss: 0.013062\n",
      "[179/01095] train_loss: 0.013766\n",
      "[179/01145] train_loss: 0.013688\n",
      "[179/01195] train_loss: 0.013397\n",
      "[180/00019] train_loss: 0.013745\n",
      "[180/00069] train_loss: 0.014368\n",
      "[180/00119] train_loss: 0.014915\n",
      "[180/00169] train_loss: 0.014882\n",
      "[180/00219] train_loss: 0.013933\n",
      "[180/00269] train_loss: 0.013863\n",
      "[180/00319] train_loss: 0.013564\n",
      "[180/00369] train_loss: 0.014348\n",
      "[180/00419] train_loss: 0.013360\n",
      "[180/00469] train_loss: 0.013474\n",
      "[180/00519] train_loss: 0.013863\n",
      "[180/00569] train_loss: 0.012932\n",
      "[180/00619] train_loss: 0.013673\n",
      "[180/00669] train_loss: 0.014267\n",
      "[180/00719] train_loss: 0.013240\n",
      "[180/00769] train_loss: 0.014047\n",
      "[180/00819] train_loss: 0.012777\n",
      "[180/00869] train_loss: 0.014367\n",
      "[180/00919] train_loss: 0.013678\n",
      "[180/00969] train_loss: 0.013575\n",
      "[180/01019] train_loss: 0.013408\n",
      "[180/01069] train_loss: 0.013679\n",
      "[180/01119] train_loss: 0.013214\n",
      "[180/01169] train_loss: 0.013364\n",
      "[180/01219] train_loss: 0.013584\n",
      "[181/00043] train_loss: 0.015211\n",
      "[181/00093] train_loss: 0.014562\n",
      "[181/00143] train_loss: 0.014151\n",
      "[181/00193] train_loss: 0.014088\n",
      "[181/00243] train_loss: 0.014003\n",
      "[181/00293] train_loss: 0.013958\n",
      "[181/00343] train_loss: 0.013713\n",
      "[181/00393] train_loss: 0.013399\n",
      "[181/00443] train_loss: 0.014889\n",
      "[181/00493] train_loss: 0.013587\n",
      "[181/00543] train_loss: 0.014411\n",
      "[181/00593] train_loss: 0.013144\n",
      "[181/00643] train_loss: 0.013726\n",
      "[181/00693] train_loss: 0.013809\n",
      "[181/00743] train_loss: 0.013771\n",
      "[181/00793] train_loss: 0.014244\n",
      "[181/00843] train_loss: 0.012879\n",
      "[181/00893] train_loss: 0.013372\n",
      "[181/00943] train_loss: 0.012780\n",
      "[181/00993] train_loss: 0.012997\n",
      "[181/01043] train_loss: 0.013565\n",
      "[181/01093] train_loss: 0.013742\n",
      "[181/01143] train_loss: 0.013870\n",
      "[181/01193] train_loss: 0.012805\n",
      "[182/00017] train_loss: 0.014850\n",
      "[182/00067] train_loss: 0.014704\n",
      "[182/00117] train_loss: 0.014532\n",
      "[182/00167] train_loss: 0.013531\n",
      "[182/00217] train_loss: 0.013849\n",
      "[182/00267] train_loss: 0.014091\n",
      "[182/00317] train_loss: 0.013712\n",
      "[182/00367] train_loss: 0.013369\n",
      "[182/00417] train_loss: 0.014276\n",
      "[182/00467] train_loss: 0.014233\n",
      "[182/00517] train_loss: 0.013871\n",
      "[182/00567] train_loss: 0.013061\n",
      "[182/00617] train_loss: 0.013052\n",
      "[182/00667] train_loss: 0.013770\n",
      "[182/00717] train_loss: 0.013696\n",
      "[182/00767] train_loss: 0.013297\n",
      "[182/00817] train_loss: 0.013104\n",
      "[182/00867] train_loss: 0.013903\n",
      "[182/00917] train_loss: 0.013325\n",
      "[182/00967] train_loss: 0.013769\n",
      "[182/01017] train_loss: 0.013326\n",
      "[182/01067] train_loss: 0.013098\n",
      "[182/01117] train_loss: 0.013819\n",
      "[182/01167] train_loss: 0.014129\n",
      "[182/01217] train_loss: 0.013707\n",
      "[183/00041] train_loss: 0.014929\n",
      "[183/00091] train_loss: 0.014488\n",
      "[183/00141] train_loss: 0.014059\n",
      "[183/00191] train_loss: 0.014652\n",
      "[183/00241] train_loss: 0.013503\n",
      "[183/00291] train_loss: 0.013876\n",
      "[183/00341] train_loss: 0.014536\n",
      "[183/00391] train_loss: 0.013565\n",
      "[183/00441] train_loss: 0.013967\n",
      "[183/00491] train_loss: 0.014221\n",
      "[183/00541] train_loss: 0.013343\n",
      "[183/00591] train_loss: 0.013594\n",
      "[183/00641] train_loss: 0.013917\n",
      "[183/00691] train_loss: 0.014785\n",
      "[183/00741] train_loss: 0.014170\n",
      "[183/00791] train_loss: 0.013071\n",
      "[183/00841] train_loss: 0.012609\n",
      "[183/00891] train_loss: 0.012811\n",
      "[183/00941] train_loss: 0.013292\n",
      "[183/00991] train_loss: 0.013848\n",
      "[183/01041] train_loss: 0.013363\n",
      "[183/01091] train_loss: 0.013737\n",
      "[183/01141] train_loss: 0.012857\n",
      "[183/01191] train_loss: 0.013382\n",
      "[184/00015] train_loss: 0.013304\n",
      "[184/00065] train_loss: 0.014498\n",
      "[184/00115] train_loss: 0.014703\n",
      "[184/00165] train_loss: 0.014269\n",
      "[184/00215] train_loss: 0.013457\n",
      "[184/00265] train_loss: 0.014289\n",
      "[184/00315] train_loss: 0.014206\n",
      "[184/00365] train_loss: 0.014211\n",
      "[184/00415] train_loss: 0.013642\n",
      "[184/00465] train_loss: 0.013655\n",
      "[184/00515] train_loss: 0.013734\n",
      "[184/00565] train_loss: 0.013444\n",
      "[184/00615] train_loss: 0.013901\n",
      "[184/00665] train_loss: 0.013529\n",
      "[184/00715] train_loss: 0.013459\n",
      "[184/00765] train_loss: 0.013658\n",
      "[184/00815] train_loss: 0.013122\n",
      "[184/00865] train_loss: 0.014107\n",
      "[184/00915] train_loss: 0.013743\n",
      "[184/00965] train_loss: 0.013208\n",
      "[184/01015] train_loss: 0.012928\n",
      "[184/01065] train_loss: 0.014394\n",
      "[184/01115] train_loss: 0.013509\n",
      "[184/01165] train_loss: 0.013801\n",
      "[184/01215] train_loss: 0.014141\n",
      "[185/00039] train_loss: 0.014772\n",
      "[185/00089] train_loss: 0.014952\n",
      "[185/00139] train_loss: 0.014195\n",
      "[185/00189] train_loss: 0.013671\n",
      "[185/00239] train_loss: 0.013920\n",
      "[185/00289] train_loss: 0.014280\n",
      "[185/00339] train_loss: 0.013605\n",
      "[185/00389] train_loss: 0.013844\n",
      "[185/00439] train_loss: 0.014346\n",
      "[185/00489] train_loss: 0.014760\n",
      "[185/00539] train_loss: 0.013505\n",
      "[185/00589] train_loss: 0.013136\n",
      "[185/00639] train_loss: 0.013236\n",
      "[185/00689] train_loss: 0.013370\n",
      "[185/00739] train_loss: 0.013641\n",
      "[185/00789] train_loss: 0.013636\n",
      "[185/00839] train_loss: 0.012732\n",
      "[185/00889] train_loss: 0.014494\n",
      "[185/00939] train_loss: 0.013447\n",
      "[185/00989] train_loss: 0.012904\n",
      "[185/01039] train_loss: 0.013043\n",
      "[185/01089] train_loss: 0.013543\n",
      "[185/01139] train_loss: 0.013442\n",
      "[185/01189] train_loss: 0.013902\n",
      "[186/00013] train_loss: 0.014454\n",
      "[186/00063] train_loss: 0.013934\n",
      "[186/00113] train_loss: 0.014380\n",
      "[186/00163] train_loss: 0.013999\n",
      "[186/00213] train_loss: 0.014296\n",
      "[186/00263] train_loss: 0.013761\n",
      "[186/00313] train_loss: 0.013706\n",
      "[186/00363] train_loss: 0.014078\n",
      "[186/00413] train_loss: 0.013665\n",
      "[186/00463] train_loss: 0.013563\n",
      "[186/00513] train_loss: 0.014102\n",
      "[186/00563] train_loss: 0.013800\n",
      "[186/00613] train_loss: 0.012874\n",
      "[186/00663] train_loss: 0.013848\n",
      "[186/00713] train_loss: 0.014320\n",
      "[186/00763] train_loss: 0.013355\n",
      "[186/00813] train_loss: 0.014146\n",
      "[186/00863] train_loss: 0.013665\n",
      "[186/00913] train_loss: 0.013226\n",
      "[186/00963] train_loss: 0.013137\n",
      "[186/01013] train_loss: 0.013166\n",
      "[186/01063] train_loss: 0.012662\n",
      "[186/01113] train_loss: 0.013682\n",
      "[186/01163] train_loss: 0.013153\n",
      "[186/01213] train_loss: 0.013506\n",
      "[187/00037] train_loss: 0.014501\n",
      "[187/00087] train_loss: 0.015009\n",
      "[187/00137] train_loss: 0.014260\n",
      "[187/00187] train_loss: 0.014338\n",
      "[187/00237] train_loss: 0.013948\n",
      "[187/00287] train_loss: 0.013332\n",
      "[187/00337] train_loss: 0.014403\n",
      "[187/00387] train_loss: 0.013291\n",
      "[187/00437] train_loss: 0.014309\n",
      "[187/00487] train_loss: 0.012819\n",
      "[187/00537] train_loss: 0.013048\n",
      "[187/00587] train_loss: 0.013780\n",
      "[187/00637] train_loss: 0.013564\n",
      "[187/00687] train_loss: 0.013177\n",
      "[187/00737] train_loss: 0.013694\n",
      "[187/00787] train_loss: 0.013947\n",
      "[187/00837] train_loss: 0.013326\n",
      "[187/00887] train_loss: 0.014413\n",
      "[187/00937] train_loss: 0.014077\n",
      "[187/00987] train_loss: 0.014067\n",
      "[187/01037] train_loss: 0.013312\n",
      "[187/01087] train_loss: 0.012783\n",
      "[187/01137] train_loss: 0.012888\n",
      "[187/01187] train_loss: 0.013029\n",
      "[188/00011] train_loss: 0.012688\n",
      "[188/00061] train_loss: 0.014618\n",
      "[188/00111] train_loss: 0.014889\n",
      "[188/00161] train_loss: 0.015260\n",
      "[188/00211] train_loss: 0.013613\n",
      "[188/00261] train_loss: 0.013831\n",
      "[188/00311] train_loss: 0.013938\n",
      "[188/00361] train_loss: 0.014497\n",
      "[188/00411] train_loss: 0.013239\n",
      "[188/00461] train_loss: 0.013278\n",
      "[188/00511] train_loss: 0.013834\n",
      "[188/00561] train_loss: 0.013818\n",
      "[188/00611] train_loss: 0.013174\n",
      "[188/00661] train_loss: 0.014106\n",
      "[188/00711] train_loss: 0.012585\n",
      "[188/00761] train_loss: 0.013358\n",
      "[188/00811] train_loss: 0.013445\n",
      "[188/00861] train_loss: 0.013147\n",
      "[188/00911] train_loss: 0.013314\n",
      "[188/00961] train_loss: 0.013816\n",
      "[188/01011] train_loss: 0.013337\n",
      "[188/01061] train_loss: 0.012604\n",
      "[188/01111] train_loss: 0.014182\n",
      "[188/01161] train_loss: 0.012702\n",
      "[188/01211] train_loss: 0.013791\n",
      "[189/00035] train_loss: 0.014567\n",
      "[189/00085] train_loss: 0.015421\n",
      "[189/00135] train_loss: 0.014193\n",
      "[189/00185] train_loss: 0.014330\n",
      "[189/00235] train_loss: 0.013772\n",
      "[189/00285] train_loss: 0.013047\n",
      "[189/00335] train_loss: 0.013377\n",
      "[189/00385] train_loss: 0.013842\n",
      "[189/00435] train_loss: 0.013422\n",
      "[189/00485] train_loss: 0.013096\n",
      "[189/00535] train_loss: 0.013579\n",
      "[189/00585] train_loss: 0.014862\n",
      "[189/00635] train_loss: 0.014100\n",
      "[189/00685] train_loss: 0.013400\n",
      "[189/00735] train_loss: 0.014161\n",
      "[189/00785] train_loss: 0.012723\n",
      "[189/00835] train_loss: 0.013041\n",
      "[189/00885] train_loss: 0.013742\n",
      "[189/00935] train_loss: 0.014644\n",
      "[189/00985] train_loss: 0.013991\n",
      "[189/01035] train_loss: 0.013922\n",
      "[189/01085] train_loss: 0.013259\n",
      "[189/01135] train_loss: 0.013440\n",
      "[189/01185] train_loss: 0.013105\n",
      "[190/00009] train_loss: 0.013278\n",
      "[190/00059] train_loss: 0.014141\n",
      "[190/00109] train_loss: 0.013614\n",
      "[190/00159] train_loss: 0.013496\n",
      "[190/00209] train_loss: 0.013757\n",
      "[190/00259] train_loss: 0.014312\n",
      "[190/00309] train_loss: 0.013350\n",
      "[190/00359] train_loss: 0.013622\n",
      "[190/00409] train_loss: 0.013122\n",
      "[190/00459] train_loss: 0.013794\n",
      "[190/00509] train_loss: 0.013347\n",
      "[190/00559] train_loss: 0.013697\n",
      "[190/00609] train_loss: 0.013587\n",
      "[190/00659] train_loss: 0.013887\n",
      "[190/00709] train_loss: 0.012881\n",
      "[190/00759] train_loss: 0.013380\n",
      "[190/00809] train_loss: 0.013802\n",
      "[190/00859] train_loss: 0.013408\n",
      "[190/00909] train_loss: 0.014163\n",
      "[190/00959] train_loss: 0.013645\n",
      "[190/01009] train_loss: 0.013829\n",
      "[190/01059] train_loss: 0.013274\n",
      "[190/01109] train_loss: 0.014075\n",
      "[190/01159] train_loss: 0.013417\n",
      "[190/01209] train_loss: 0.013071\n",
      "[191/00033] train_loss: 0.014970\n",
      "[191/00083] train_loss: 0.014391\n",
      "[191/00133] train_loss: 0.013927\n",
      "[191/00183] train_loss: 0.013803\n",
      "[191/00233] train_loss: 0.014041\n",
      "[191/00283] train_loss: 0.014660\n",
      "[191/00333] train_loss: 0.013566\n",
      "[191/00383] train_loss: 0.013183\n",
      "[191/00433] train_loss: 0.013177\n",
      "[191/00483] train_loss: 0.013800\n",
      "[191/00533] train_loss: 0.014278\n",
      "[191/00583] train_loss: 0.012540\n",
      "[191/00633] train_loss: 0.013336\n",
      "[191/00683] train_loss: 0.013469\n",
      "[191/00733] train_loss: 0.014215\n",
      "[191/00783] train_loss: 0.014014\n",
      "[191/00833] train_loss: 0.013003\n",
      "[191/00883] train_loss: 0.013142\n",
      "[191/00933] train_loss: 0.013041\n",
      "[191/00983] train_loss: 0.013969\n",
      "[191/01033] train_loss: 0.013365\n",
      "[191/01083] train_loss: 0.013445\n",
      "[191/01133] train_loss: 0.013665\n",
      "[191/01183] train_loss: 0.014363\n",
      "[192/00007] train_loss: 0.013517\n",
      "[192/00057] train_loss: 0.014935\n",
      "[192/00107] train_loss: 0.014434\n",
      "[192/00157] train_loss: 0.013648\n",
      "[192/00207] train_loss: 0.014171\n",
      "[192/00257] train_loss: 0.014672\n",
      "[192/00307] train_loss: 0.014072\n",
      "[192/00357] train_loss: 0.013395\n",
      "[192/00407] train_loss: 0.013801\n",
      "[192/00457] train_loss: 0.015251\n",
      "[192/00507] train_loss: 0.013489\n",
      "[192/00557] train_loss: 0.013677\n",
      "[192/00607] train_loss: 0.013592\n",
      "[192/00657] train_loss: 0.013321\n",
      "[192/00707] train_loss: 0.013256\n",
      "[192/00757] train_loss: 0.012901\n",
      "[192/00807] train_loss: 0.013773\n",
      "[192/00857] train_loss: 0.012723\n",
      "[192/00907] train_loss: 0.013653\n",
      "[192/00957] train_loss: 0.013069\n",
      "[192/01007] train_loss: 0.013124\n",
      "[192/01057] train_loss: 0.013413\n",
      "[192/01107] train_loss: 0.013061\n",
      "[192/01157] train_loss: 0.012804\n",
      "[192/01207] train_loss: 0.013398\n",
      "[193/00031] train_loss: 0.014225\n",
      "[193/00081] train_loss: 0.014269\n",
      "[193/00131] train_loss: 0.013888\n",
      "[193/00181] train_loss: 0.013927\n",
      "[193/00231] train_loss: 0.013890\n",
      "[193/00281] train_loss: 0.014471\n",
      "[193/00331] train_loss: 0.013961\n",
      "[193/00381] train_loss: 0.013807\n",
      "[193/00431] train_loss: 0.013372\n",
      "[193/00481] train_loss: 0.013838\n",
      "[193/00531] train_loss: 0.013234\n",
      "[193/00581] train_loss: 0.014034\n",
      "[193/00631] train_loss: 0.013345\n",
      "[193/00681] train_loss: 0.013456\n",
      "[193/00731] train_loss: 0.013699\n",
      "[193/00781] train_loss: 0.013698\n",
      "[193/00831] train_loss: 0.013855\n",
      "[193/00881] train_loss: 0.013423\n",
      "[193/00931] train_loss: 0.013342\n",
      "[193/00981] train_loss: 0.013788\n",
      "[193/01031] train_loss: 0.013240\n",
      "[193/01081] train_loss: 0.013080\n",
      "[193/01131] train_loss: 0.013215\n",
      "[193/01181] train_loss: 0.013002\n",
      "[194/00005] train_loss: 0.013056\n",
      "[194/00055] train_loss: 0.013905\n",
      "[194/00105] train_loss: 0.014727\n",
      "[194/00155] train_loss: 0.014477\n",
      "[194/00205] train_loss: 0.014709\n",
      "[194/00255] train_loss: 0.013376\n",
      "[194/00305] train_loss: 0.014120\n",
      "[194/00355] train_loss: 0.013018\n",
      "[194/00405] train_loss: 0.013579\n",
      "[194/00455] train_loss: 0.014129\n",
      "[194/00505] train_loss: 0.013836\n",
      "[194/00555] train_loss: 0.013150\n",
      "[194/00605] train_loss: 0.013633\n",
      "[194/00655] train_loss: 0.012995\n",
      "[194/00705] train_loss: 0.012758\n",
      "[194/00755] train_loss: 0.013501\n",
      "[194/00805] train_loss: 0.013653\n",
      "[194/00855] train_loss: 0.013294\n",
      "[194/00905] train_loss: 0.013267\n",
      "[194/00955] train_loss: 0.013433\n",
      "[194/01005] train_loss: 0.013406\n",
      "[194/01055] train_loss: 0.013770\n",
      "[194/01105] train_loss: 0.012682\n",
      "[194/01155] train_loss: 0.013631\n",
      "[194/01205] train_loss: 0.013488\n",
      "[195/00029] train_loss: 0.014324\n",
      "[195/00079] train_loss: 0.014859\n",
      "[195/00129] train_loss: 0.014317\n",
      "[195/00179] train_loss: 0.014043\n",
      "[195/00229] train_loss: 0.013577\n",
      "[195/00279] train_loss: 0.013462\n",
      "[195/00329] train_loss: 0.013503\n",
      "[195/00379] train_loss: 0.014138\n",
      "[195/00429] train_loss: 0.013627\n",
      "[195/00479] train_loss: 0.013436\n",
      "[195/00529] train_loss: 0.012705\n",
      "[195/00579] train_loss: 0.014152\n",
      "[195/00629] train_loss: 0.013292\n",
      "[195/00679] train_loss: 0.013376\n",
      "[195/00729] train_loss: 0.013142\n",
      "[195/00779] train_loss: 0.013716\n",
      "[195/00829] train_loss: 0.013310\n",
      "[195/00879] train_loss: 0.013495\n",
      "[195/00929] train_loss: 0.013392\n",
      "[195/00979] train_loss: 0.012873\n",
      "[195/01029] train_loss: 0.013343\n",
      "[195/01079] train_loss: 0.012766\n",
      "[195/01129] train_loss: 0.013124\n",
      "[195/01179] train_loss: 0.013832\n",
      "[196/00003] train_loss: 0.012618\n",
      "[196/00053] train_loss: 0.014896\n",
      "[196/00103] train_loss: 0.015364\n",
      "[196/00153] train_loss: 0.013558\n",
      "[196/00203] train_loss: 0.014413\n",
      "[196/00253] train_loss: 0.013622\n",
      "[196/00303] train_loss: 0.013619\n",
      "[196/00353] train_loss: 0.013064\n",
      "[196/00403] train_loss: 0.013747\n",
      "[196/00453] train_loss: 0.013632\n",
      "[196/00503] train_loss: 0.013942\n",
      "[196/00553] train_loss: 0.013535\n",
      "[196/00603] train_loss: 0.012455\n",
      "[196/00653] train_loss: 0.013162\n",
      "[196/00703] train_loss: 0.013178\n",
      "[196/00753] train_loss: 0.013606\n",
      "[196/00803] train_loss: 0.012953\n",
      "[196/00853] train_loss: 0.013752\n",
      "[196/00903] train_loss: 0.013486\n",
      "[196/00953] train_loss: 0.012840\n",
      "[196/01003] train_loss: 0.013161\n",
      "[196/01053] train_loss: 0.013009\n",
      "[196/01103] train_loss: 0.013714\n",
      "[196/01153] train_loss: 0.014094\n",
      "[196/01203] train_loss: 0.012850\n",
      "[197/00027] train_loss: 0.014632\n",
      "[197/00077] train_loss: 0.014508\n",
      "[197/00127] train_loss: 0.014054\n",
      "[197/00177] train_loss: 0.013719\n",
      "[197/00227] train_loss: 0.013261\n",
      "[197/00277] train_loss: 0.014252\n",
      "[197/00327] train_loss: 0.013185\n",
      "[197/00377] train_loss: 0.013775\n",
      "[197/00427] train_loss: 0.013496\n",
      "[197/00477] train_loss: 0.014330\n",
      "[197/00527] train_loss: 0.013551\n",
      "[197/00577] train_loss: 0.013587\n",
      "[197/00627] train_loss: 0.012842\n",
      "[197/00677] train_loss: 0.013339\n",
      "[197/00727] train_loss: 0.013118\n",
      "[197/00777] train_loss: 0.013165\n",
      "[197/00827] train_loss: 0.013392\n",
      "[197/00877] train_loss: 0.013502\n",
      "[197/00927] train_loss: 0.013607\n",
      "[197/00977] train_loss: 0.013940\n",
      "[197/01027] train_loss: 0.013091\n",
      "[197/01077] train_loss: 0.013661\n",
      "[197/01127] train_loss: 0.013492\n",
      "[197/01177] train_loss: 0.013841\n",
      "[198/00001] train_loss: 0.013944\n",
      "[198/00051] train_loss: 0.014525\n",
      "[198/00101] train_loss: 0.014118\n",
      "[198/00151] train_loss: 0.014177\n",
      "[198/00201] train_loss: 0.013718\n",
      "[198/00251] train_loss: 0.013759\n",
      "[198/00301] train_loss: 0.013925\n",
      "[198/00351] train_loss: 0.014150\n",
      "[198/00401] train_loss: 0.013712\n",
      "[198/00451] train_loss: 0.013398\n",
      "[198/00501] train_loss: 0.012762\n",
      "[198/00551] train_loss: 0.013406\n",
      "[198/00601] train_loss: 0.013938\n",
      "[198/00651] train_loss: 0.013573\n",
      "[198/00701] train_loss: 0.012808\n",
      "[198/00751] train_loss: 0.013670\n",
      "[198/00801] train_loss: 0.013345\n",
      "[198/00851] train_loss: 0.013674\n",
      "[198/00901] train_loss: 0.013126\n",
      "[198/00951] train_loss: 0.013041\n",
      "[198/01001] train_loss: 0.013110\n",
      "[198/01051] train_loss: 0.013696\n",
      "[198/01101] train_loss: 0.014250\n",
      "[198/01151] train_loss: 0.012427\n",
      "[198/01201] train_loss: 0.013812\n",
      "[199/00025] train_loss: 0.013982\n",
      "[199/00075] train_loss: 0.014180\n",
      "[199/00125] train_loss: 0.014103\n",
      "[199/00175] train_loss: 0.013870\n",
      "[199/00225] train_loss: 0.014003\n",
      "[199/00275] train_loss: 0.013475\n",
      "[199/00325] train_loss: 0.014258\n",
      "[199/00375] train_loss: 0.013540\n",
      "[199/00425] train_loss: 0.013865\n",
      "[199/00475] train_loss: 0.013774\n",
      "[199/00525] train_loss: 0.013191\n",
      "[199/00575] train_loss: 0.013572\n",
      "[199/00625] train_loss: 0.013165\n",
      "[199/00675] train_loss: 0.013324\n",
      "[199/00725] train_loss: 0.014202\n",
      "[199/00775] train_loss: 0.012864\n",
      "[199/00825] train_loss: 0.013486\n",
      "[199/00875] train_loss: 0.014078\n",
      "[199/00925] train_loss: 0.013592\n",
      "[199/00975] train_loss: 0.013108\n",
      "[199/01025] train_loss: 0.014500\n",
      "[199/01075] train_loss: 0.012812\n",
      "[199/01125] train_loss: 0.013699\n",
      "[199/01175] train_loss: 0.012942\n",
      "[199/01225] train_loss: 0.012477\n",
      "[200/00049] train_loss: 0.015096\n",
      "[200/00099] train_loss: 0.015077\n",
      "[200/00149] train_loss: 0.013217\n",
      "[200/00199] train_loss: 0.013836\n",
      "[200/00249] train_loss: 0.014164\n",
      "[200/00299] train_loss: 0.013613\n",
      "[200/00349] train_loss: 0.013245\n",
      "[200/00399] train_loss: 0.014025\n",
      "[200/00449] train_loss: 0.014019\n",
      "[200/00499] train_loss: 0.013721\n",
      "[200/00549] train_loss: 0.013503\n",
      "[200/00599] train_loss: 0.013663\n",
      "[200/00649] train_loss: 0.012922\n",
      "[200/00699] train_loss: 0.013768\n",
      "[200/00749] train_loss: 0.012537\n",
      "[200/00799] train_loss: 0.014292\n",
      "[200/00849] train_loss: 0.013494\n",
      "[200/00899] train_loss: 0.013100\n",
      "[200/00949] train_loss: 0.013799\n",
      "[200/00999] train_loss: 0.013503\n",
      "[200/01049] train_loss: 0.013505\n",
      "[200/01099] train_loss: 0.013422\n",
      "[200/01149] train_loss: 0.012874\n",
      "[200/01199] train_loss: 0.013211\n",
      "[201/00023] train_loss: 0.013833\n",
      "[201/00073] train_loss: 0.014391\n",
      "[201/00123] train_loss: 0.014254\n",
      "[201/00173] train_loss: 0.013876\n",
      "[201/00223] train_loss: 0.014097\n",
      "[201/00273] train_loss: 0.013106\n",
      "[201/00323] train_loss: 0.013163\n",
      "[201/00373] train_loss: 0.013532\n",
      "[201/00423] train_loss: 0.013615\n",
      "[201/00473] train_loss: 0.014526\n",
      "[201/00523] train_loss: 0.014144\n",
      "[201/00573] train_loss: 0.014019\n",
      "[201/00623] train_loss: 0.012599\n",
      "[201/00673] train_loss: 0.012463\n",
      "[201/00723] train_loss: 0.014615\n",
      "[201/00773] train_loss: 0.012687\n",
      "[201/00823] train_loss: 0.014040\n",
      "[201/00873] train_loss: 0.013436\n",
      "[201/00923] train_loss: 0.013985\n",
      "[201/00973] train_loss: 0.013863\n",
      "[201/01023] train_loss: 0.013156\n",
      "[201/01073] train_loss: 0.013317\n",
      "[201/01123] train_loss: 0.013399\n",
      "[201/01173] train_loss: 0.012886\n",
      "[201/01223] train_loss: 0.013334\n",
      "[202/00047] train_loss: 0.014703\n",
      "[202/00097] train_loss: 0.013861\n",
      "[202/00147] train_loss: 0.013757\n",
      "[202/00197] train_loss: 0.014707\n",
      "[202/00247] train_loss: 0.013077\n",
      "[202/00297] train_loss: 0.013988\n",
      "[202/00347] train_loss: 0.013443\n",
      "[202/00397] train_loss: 0.013194\n",
      "[202/00447] train_loss: 0.012998\n",
      "[202/00497] train_loss: 0.013700\n",
      "[202/00547] train_loss: 0.012930\n",
      "[202/00597] train_loss: 0.013152\n",
      "[202/00647] train_loss: 0.012968\n",
      "[202/00697] train_loss: 0.013065\n",
      "[202/00747] train_loss: 0.013526\n",
      "[202/00797] train_loss: 0.013790\n",
      "[202/00847] train_loss: 0.013585\n",
      "[202/00897] train_loss: 0.013472\n",
      "[202/00947] train_loss: 0.013074\n",
      "[202/00997] train_loss: 0.013457\n",
      "[202/01047] train_loss: 0.013471\n",
      "[202/01097] train_loss: 0.013780\n",
      "[202/01147] train_loss: 0.012445\n",
      "[202/01197] train_loss: 0.013881\n",
      "[203/00021] train_loss: 0.013790\n",
      "[203/00071] train_loss: 0.014958\n",
      "[203/00121] train_loss: 0.013581\n",
      "[203/00171] train_loss: 0.013772\n",
      "[203/00221] train_loss: 0.013592\n",
      "[203/00271] train_loss: 0.013809\n",
      "[203/00321] train_loss: 0.014035\n",
      "[203/00371] train_loss: 0.013231\n",
      "[203/00421] train_loss: 0.013792\n",
      "[203/00471] train_loss: 0.013262\n",
      "[203/00521] train_loss: 0.013923\n",
      "[203/00571] train_loss: 0.013114\n",
      "[203/00621] train_loss: 0.013880\n",
      "[203/00671] train_loss: 0.013442\n",
      "[203/00721] train_loss: 0.013817\n",
      "[203/00771] train_loss: 0.013581\n",
      "[203/00821] train_loss: 0.013614\n",
      "[203/00871] train_loss: 0.013608\n",
      "[203/00921] train_loss: 0.012813\n",
      "[203/00971] train_loss: 0.012523\n",
      "[203/01021] train_loss: 0.013123\n",
      "[203/01071] train_loss: 0.012959\n",
      "[203/01121] train_loss: 0.012882\n",
      "[203/01171] train_loss: 0.013290\n",
      "[203/01221] train_loss: 0.013716\n",
      "[204/00045] train_loss: 0.014267\n",
      "[204/00095] train_loss: 0.014092\n",
      "[204/00145] train_loss: 0.014154\n",
      "[204/00195] train_loss: 0.013170\n",
      "[204/00245] train_loss: 0.013054\n",
      "[204/00295] train_loss: 0.014011\n",
      "[204/00345] train_loss: 0.012659\n",
      "[204/00395] train_loss: 0.013973\n",
      "[204/00445] train_loss: 0.014546\n",
      "[204/00495] train_loss: 0.013661\n",
      "[204/00545] train_loss: 0.013183\n",
      "[204/00595] train_loss: 0.013355\n",
      "[204/00645] train_loss: 0.014379\n",
      "[204/00695] train_loss: 0.013224\n",
      "[204/00745] train_loss: 0.014181\n",
      "[204/00795] train_loss: 0.012429\n",
      "[204/00845] train_loss: 0.013012\n",
      "[204/00895] train_loss: 0.012655\n",
      "[204/00945] train_loss: 0.014101\n",
      "[204/00995] train_loss: 0.012966\n",
      "[204/01045] train_loss: 0.012928\n",
      "[204/01095] train_loss: 0.012736\n",
      "[204/01145] train_loss: 0.012666\n",
      "[204/01195] train_loss: 0.013802\n",
      "[205/00019] train_loss: 0.014343\n",
      "[205/00069] train_loss: 0.014211\n",
      "[205/00119] train_loss: 0.013785\n",
      "[205/00169] train_loss: 0.013930\n",
      "[205/00219] train_loss: 0.013933\n",
      "[205/00269] train_loss: 0.014015\n",
      "[205/00319] train_loss: 0.014552\n",
      "[205/00369] train_loss: 0.012968\n",
      "[205/00419] train_loss: 0.013427\n",
      "[205/00469] train_loss: 0.013534\n",
      "[205/00519] train_loss: 0.012718\n",
      "[205/00569] train_loss: 0.012783\n",
      "[205/00619] train_loss: 0.012752\n",
      "[205/00669] train_loss: 0.013316\n",
      "[205/00719] train_loss: 0.013990\n",
      "[205/00769] train_loss: 0.013126\n",
      "[205/00819] train_loss: 0.012630\n",
      "[205/00869] train_loss: 0.014118\n",
      "[205/00919] train_loss: 0.013064\n",
      "[205/00969] train_loss: 0.013223\n",
      "[205/01019] train_loss: 0.013558\n",
      "[205/01069] train_loss: 0.013660\n",
      "[205/01119] train_loss: 0.012614\n",
      "[205/01169] train_loss: 0.013466\n",
      "[205/01219] train_loss: 0.012962\n",
      "[206/00043] train_loss: 0.014940\n",
      "[206/00093] train_loss: 0.014210\n",
      "[206/00143] train_loss: 0.014011\n",
      "[206/00193] train_loss: 0.013755\n",
      "[206/00243] train_loss: 0.013652\n",
      "[206/00293] train_loss: 0.013617\n",
      "[206/00343] train_loss: 0.013943\n",
      "[206/00393] train_loss: 0.013606\n",
      "[206/00443] train_loss: 0.013267\n",
      "[206/00493] train_loss: 0.013400\n",
      "[206/00543] train_loss: 0.012787\n",
      "[206/00593] train_loss: 0.013271\n",
      "[206/00643] train_loss: 0.012892\n",
      "[206/00693] train_loss: 0.013779\n",
      "[206/00743] train_loss: 0.013674\n",
      "[206/00793] train_loss: 0.013491\n",
      "[206/00843] train_loss: 0.013042\n",
      "[206/00893] train_loss: 0.012900\n",
      "[206/00943] train_loss: 0.013624\n",
      "[206/00993] train_loss: 0.013658\n",
      "[206/01043] train_loss: 0.013450\n",
      "[206/01093] train_loss: 0.013554\n",
      "[206/01143] train_loss: 0.013223\n",
      "[206/01193] train_loss: 0.013025\n",
      "[207/00017] train_loss: 0.013924\n",
      "[207/00067] train_loss: 0.014140\n",
      "[207/00117] train_loss: 0.013559\n",
      "[207/00167] train_loss: 0.014299\n",
      "[207/00217] train_loss: 0.013641\n",
      "[207/00267] train_loss: 0.012919\n",
      "[207/00317] train_loss: 0.013893\n",
      "[207/00367] train_loss: 0.013892\n",
      "[207/00417] train_loss: 0.013250\n",
      "[207/00467] train_loss: 0.013448\n",
      "[207/00517] train_loss: 0.013700\n",
      "[207/00567] train_loss: 0.013391\n",
      "[207/00617] train_loss: 0.014178\n",
      "[207/00667] train_loss: 0.013622\n",
      "[207/00717] train_loss: 0.013254\n",
      "[207/00767] train_loss: 0.013212\n",
      "[207/00817] train_loss: 0.013919\n",
      "[207/00867] train_loss: 0.013710\n",
      "[207/00917] train_loss: 0.013580\n",
      "[207/00967] train_loss: 0.013291\n",
      "[207/01017] train_loss: 0.012935\n",
      "[207/01067] train_loss: 0.012842\n",
      "[207/01117] train_loss: 0.014029\n",
      "[207/01167] train_loss: 0.013240\n",
      "[207/01217] train_loss: 0.012555\n",
      "[208/00041] train_loss: 0.014154\n",
      "[208/00091] train_loss: 0.014123\n",
      "[208/00141] train_loss: 0.013563\n",
      "[208/00191] train_loss: 0.014607\n",
      "[208/00241] train_loss: 0.013578\n",
      "[208/00291] train_loss: 0.013341\n",
      "[208/00341] train_loss: 0.013833\n",
      "[208/00391] train_loss: 0.014721\n",
      "[208/00441] train_loss: 0.014234\n",
      "[208/00491] train_loss: 0.013875\n",
      "[208/00541] train_loss: 0.012973\n",
      "[208/00591] train_loss: 0.013374\n",
      "[208/00641] train_loss: 0.013267\n",
      "[208/00691] train_loss: 0.013431\n",
      "[208/00741] train_loss: 0.013014\n",
      "[208/00791] train_loss: 0.012768\n",
      "[208/00841] train_loss: 0.013043\n",
      "[208/00891] train_loss: 0.014218\n",
      "[208/00941] train_loss: 0.012991\n",
      "[208/00991] train_loss: 0.013357\n",
      "[208/01041] train_loss: 0.013067\n",
      "[208/01091] train_loss: 0.012666\n",
      "[208/01141] train_loss: 0.013575\n",
      "[208/01191] train_loss: 0.013488\n",
      "[209/00015] train_loss: 0.013648\n",
      "[209/00065] train_loss: 0.013788\n",
      "[209/00115] train_loss: 0.013994\n",
      "[209/00165] train_loss: 0.013785\n",
      "[209/00215] train_loss: 0.014081\n",
      "[209/00265] train_loss: 0.013656\n",
      "[209/00315] train_loss: 0.013958\n",
      "[209/00365] train_loss: 0.013126\n",
      "[209/00415] train_loss: 0.013818\n",
      "[209/00465] train_loss: 0.014089\n",
      "[209/00515] train_loss: 0.012739\n",
      "[209/00565] train_loss: 0.014051\n",
      "[209/00615] train_loss: 0.012983\n",
      "[209/00665] train_loss: 0.013686\n",
      "[209/00715] train_loss: 0.013457\n",
      "[209/00765] train_loss: 0.013390\n",
      "[209/00815] train_loss: 0.012967\n",
      "[209/00865] train_loss: 0.013452\n",
      "[209/00915] train_loss: 0.013671\n",
      "[209/00965] train_loss: 0.012830\n",
      "[209/01015] train_loss: 0.012955\n",
      "[209/01065] train_loss: 0.012969\n",
      "[209/01115] train_loss: 0.012834\n",
      "[209/01165] train_loss: 0.014317\n",
      "[209/01215] train_loss: 0.012509\n",
      "[210/00039] train_loss: 0.014271\n",
      "[210/00089] train_loss: 0.014030\n",
      "[210/00139] train_loss: 0.013947\n",
      "[210/00189] train_loss: 0.014181\n",
      "[210/00239] train_loss: 0.014168\n",
      "[210/00289] train_loss: 0.013605\n",
      "[210/00339] train_loss: 0.013835\n",
      "[210/00389] train_loss: 0.013929\n",
      "[210/00439] train_loss: 0.012953\n",
      "[210/00489] train_loss: 0.012609\n",
      "[210/00539] train_loss: 0.012951\n",
      "[210/00589] train_loss: 0.013236\n",
      "[210/00639] train_loss: 0.013116\n",
      "[210/00689] train_loss: 0.013477\n",
      "[210/00739] train_loss: 0.013823\n",
      "[210/00789] train_loss: 0.013115\n",
      "[210/00839] train_loss: 0.012993\n",
      "[210/00889] train_loss: 0.013398\n",
      "[210/00939] train_loss: 0.012957\n",
      "[210/00989] train_loss: 0.012553\n",
      "[210/01039] train_loss: 0.013543\n",
      "[210/01089] train_loss: 0.013847\n",
      "[210/01139] train_loss: 0.013452\n",
      "[210/01189] train_loss: 0.013318\n",
      "[211/00013] train_loss: 0.013808\n",
      "[211/00063] train_loss: 0.014974\n",
      "[211/00113] train_loss: 0.013720\n",
      "[211/00163] train_loss: 0.013592\n",
      "[211/00213] train_loss: 0.014067\n",
      "[211/00263] train_loss: 0.013893\n",
      "[211/00313] train_loss: 0.013710\n",
      "[211/00363] train_loss: 0.013120\n",
      "[211/00413] train_loss: 0.013154\n",
      "[211/00463] train_loss: 0.013557\n",
      "[211/00513] train_loss: 0.012964\n",
      "[211/00563] train_loss: 0.012614\n",
      "[211/00613] train_loss: 0.012973\n",
      "[211/00663] train_loss: 0.013980\n",
      "[211/00713] train_loss: 0.013576\n",
      "[211/00763] train_loss: 0.013566\n",
      "[211/00813] train_loss: 0.013324\n",
      "[211/00863] train_loss: 0.012971\n",
      "[211/00913] train_loss: 0.012963\n",
      "[211/00963] train_loss: 0.014380\n",
      "[211/01013] train_loss: 0.013450\n",
      "[211/01063] train_loss: 0.012909\n",
      "[211/01113] train_loss: 0.013230\n",
      "[211/01163] train_loss: 0.013095\n",
      "[211/01213] train_loss: 0.013094\n",
      "[212/00037] train_loss: 0.013868\n",
      "[212/00087] train_loss: 0.014033\n",
      "[212/00137] train_loss: 0.013215\n",
      "[212/00187] train_loss: 0.013661\n",
      "[212/00237] train_loss: 0.013323\n",
      "[212/00287] train_loss: 0.013622\n",
      "[212/00337] train_loss: 0.013932\n",
      "[212/00387] train_loss: 0.014081\n",
      "[212/00437] train_loss: 0.013276\n",
      "[212/00487] train_loss: 0.013624\n",
      "[212/00537] train_loss: 0.012842\n",
      "[212/00587] train_loss: 0.013350\n",
      "[212/00637] train_loss: 0.013712\n",
      "[212/00687] train_loss: 0.013404\n",
      "[212/00737] train_loss: 0.013084\n",
      "[212/00787] train_loss: 0.013600\n",
      "[212/00837] train_loss: 0.013234\n",
      "[212/00887] train_loss: 0.013844\n",
      "[212/00937] train_loss: 0.013018\n",
      "[212/00987] train_loss: 0.012944\n",
      "[212/01037] train_loss: 0.012704\n",
      "[212/01087] train_loss: 0.012812\n",
      "[212/01137] train_loss: 0.012584\n",
      "[212/01187] train_loss: 0.013220\n",
      "[213/00011] train_loss: 0.013045\n",
      "[213/00061] train_loss: 0.014731\n",
      "[213/00111] train_loss: 0.014496\n",
      "[213/00161] train_loss: 0.014210\n",
      "[213/00211] train_loss: 0.014116\n",
      "[213/00261] train_loss: 0.013150\n",
      "[213/00311] train_loss: 0.014279\n",
      "[213/00361] train_loss: 0.013362\n",
      "[213/00411] train_loss: 0.013803\n",
      "[213/00461] train_loss: 0.013394\n",
      "[213/00511] train_loss: 0.013124\n",
      "[213/00561] train_loss: 0.013328\n",
      "[213/00611] train_loss: 0.013677\n",
      "[213/00661] train_loss: 0.012887\n",
      "[213/00711] train_loss: 0.013308\n",
      "[213/00761] train_loss: 0.013338\n",
      "[213/00811] train_loss: 0.013239\n",
      "[213/00861] train_loss: 0.012925\n",
      "[213/00911] train_loss: 0.013429\n",
      "[213/00961] train_loss: 0.012812\n",
      "[213/01011] train_loss: 0.013614\n",
      "[213/01061] train_loss: 0.012983\n",
      "[213/01111] train_loss: 0.014161\n",
      "[213/01161] train_loss: 0.013016\n",
      "[213/01211] train_loss: 0.012918\n",
      "[214/00035] train_loss: 0.013120\n",
      "[214/00085] train_loss: 0.014004\n",
      "[214/00135] train_loss: 0.013694\n",
      "[214/00185] train_loss: 0.014477\n",
      "[214/00235] train_loss: 0.015006\n",
      "[214/00285] train_loss: 0.013405\n",
      "[214/00335] train_loss: 0.012904\n",
      "[214/00385] train_loss: 0.013143\n",
      "[214/00435] train_loss: 0.013874\n",
      "[214/00485] train_loss: 0.013313\n",
      "[214/00535] train_loss: 0.013089\n",
      "[214/00585] train_loss: 0.013331\n",
      "[214/00635] train_loss: 0.012460\n",
      "[214/00685] train_loss: 0.014016\n",
      "[214/00735] train_loss: 0.014023\n",
      "[214/00785] train_loss: 0.013333\n",
      "[214/00835] train_loss: 0.013302\n",
      "[214/00885] train_loss: 0.013466\n",
      "[214/00935] train_loss: 0.013449\n",
      "[214/00985] train_loss: 0.013031\n",
      "[214/01035] train_loss: 0.012669\n",
      "[214/01085] train_loss: 0.012952\n",
      "[214/01135] train_loss: 0.012817\n",
      "[214/01185] train_loss: 0.013366\n",
      "[215/00009] train_loss: 0.013199\n",
      "[215/00059] train_loss: 0.014781\n",
      "[215/00109] train_loss: 0.014593\n",
      "[215/00159] train_loss: 0.013671\n",
      "[215/00209] train_loss: 0.014267\n",
      "[215/00259] train_loss: 0.013888\n",
      "[215/00309] train_loss: 0.013433\n",
      "[215/00359] train_loss: 0.013152\n",
      "[215/00409] train_loss: 0.013056\n",
      "[215/00459] train_loss: 0.013215\n",
      "[215/00509] train_loss: 0.013485\n",
      "[215/00559] train_loss: 0.012793\n",
      "[215/00609] train_loss: 0.013205\n",
      "[215/00659] train_loss: 0.013615\n",
      "[215/00709] train_loss: 0.012657\n",
      "[215/00759] train_loss: 0.013529\n",
      "[215/00809] train_loss: 0.013004\n",
      "[215/00859] train_loss: 0.013075\n",
      "[215/00909] train_loss: 0.013273\n",
      "[215/00959] train_loss: 0.013618\n",
      "[215/01009] train_loss: 0.013697\n",
      "[215/01059] train_loss: 0.012984\n",
      "[215/01109] train_loss: 0.013547\n",
      "[215/01159] train_loss: 0.013333\n",
      "[215/01209] train_loss: 0.012309\n",
      "[216/00033] train_loss: 0.013020\n",
      "[216/00083] train_loss: 0.014642\n",
      "[216/00133] train_loss: 0.014088\n",
      "[216/00183] train_loss: 0.015081\n",
      "[216/00233] train_loss: 0.013822\n",
      "[216/00283] train_loss: 0.012976\n",
      "[216/00333] train_loss: 0.013300\n",
      "[216/00383] train_loss: 0.013152\n",
      "[216/00433] train_loss: 0.012922\n",
      "[216/00483] train_loss: 0.013314\n",
      "[216/00533] train_loss: 0.013034\n",
      "[216/00583] train_loss: 0.014313\n",
      "[216/00633] train_loss: 0.012657\n",
      "[216/00683] train_loss: 0.014041\n",
      "[216/00733] train_loss: 0.013213\n",
      "[216/00783] train_loss: 0.013051\n",
      "[216/00833] train_loss: 0.012732\n",
      "[216/00883] train_loss: 0.012503\n",
      "[216/00933] train_loss: 0.013131\n",
      "[216/00983] train_loss: 0.013229\n",
      "[216/01033] train_loss: 0.012336\n",
      "[216/01083] train_loss: 0.013189\n",
      "[216/01133] train_loss: 0.013065\n",
      "[216/01183] train_loss: 0.013315\n",
      "[217/00007] train_loss: 0.014218\n",
      "[217/00057] train_loss: 0.014100\n",
      "[217/00107] train_loss: 0.014834\n",
      "[217/00157] train_loss: 0.013961\n",
      "[217/00207] train_loss: 0.014336\n",
      "[217/00257] train_loss: 0.013335\n",
      "[217/00307] train_loss: 0.013404\n",
      "[217/00357] train_loss: 0.013652\n",
      "[217/00407] train_loss: 0.012421\n",
      "[217/00457] train_loss: 0.012720\n",
      "[217/00507] train_loss: 0.014018\n",
      "[217/00557] train_loss: 0.013591\n",
      "[217/00607] train_loss: 0.013039\n",
      "[217/00657] train_loss: 0.012576\n",
      "[217/00707] train_loss: 0.014531\n",
      "[217/00757] train_loss: 0.013129\n",
      "[217/00807] train_loss: 0.013583\n",
      "[217/00857] train_loss: 0.013390\n",
      "[217/00907] train_loss: 0.012223\n",
      "[217/00957] train_loss: 0.012751\n",
      "[217/01007] train_loss: 0.012504\n",
      "[217/01057] train_loss: 0.013336\n",
      "[217/01107] train_loss: 0.013418\n",
      "[217/01157] train_loss: 0.013684\n",
      "[217/01207] train_loss: 0.012345\n",
      "[218/00031] train_loss: 0.013793\n",
      "[218/00081] train_loss: 0.014248\n",
      "[218/00131] train_loss: 0.014092\n",
      "[218/00181] train_loss: 0.013895\n",
      "[218/00231] train_loss: 0.012553\n",
      "[218/00281] train_loss: 0.013346\n",
      "[218/00331] train_loss: 0.013415\n",
      "[218/00381] train_loss: 0.013635\n",
      "[218/00431] train_loss: 0.013822\n",
      "[218/00481] train_loss: 0.013927\n",
      "[218/00531] train_loss: 0.013185\n",
      "[218/00581] train_loss: 0.013695\n",
      "[218/00631] train_loss: 0.013360\n",
      "[218/00681] train_loss: 0.013653\n",
      "[218/00731] train_loss: 0.012673\n",
      "[218/00781] train_loss: 0.012371\n",
      "[218/00831] train_loss: 0.013842\n",
      "[218/00881] train_loss: 0.013311\n",
      "[218/00931] train_loss: 0.013415\n",
      "[218/00981] train_loss: 0.012804\n",
      "[218/01031] train_loss: 0.013592\n",
      "[218/01081] train_loss: 0.014126\n",
      "[218/01131] train_loss: 0.013330\n",
      "[218/01181] train_loss: 0.013185\n",
      "[219/00005] train_loss: 0.012092\n",
      "[219/00055] train_loss: 0.014662\n",
      "[219/00105] train_loss: 0.014686\n",
      "[219/00155] train_loss: 0.014065\n",
      "[219/00205] train_loss: 0.014890\n",
      "[219/00255] train_loss: 0.013382\n",
      "[219/00305] train_loss: 0.013472\n",
      "[219/00355] train_loss: 0.013591\n",
      "[219/00405] train_loss: 0.013330\n",
      "[219/00455] train_loss: 0.013663\n",
      "[219/00505] train_loss: 0.013095\n",
      "[219/00555] train_loss: 0.013419\n",
      "[219/00605] train_loss: 0.013276\n",
      "[219/00655] train_loss: 0.013179\n",
      "[219/00705] train_loss: 0.012367\n",
      "[219/00755] train_loss: 0.012858\n",
      "[219/00805] train_loss: 0.013876\n",
      "[219/00855] train_loss: 0.013145\n",
      "[219/00905] train_loss: 0.013168\n",
      "[219/00955] train_loss: 0.012881\n",
      "[219/01005] train_loss: 0.012784\n",
      "[219/01055] train_loss: 0.013242\n",
      "[219/01105] train_loss: 0.012395\n",
      "[219/01155] train_loss: 0.013546\n",
      "[219/01205] train_loss: 0.012819\n",
      "[220/00029] train_loss: 0.013731\n",
      "[220/00079] train_loss: 0.014792\n",
      "[220/00129] train_loss: 0.013660\n",
      "[220/00179] train_loss: 0.013164\n",
      "[220/00229] train_loss: 0.013462\n",
      "[220/00279] train_loss: 0.014549\n",
      "[220/00329] train_loss: 0.013859\n",
      "[220/00379] train_loss: 0.013376\n",
      "[220/00429] train_loss: 0.013321\n",
      "[220/00479] train_loss: 0.013975\n",
      "[220/00529] train_loss: 0.013256\n",
      "[220/00579] train_loss: 0.013351\n",
      "[220/00629] train_loss: 0.013846\n",
      "[220/00679] train_loss: 0.013205\n",
      "[220/00729] train_loss: 0.013537\n",
      "[220/00779] train_loss: 0.013484\n",
      "[220/00829] train_loss: 0.013419\n",
      "[220/00879] train_loss: 0.013239\n",
      "[220/00929] train_loss: 0.012736\n",
      "[220/00979] train_loss: 0.012575\n",
      "[220/01029] train_loss: 0.013224\n",
      "[220/01079] train_loss: 0.012614\n",
      "[220/01129] train_loss: 0.012838\n",
      "[220/01179] train_loss: 0.013253\n",
      "[221/00003] train_loss: 0.013592\n",
      "[221/00053] train_loss: 0.015270\n",
      "[221/00103] train_loss: 0.013687\n",
      "[221/00153] train_loss: 0.014576\n",
      "[221/00203] train_loss: 0.013903\n",
      "[221/00253] train_loss: 0.014255\n",
      "[221/00303] train_loss: 0.013025\n",
      "[221/00353] train_loss: 0.013249\n",
      "[221/00403] train_loss: 0.013111\n",
      "[221/00453] train_loss: 0.012359\n",
      "[221/00503] train_loss: 0.013631\n",
      "[221/00553] train_loss: 0.012735\n",
      "[221/00603] train_loss: 0.013400\n",
      "[221/00653] train_loss: 0.013359\n",
      "[221/00703] train_loss: 0.013702\n",
      "[221/00753] train_loss: 0.013709\n",
      "[221/00803] train_loss: 0.012771\n",
      "[221/00853] train_loss: 0.013076\n",
      "[221/00903] train_loss: 0.013201\n",
      "[221/00953] train_loss: 0.013245\n",
      "[221/01003] train_loss: 0.013046\n",
      "[221/01053] train_loss: 0.012734\n",
      "[221/01103] train_loss: 0.013558\n",
      "[221/01153] train_loss: 0.013412\n",
      "[221/01203] train_loss: 0.012898\n",
      "[222/00027] train_loss: 0.014235\n",
      "[222/00077] train_loss: 0.014838\n",
      "[222/00127] train_loss: 0.013803\n",
      "[222/00177] train_loss: 0.014492\n",
      "[222/00227] train_loss: 0.012902\n",
      "[222/00277] train_loss: 0.013223\n",
      "[222/00327] train_loss: 0.013717\n",
      "[222/00377] train_loss: 0.013321\n",
      "[222/00427] train_loss: 0.014500\n",
      "[222/00477] train_loss: 0.013241\n",
      "[222/00527] train_loss: 0.014172\n",
      "[222/00577] train_loss: 0.012959\n",
      "[222/00627] train_loss: 0.013193\n",
      "[222/00677] train_loss: 0.013089\n",
      "[222/00727] train_loss: 0.013083\n",
      "[222/00777] train_loss: 0.012748\n",
      "[222/00827] train_loss: 0.013345\n",
      "[222/00877] train_loss: 0.013472\n",
      "[222/00927] train_loss: 0.012725\n",
      "[222/00977] train_loss: 0.012837\n",
      "[222/01027] train_loss: 0.013408\n",
      "[222/01077] train_loss: 0.012835\n",
      "[222/01127] train_loss: 0.013938\n",
      "[222/01177] train_loss: 0.012508\n",
      "[223/00001] train_loss: 0.013179\n",
      "[223/00051] train_loss: 0.014009\n",
      "[223/00101] train_loss: 0.013928\n",
      "[223/00151] train_loss: 0.013702\n",
      "[223/00201] train_loss: 0.013055\n",
      "[223/00251] train_loss: 0.013450\n",
      "[223/00301] train_loss: 0.013341\n",
      "[223/00351] train_loss: 0.013286\n",
      "[223/00401] train_loss: 0.013074\n",
      "[223/00451] train_loss: 0.012823\n",
      "[223/00501] train_loss: 0.014175\n",
      "[223/00551] train_loss: 0.013130\n",
      "[223/00601] train_loss: 0.013034\n",
      "[223/00651] train_loss: 0.012541\n",
      "[223/00701] train_loss: 0.013192\n",
      "[223/00751] train_loss: 0.013136\n",
      "[223/00801] train_loss: 0.013921\n",
      "[223/00851] train_loss: 0.013672\n",
      "[223/00901] train_loss: 0.012081\n",
      "[223/00951] train_loss: 0.013236\n",
      "[223/01001] train_loss: 0.013057\n",
      "[223/01051] train_loss: 0.013274\n",
      "[223/01101] train_loss: 0.013578\n",
      "[223/01151] train_loss: 0.012905\n",
      "[223/01201] train_loss: 0.012804\n",
      "[224/00025] train_loss: 0.013986\n",
      "[224/00075] train_loss: 0.014353\n",
      "[224/00125] train_loss: 0.013782\n",
      "[224/00175] train_loss: 0.013826\n",
      "[224/00225] train_loss: 0.013588\n",
      "[224/00275] train_loss: 0.014121\n",
      "[224/00325] train_loss: 0.012870\n",
      "[224/00375] train_loss: 0.013224\n",
      "[224/00425] train_loss: 0.013381\n",
      "[224/00475] train_loss: 0.013114\n",
      "[224/00525] train_loss: 0.012849\n",
      "[224/00575] train_loss: 0.012925\n",
      "[224/00625] train_loss: 0.013745\n",
      "[224/00675] train_loss: 0.012561\n",
      "[224/00725] train_loss: 0.012945\n",
      "[224/00775] train_loss: 0.013374\n",
      "[224/00825] train_loss: 0.013088\n",
      "[224/00875] train_loss: 0.013438\n",
      "[224/00925] train_loss: 0.012918\n",
      "[224/00975] train_loss: 0.013096\n",
      "[224/01025] train_loss: 0.012948\n",
      "[224/01075] train_loss: 0.013546\n",
      "[224/01125] train_loss: 0.013301\n",
      "[224/01175] train_loss: 0.012693\n",
      "[224/01225] train_loss: 0.012857\n",
      "[225/00049] train_loss: 0.014450\n",
      "[225/00099] train_loss: 0.014218\n",
      "[225/00149] train_loss: 0.014044\n",
      "[225/00199] train_loss: 0.013888\n",
      "[225/00249] train_loss: 0.013287\n",
      "[225/00299] train_loss: 0.013922\n",
      "[225/00349] train_loss: 0.014162\n",
      "[225/00399] train_loss: 0.013400\n",
      "[225/00449] train_loss: 0.012969\n",
      "[225/00499] train_loss: 0.012953\n",
      "[225/00549] train_loss: 0.012921\n",
      "[225/00599] train_loss: 0.012422\n",
      "[225/00649] train_loss: 0.013776\n",
      "[225/00699] train_loss: 0.012918\n",
      "[225/00749] train_loss: 0.012566\n",
      "[225/00799] train_loss: 0.013153\n",
      "[225/00849] train_loss: 0.012738\n",
      "[225/00899] train_loss: 0.012971\n",
      "[225/00949] train_loss: 0.012907\n",
      "[225/00999] train_loss: 0.013812\n",
      "[225/01049] train_loss: 0.013283\n",
      "[225/01099] train_loss: 0.013196\n",
      "[225/01149] train_loss: 0.013180\n",
      "[225/01199] train_loss: 0.012408\n",
      "[226/00023] train_loss: 0.013185\n",
      "[226/00073] train_loss: 0.014059\n",
      "[226/00123] train_loss: 0.014208\n",
      "[226/00173] train_loss: 0.013058\n",
      "[226/00223] train_loss: 0.013323\n",
      "[226/00273] train_loss: 0.013234\n",
      "[226/00323] train_loss: 0.013322\n",
      "[226/00373] train_loss: 0.013891\n",
      "[226/00423] train_loss: 0.013211\n",
      "[226/00473] train_loss: 0.013106\n",
      "[226/00523] train_loss: 0.013108\n",
      "[226/00573] train_loss: 0.013530\n",
      "[226/00623] train_loss: 0.012786\n",
      "[226/00673] train_loss: 0.013403\n",
      "[226/00723] train_loss: 0.012879\n",
      "[226/00773] train_loss: 0.012645\n",
      "[226/00823] train_loss: 0.014286\n",
      "[226/00873] train_loss: 0.014153\n",
      "[226/00923] train_loss: 0.013286\n",
      "[226/00973] train_loss: 0.012844\n",
      "[226/01023] train_loss: 0.012493\n",
      "[226/01073] train_loss: 0.013566\n",
      "[226/01123] train_loss: 0.013476\n",
      "[226/01173] train_loss: 0.013779\n",
      "[226/01223] train_loss: 0.012700\n",
      "[227/00047] train_loss: 0.014203\n",
      "[227/00097] train_loss: 0.013461\n",
      "[227/00147] train_loss: 0.013682\n",
      "[227/00197] train_loss: 0.013804\n",
      "[227/00247] train_loss: 0.013189\n",
      "[227/00297] train_loss: 0.013879\n",
      "[227/00347] train_loss: 0.013570\n",
      "[227/00397] train_loss: 0.013494\n",
      "[227/00447] train_loss: 0.012609\n",
      "[227/00497] train_loss: 0.013094\n",
      "[227/00547] train_loss: 0.013528\n",
      "[227/00597] train_loss: 0.013072\n",
      "[227/00647] train_loss: 0.013453\n",
      "[227/00697] train_loss: 0.013337\n",
      "[227/00747] train_loss: 0.013047\n",
      "[227/00797] train_loss: 0.013614\n",
      "[227/00847] train_loss: 0.013052\n",
      "[227/00897] train_loss: 0.013116\n",
      "[227/00947] train_loss: 0.013896\n",
      "[227/00997] train_loss: 0.013289\n",
      "[227/01047] train_loss: 0.013033\n",
      "[227/01097] train_loss: 0.013008\n",
      "[227/01147] train_loss: 0.013409\n",
      "[227/01197] train_loss: 0.013240\n",
      "[228/00021] train_loss: 0.012992\n",
      "[228/00071] train_loss: 0.014231\n",
      "[228/00121] train_loss: 0.013460\n",
      "[228/00171] train_loss: 0.013742\n",
      "[228/00221] train_loss: 0.014522\n",
      "[228/00271] train_loss: 0.013256\n",
      "[228/00321] train_loss: 0.013035\n",
      "[228/00371] train_loss: 0.013614\n",
      "[228/00421] train_loss: 0.012904\n",
      "[228/00471] train_loss: 0.014176\n",
      "[228/00521] train_loss: 0.012682\n",
      "[228/00571] train_loss: 0.013217\n",
      "[228/00621] train_loss: 0.013355\n",
      "[228/00671] train_loss: 0.013613\n",
      "[228/00721] train_loss: 0.012607\n",
      "[228/00771] train_loss: 0.012747\n",
      "[228/00821] train_loss: 0.012999\n",
      "[228/00871] train_loss: 0.014237\n",
      "[228/00921] train_loss: 0.013120\n",
      "[228/00971] train_loss: 0.013401\n",
      "[228/01021] train_loss: 0.013458\n",
      "[228/01071] train_loss: 0.013241\n",
      "[228/01121] train_loss: 0.012894\n",
      "[228/01171] train_loss: 0.013435\n",
      "[228/01221] train_loss: 0.013508\n",
      "[229/00045] train_loss: 0.013687\n",
      "[229/00095] train_loss: 0.013825\n",
      "[229/00145] train_loss: 0.013915\n",
      "[229/00195] train_loss: 0.013366\n",
      "[229/00245] train_loss: 0.014021\n",
      "[229/00295] train_loss: 0.014562\n",
      "[229/00345] train_loss: 0.013858\n",
      "[229/00395] train_loss: 0.012830\n",
      "[229/00445] train_loss: 0.013648\n",
      "[229/00495] train_loss: 0.013707\n",
      "[229/00545] train_loss: 0.012658\n",
      "[229/00595] train_loss: 0.013367\n",
      "[229/00645] train_loss: 0.013178\n",
      "[229/00695] train_loss: 0.013549\n",
      "[229/00745] train_loss: 0.013163\n",
      "[229/00795] train_loss: 0.014055\n",
      "[229/00845] train_loss: 0.012519\n",
      "[229/00895] train_loss: 0.012896\n",
      "[229/00945] train_loss: 0.012335\n",
      "[229/00995] train_loss: 0.013531\n",
      "[229/01045] train_loss: 0.012891\n",
      "[229/01095] train_loss: 0.013233\n",
      "[229/01145] train_loss: 0.012370\n",
      "[229/01195] train_loss: 0.013214\n",
      "[230/00019] train_loss: 0.013165\n",
      "[230/00069] train_loss: 0.013594\n",
      "[230/00119] train_loss: 0.013563\n",
      "[230/00169] train_loss: 0.013958\n",
      "[230/00219] train_loss: 0.014012\n",
      "[230/00269] train_loss: 0.014085\n",
      "[230/00319] train_loss: 0.013609\n",
      "[230/00369] train_loss: 0.012881\n",
      "[230/00419] train_loss: 0.013095\n",
      "[230/00469] train_loss: 0.013456\n",
      "[230/00519] train_loss: 0.013333\n",
      "[230/00569] train_loss: 0.012909\n",
      "[230/00619] train_loss: 0.013472\n",
      "[230/00669] train_loss: 0.012357\n",
      "[230/00719] train_loss: 0.013537\n",
      "[230/00769] train_loss: 0.012994\n",
      "[230/00819] train_loss: 0.012520\n",
      "[230/00869] train_loss: 0.013396\n",
      "[230/00919] train_loss: 0.013376\n",
      "[230/00969] train_loss: 0.012732\n",
      "[230/01019] train_loss: 0.014029\n",
      "[230/01069] train_loss: 0.012230\n",
      "[230/01119] train_loss: 0.013076\n",
      "[230/01169] train_loss: 0.012802\n",
      "[230/01219] train_loss: 0.012514\n",
      "[231/00043] train_loss: 0.014285\n",
      "[231/00093] train_loss: 0.014510\n",
      "[231/00143] train_loss: 0.013601\n",
      "[231/00193] train_loss: 0.013162\n",
      "[231/00243] train_loss: 0.013477\n",
      "[231/00293] train_loss: 0.013349\n",
      "[231/00343] train_loss: 0.013255\n",
      "[231/00393] train_loss: 0.013440\n",
      "[231/00443] train_loss: 0.013590\n",
      "[231/00493] train_loss: 0.013118\n",
      "[231/00543] train_loss: 0.013463\n",
      "[231/00593] train_loss: 0.013298\n",
      "[231/00643] train_loss: 0.012915\n",
      "[231/00693] train_loss: 0.013078\n",
      "[231/00743] train_loss: 0.013961\n",
      "[231/00793] train_loss: 0.013262\n",
      "[231/00843] train_loss: 0.012265\n",
      "[231/00893] train_loss: 0.013323\n",
      "[231/00943] train_loss: 0.013186\n",
      "[231/00993] train_loss: 0.013182\n",
      "[231/01043] train_loss: 0.012967\n",
      "[231/01093] train_loss: 0.013107\n",
      "[231/01143] train_loss: 0.013390\n",
      "[231/01193] train_loss: 0.013479\n",
      "[232/00017] train_loss: 0.012699\n",
      "[232/00067] train_loss: 0.013982\n",
      "[232/00117] train_loss: 0.014035\n",
      "[232/00167] train_loss: 0.013604\n",
      "[232/00217] train_loss: 0.013594\n",
      "[232/00267] train_loss: 0.013463\n",
      "[232/00317] train_loss: 0.012347\n",
      "[232/00367] train_loss: 0.013299\n",
      "[232/00417] train_loss: 0.013871\n",
      "[232/00467] train_loss: 0.013400\n",
      "[232/00517] train_loss: 0.012654\n",
      "[232/00567] train_loss: 0.013392\n",
      "[232/00617] train_loss: 0.013623\n",
      "[232/00667] train_loss: 0.013928\n",
      "[232/00717] train_loss: 0.013667\n",
      "[232/00767] train_loss: 0.013003\n",
      "[232/00817] train_loss: 0.013097\n",
      "[232/00867] train_loss: 0.012656\n",
      "[232/00917] train_loss: 0.013348\n",
      "[232/00967] train_loss: 0.013659\n",
      "[232/01017] train_loss: 0.012981\n",
      "[232/01067] train_loss: 0.013141\n",
      "[232/01117] train_loss: 0.013604\n",
      "[232/01167] train_loss: 0.013020\n",
      "[232/01217] train_loss: 0.013029\n",
      "[233/00041] train_loss: 0.013439\n",
      "[233/00091] train_loss: 0.013400\n",
      "[233/00141] train_loss: 0.013473\n",
      "[233/00191] train_loss: 0.013502\n",
      "[233/00241] train_loss: 0.013533\n",
      "[233/00291] train_loss: 0.012771\n",
      "[233/00341] train_loss: 0.013601\n",
      "[233/00391] train_loss: 0.014320\n",
      "[233/00441] train_loss: 0.013076\n",
      "[233/00491] train_loss: 0.012824\n",
      "[233/00541] train_loss: 0.013389\n",
      "[233/00591] train_loss: 0.013125\n",
      "[233/00641] train_loss: 0.013284\n",
      "[233/00691] train_loss: 0.013210\n",
      "[233/00741] train_loss: 0.013491\n",
      "[233/00791] train_loss: 0.013326\n",
      "[233/00841] train_loss: 0.013432\n",
      "[233/00891] train_loss: 0.014030\n",
      "[233/00941] train_loss: 0.012917\n",
      "[233/00991] train_loss: 0.012314\n",
      "[233/01041] train_loss: 0.012855\n",
      "[233/01091] train_loss: 0.013922\n",
      "[233/01141] train_loss: 0.013307\n",
      "[233/01191] train_loss: 0.013258\n",
      "[234/00015] train_loss: 0.013427\n",
      "[234/00065] train_loss: 0.013868\n",
      "[234/00115] train_loss: 0.014076\n",
      "[234/00165] train_loss: 0.012574\n",
      "[234/00215] train_loss: 0.014200\n",
      "[234/00265] train_loss: 0.014119\n",
      "[234/00315] train_loss: 0.013136\n",
      "[234/00365] train_loss: 0.013722\n",
      "[234/00415] train_loss: 0.012955\n",
      "[234/00465] train_loss: 0.013193\n",
      "[234/00515] train_loss: 0.013259\n",
      "[234/00565] train_loss: 0.013211\n",
      "[234/00615] train_loss: 0.013075\n",
      "[234/00665] train_loss: 0.013500\n",
      "[234/00715] train_loss: 0.013220\n",
      "[234/00765] train_loss: 0.012596\n",
      "[234/00815] train_loss: 0.012816\n",
      "[234/00865] train_loss: 0.013226\n",
      "[234/00915] train_loss: 0.012518\n",
      "[234/00965] train_loss: 0.014251\n",
      "[234/01015] train_loss: 0.013454\n",
      "[234/01065] train_loss: 0.013666\n",
      "[234/01115] train_loss: 0.012611\n",
      "[234/01165] train_loss: 0.012245\n",
      "[234/01215] train_loss: 0.012659\n",
      "[235/00039] train_loss: 0.013235\n",
      "[235/00089] train_loss: 0.013695\n",
      "[235/00139] train_loss: 0.012662\n",
      "[235/00189] train_loss: 0.013727\n",
      "[235/00239] train_loss: 0.013939\n",
      "[235/00289] train_loss: 0.013299\n",
      "[235/00339] train_loss: 0.013223\n",
      "[235/00389] train_loss: 0.012987\n",
      "[235/00439] train_loss: 0.013178\n",
      "[235/00489] train_loss: 0.013793\n",
      "[235/00539] train_loss: 0.013725\n",
      "[235/00589] train_loss: 0.012597\n",
      "[235/00639] train_loss: 0.013215\n",
      "[235/00689] train_loss: 0.013612\n",
      "[235/00739] train_loss: 0.013530\n",
      "[235/00789] train_loss: 0.013155\n",
      "[235/00839] train_loss: 0.013439\n",
      "[235/00889] train_loss: 0.014003\n",
      "[235/00939] train_loss: 0.013530\n",
      "[235/00989] train_loss: 0.013520\n",
      "[235/01039] train_loss: 0.013308\n",
      "[235/01089] train_loss: 0.013235\n",
      "[235/01139] train_loss: 0.012490\n",
      "[235/01189] train_loss: 0.012188\n",
      "[236/00013] train_loss: 0.013908\n",
      "[236/00063] train_loss: 0.013321\n",
      "[236/00113] train_loss: 0.014144\n",
      "[236/00163] train_loss: 0.013785\n",
      "[236/00213] train_loss: 0.013723\n",
      "[236/00263] train_loss: 0.013763\n",
      "[236/00313] train_loss: 0.013460\n",
      "[236/00363] train_loss: 0.012928\n",
      "[236/00413] train_loss: 0.013307\n",
      "[236/00463] train_loss: 0.012635\n",
      "[236/00513] train_loss: 0.013279\n",
      "[236/00563] train_loss: 0.013472\n",
      "[236/00613] train_loss: 0.013785\n",
      "[236/00663] train_loss: 0.013337\n",
      "[236/00713] train_loss: 0.013061\n",
      "[236/00763] train_loss: 0.013355\n",
      "[236/00813] train_loss: 0.013110\n",
      "[236/00863] train_loss: 0.013021\n",
      "[236/00913] train_loss: 0.013283\n",
      "[236/00963] train_loss: 0.012677\n",
      "[236/01013] train_loss: 0.013002\n",
      "[236/01063] train_loss: 0.012950\n",
      "[236/01113] train_loss: 0.012481\n",
      "[236/01163] train_loss: 0.013276\n",
      "[236/01213] train_loss: 0.013196\n",
      "[237/00037] train_loss: 0.014115\n",
      "[237/00087] train_loss: 0.013172\n",
      "[237/00137] train_loss: 0.013213\n",
      "[237/00187] train_loss: 0.013357\n",
      "[237/00237] train_loss: 0.013397\n",
      "[237/00287] train_loss: 0.012935\n",
      "[237/00337] train_loss: 0.012978\n",
      "[237/00387] train_loss: 0.013408\n",
      "[237/00437] train_loss: 0.013777\n",
      "[237/00487] train_loss: 0.014103\n",
      "[237/00537] train_loss: 0.013193\n",
      "[237/00587] train_loss: 0.013140\n",
      "[237/00637] train_loss: 0.012740\n",
      "[237/00687] train_loss: 0.013377\n",
      "[237/00737] train_loss: 0.012529\n",
      "[237/00787] train_loss: 0.012696\n",
      "[237/00837] train_loss: 0.013468\n",
      "[237/00887] train_loss: 0.013327\n",
      "[237/00937] train_loss: 0.013252\n",
      "[237/00987] train_loss: 0.013332\n",
      "[237/01037] train_loss: 0.013468\n",
      "[237/01087] train_loss: 0.013688\n",
      "[237/01137] train_loss: 0.012822\n",
      "[237/01187] train_loss: 0.013096\n",
      "[238/00011] train_loss: 0.013473\n",
      "[238/00061] train_loss: 0.015057\n",
      "[238/00111] train_loss: 0.014269\n",
      "[238/00161] train_loss: 0.013673\n",
      "[238/00211] train_loss: 0.013099\n",
      "[238/00261] train_loss: 0.013189\n",
      "[238/00311] train_loss: 0.013063\n",
      "[238/00361] train_loss: 0.013152\n",
      "[238/00411] train_loss: 0.013069\n",
      "[238/00461] train_loss: 0.013069\n",
      "[238/00511] train_loss: 0.012896\n",
      "[238/00561] train_loss: 0.013505\n",
      "[238/00611] train_loss: 0.013621\n",
      "[238/00661] train_loss: 0.013505\n",
      "[238/00711] train_loss: 0.014178\n",
      "[238/00761] train_loss: 0.013210\n",
      "[238/00811] train_loss: 0.012610\n",
      "[238/00861] train_loss: 0.013114\n",
      "[238/00911] train_loss: 0.012555\n",
      "[238/00961] train_loss: 0.013121\n",
      "[238/01011] train_loss: 0.012661\n",
      "[238/01061] train_loss: 0.012786\n",
      "[238/01111] train_loss: 0.012287\n",
      "[238/01161] train_loss: 0.013192\n",
      "[238/01211] train_loss: 0.013175\n",
      "[239/00035] train_loss: 0.013819\n",
      "[239/00085] train_loss: 0.013707\n",
      "[239/00135] train_loss: 0.013958\n",
      "[239/00185] train_loss: 0.014153\n",
      "[239/00235] train_loss: 0.013644\n",
      "[239/00285] train_loss: 0.013400\n",
      "[239/00335] train_loss: 0.013410\n",
      "[239/00385] train_loss: 0.013905\n",
      "[239/00435] train_loss: 0.013861\n",
      "[239/00485] train_loss: 0.013194\n",
      "[239/00535] train_loss: 0.013255\n",
      "[239/00585] train_loss: 0.013523\n",
      "[239/00635] train_loss: 0.012552\n",
      "[239/00685] train_loss: 0.013100\n",
      "[239/00735] train_loss: 0.012834\n",
      "[239/00785] train_loss: 0.013078\n",
      "[239/00835] train_loss: 0.012842\n",
      "[239/00885] train_loss: 0.013371\n",
      "[239/00935] train_loss: 0.013122\n",
      "[239/00985] train_loss: 0.012953\n",
      "[239/01035] train_loss: 0.012644\n",
      "[239/01085] train_loss: 0.012733\n",
      "[239/01135] train_loss: 0.012565\n",
      "[239/01185] train_loss: 0.013347\n",
      "[240/00009] train_loss: 0.013297\n",
      "[240/00059] train_loss: 0.013720\n",
      "[240/00109] train_loss: 0.013419\n",
      "[240/00159] train_loss: 0.014529\n",
      "[240/00209] train_loss: 0.013500\n",
      "[240/00259] train_loss: 0.013477\n",
      "[240/00309] train_loss: 0.014050\n",
      "[240/00359] train_loss: 0.012797\n",
      "[240/00409] train_loss: 0.013659\n",
      "[240/00459] train_loss: 0.012885\n",
      "[240/00509] train_loss: 0.012488\n",
      "[240/00559] train_loss: 0.012308\n",
      "[240/00609] train_loss: 0.012748\n",
      "[240/00659] train_loss: 0.012554\n",
      "[240/00709] train_loss: 0.012935\n",
      "[240/00759] train_loss: 0.012824\n",
      "[240/00809] train_loss: 0.013223\n",
      "[240/00859] train_loss: 0.013454\n",
      "[240/00909] train_loss: 0.012898\n",
      "[240/00959] train_loss: 0.013162\n",
      "[240/01009] train_loss: 0.012498\n",
      "[240/01059] train_loss: 0.013106\n",
      "[240/01109] train_loss: 0.013314\n",
      "[240/01159] train_loss: 0.012598\n",
      "[240/01209] train_loss: 0.012859\n",
      "[241/00033] train_loss: 0.013658\n",
      "[241/00083] train_loss: 0.013999\n",
      "[241/00133] train_loss: 0.013748\n",
      "[241/00183] train_loss: 0.013615\n",
      "[241/00233] train_loss: 0.013691\n",
      "[241/00283] train_loss: 0.013977\n",
      "[241/00333] train_loss: 0.012866\n",
      "[241/00383] train_loss: 0.013498\n",
      "[241/00433] train_loss: 0.012481\n",
      "[241/00483] train_loss: 0.012437\n",
      "[241/00533] train_loss: 0.012927\n",
      "[241/00583] train_loss: 0.013295\n",
      "[241/00633] train_loss: 0.013133\n",
      "[241/00683] train_loss: 0.013437\n",
      "[241/00733] train_loss: 0.013794\n",
      "[241/00783] train_loss: 0.013487\n",
      "[241/00833] train_loss: 0.012957\n",
      "[241/00883] train_loss: 0.012248\n",
      "[241/00933] train_loss: 0.012235\n",
      "[241/00983] train_loss: 0.013205\n",
      "[241/01033] train_loss: 0.012805\n",
      "[241/01083] train_loss: 0.013105\n",
      "[241/01133] train_loss: 0.013008\n",
      "[241/01183] train_loss: 0.013230\n",
      "[242/00007] train_loss: 0.013091\n",
      "[242/00057] train_loss: 0.014320\n",
      "[242/00107] train_loss: 0.013862\n",
      "[242/00157] train_loss: 0.014880\n",
      "[242/00207] train_loss: 0.013187\n",
      "[242/00257] train_loss: 0.013382\n",
      "[242/00307] train_loss: 0.013525\n",
      "[242/00357] train_loss: 0.014029\n",
      "[242/00407] train_loss: 0.013368\n",
      "[242/00457] train_loss: 0.012723\n",
      "[242/00507] train_loss: 0.012510\n",
      "[242/00557] train_loss: 0.013614\n",
      "[242/00607] train_loss: 0.012693\n",
      "[242/00657] train_loss: 0.012579\n",
      "[242/00707] train_loss: 0.012129\n",
      "[242/00757] train_loss: 0.012961\n",
      "[242/00807] train_loss: 0.012430\n",
      "[242/00857] train_loss: 0.012477\n",
      "[242/00907] train_loss: 0.014027\n",
      "[242/00957] train_loss: 0.013180\n",
      "[242/01007] train_loss: 0.012619\n",
      "[242/01057] train_loss: 0.013079\n",
      "[242/01107] train_loss: 0.012958\n",
      "[242/01157] train_loss: 0.013222\n",
      "[242/01207] train_loss: 0.012748\n",
      "[243/00031] train_loss: 0.013666\n",
      "[243/00081] train_loss: 0.013816\n",
      "[243/00131] train_loss: 0.014040\n",
      "[243/00181] train_loss: 0.013971\n",
      "[243/00231] train_loss: 0.013839\n",
      "[243/00281] train_loss: 0.014245\n",
      "[243/00331] train_loss: 0.013095\n",
      "[243/00381] train_loss: 0.013149\n",
      "[243/00431] train_loss: 0.012273\n",
      "[243/00481] train_loss: 0.013201\n",
      "[243/00531] train_loss: 0.012987\n",
      "[243/00581] train_loss: 0.013371\n",
      "[243/00631] train_loss: 0.012852\n",
      "[243/00681] train_loss: 0.012774\n",
      "[243/00731] train_loss: 0.012397\n",
      "[243/00781] train_loss: 0.012670\n",
      "[243/00831] train_loss: 0.013341\n",
      "[243/00881] train_loss: 0.013200\n",
      "[243/00931] train_loss: 0.013020\n",
      "[243/00981] train_loss: 0.012631\n",
      "[243/01031] train_loss: 0.013197\n",
      "[243/01081] train_loss: 0.012793\n",
      "[243/01131] train_loss: 0.013708\n",
      "[243/01181] train_loss: 0.013346\n",
      "[244/00005] train_loss: 0.011931\n",
      "[244/00055] train_loss: 0.014181\n",
      "[244/00105] train_loss: 0.013207\n",
      "[244/00155] train_loss: 0.013902\n",
      "[244/00205] train_loss: 0.013340\n",
      "[244/00255] train_loss: 0.013104\n",
      "[244/00305] train_loss: 0.013770\n",
      "[244/00355] train_loss: 0.013353\n",
      "[244/00405] train_loss: 0.014252\n",
      "[244/00455] train_loss: 0.013091\n",
      "[244/00505] train_loss: 0.012400\n",
      "[244/00555] train_loss: 0.013279\n",
      "[244/00605] train_loss: 0.012608\n",
      "[244/00655] train_loss: 0.012892\n",
      "[244/00705] train_loss: 0.012078\n",
      "[244/00755] train_loss: 0.013594\n",
      "[244/00805] train_loss: 0.012771\n",
      "[244/00855] train_loss: 0.013602\n",
      "[244/00905] train_loss: 0.013575\n",
      "[244/00955] train_loss: 0.012899\n",
      "[244/01005] train_loss: 0.012709\n",
      "[244/01055] train_loss: 0.012608\n",
      "[244/01105] train_loss: 0.012472\n",
      "[244/01155] train_loss: 0.013418\n",
      "[244/01205] train_loss: 0.012846\n",
      "[245/00029] train_loss: 0.012822\n",
      "[245/00079] train_loss: 0.014351\n",
      "[245/00129] train_loss: 0.013117\n",
      "[245/00179] train_loss: 0.014595\n",
      "[245/00229] train_loss: 0.013941\n",
      "[245/00279] train_loss: 0.013930\n",
      "[245/00329] train_loss: 0.013866\n",
      "[245/00379] train_loss: 0.012856\n",
      "[245/00429] train_loss: 0.012167\n",
      "[245/00479] train_loss: 0.012794\n",
      "[245/00529] train_loss: 0.012458\n",
      "[245/00579] train_loss: 0.013303\n",
      "[245/00629] train_loss: 0.013413\n",
      "[245/00679] train_loss: 0.012929\n",
      "[245/00729] train_loss: 0.013000\n",
      "[245/00779] train_loss: 0.012834\n",
      "[245/00829] train_loss: 0.013331\n",
      "[245/00879] train_loss: 0.012906\n",
      "[245/00929] train_loss: 0.013765\n",
      "[245/00979] train_loss: 0.013005\n",
      "[245/01029] train_loss: 0.013152\n",
      "[245/01079] train_loss: 0.012447\n",
      "[245/01129] train_loss: 0.013305\n",
      "[245/01179] train_loss: 0.013328\n",
      "[246/00003] train_loss: 0.012618\n",
      "[246/00053] train_loss: 0.013471\n",
      "[246/00103] train_loss: 0.014261\n",
      "[246/00153] train_loss: 0.014123\n",
      "[246/00203] train_loss: 0.014355\n",
      "[246/00253] train_loss: 0.012954\n",
      "[246/00303] train_loss: 0.013359\n",
      "[246/00353] train_loss: 0.013397\n",
      "[246/00403] train_loss: 0.013716\n",
      "[246/00453] train_loss: 0.013261\n",
      "[246/00503] train_loss: 0.013220\n",
      "[246/00553] train_loss: 0.013067\n",
      "[246/00603] train_loss: 0.012881\n",
      "[246/00653] train_loss: 0.013190\n",
      "[246/00703] train_loss: 0.012519\n",
      "[246/00753] train_loss: 0.013704\n",
      "[246/00803] train_loss: 0.012839\n",
      "[246/00853] train_loss: 0.012664\n",
      "[246/00903] train_loss: 0.013115\n",
      "[246/00953] train_loss: 0.013231\n",
      "[246/01003] train_loss: 0.012411\n",
      "[246/01053] train_loss: 0.012574\n",
      "[246/01103] train_loss: 0.012795\n",
      "[246/01153] train_loss: 0.013046\n",
      "[246/01203] train_loss: 0.012928\n",
      "[247/00027] train_loss: 0.013024\n",
      "[247/00077] train_loss: 0.013805\n",
      "[247/00127] train_loss: 0.013583\n",
      "[247/00177] train_loss: 0.014041\n",
      "[247/00227] train_loss: 0.013595\n",
      "[247/00277] train_loss: 0.013372\n",
      "[247/00327] train_loss: 0.013062\n",
      "[247/00377] train_loss: 0.013078\n",
      "[247/00427] train_loss: 0.012656\n",
      "[247/00477] train_loss: 0.013668\n",
      "[247/00527] train_loss: 0.012857\n",
      "[247/00577] train_loss: 0.013042\n",
      "[247/00627] train_loss: 0.013356\n",
      "[247/00677] train_loss: 0.013752\n",
      "[247/00727] train_loss: 0.012503\n",
      "[247/00777] train_loss: 0.012748\n",
      "[247/00827] train_loss: 0.012945\n",
      "[247/00877] train_loss: 0.012614\n",
      "[247/00927] train_loss: 0.012880\n",
      "[247/00977] train_loss: 0.013410\n",
      "[247/01027] train_loss: 0.013216\n",
      "[247/01077] train_loss: 0.012866\n",
      "[247/01127] train_loss: 0.013056\n",
      "[247/01177] train_loss: 0.013770\n",
      "[248/00001] train_loss: 0.013090\n",
      "[248/00051] train_loss: 0.014250\n",
      "[248/00101] train_loss: 0.014988\n",
      "[248/00151] train_loss: 0.014490\n",
      "[248/00201] train_loss: 0.012953\n",
      "[248/00251] train_loss: 0.013012\n",
      "[248/00301] train_loss: 0.013647\n",
      "[248/00351] train_loss: 0.013415\n",
      "[248/00401] train_loss: 0.013046\n",
      "[248/00451] train_loss: 0.013146\n",
      "[248/00501] train_loss: 0.013073\n",
      "[248/00551] train_loss: 0.013639\n",
      "[248/00601] train_loss: 0.012860\n",
      "[248/00651] train_loss: 0.012934\n",
      "[248/00701] train_loss: 0.012846\n",
      "[248/00751] train_loss: 0.013306\n",
      "[248/00801] train_loss: 0.012719\n",
      "[248/00851] train_loss: 0.012822\n",
      "[248/00901] train_loss: 0.012962\n",
      "[248/00951] train_loss: 0.013279\n",
      "[248/01001] train_loss: 0.012423\n",
      "[248/01051] train_loss: 0.013140\n",
      "[248/01101] train_loss: 0.012929\n",
      "[248/01151] train_loss: 0.013184\n",
      "[248/01201] train_loss: 0.012645\n",
      "[249/00025] train_loss: 0.012727\n",
      "[249/00075] train_loss: 0.014228\n",
      "[249/00125] train_loss: 0.013428\n",
      "[249/00175] train_loss: 0.014229\n",
      "[249/00225] train_loss: 0.013129\n",
      "[249/00275] train_loss: 0.012701\n",
      "[249/00325] train_loss: 0.013188\n",
      "[249/00375] train_loss: 0.013779\n",
      "[249/00425] train_loss: 0.013323\n",
      "[249/00475] train_loss: 0.012734\n",
      "[249/00525] train_loss: 0.012801\n",
      "[249/00575] train_loss: 0.013226\n",
      "[249/00625] train_loss: 0.013190\n",
      "[249/00675] train_loss: 0.012805\n",
      "[249/00725] train_loss: 0.013272\n",
      "[249/00775] train_loss: 0.013050\n",
      "[249/00825] train_loss: 0.012817\n",
      "[249/00875] train_loss: 0.012891\n",
      "[249/00925] train_loss: 0.013254\n",
      "[249/00975] train_loss: 0.013440\n",
      "[249/01025] train_loss: 0.012271\n",
      "[249/01075] train_loss: 0.012719\n",
      "[249/01125] train_loss: 0.012907\n",
      "[249/01175] train_loss: 0.012851\n",
      "[249/01225] train_loss: 0.013129\n",
      "[250/00049] train_loss: 0.014283\n",
      "[250/00099] train_loss: 0.013744\n",
      "[250/00149] train_loss: 0.013187\n",
      "[250/00199] train_loss: 0.013255\n",
      "[250/00249] train_loss: 0.012817\n",
      "[250/00299] train_loss: 0.014045\n",
      "[250/00349] train_loss: 0.012919\n",
      "[250/00399] train_loss: 0.012616\n",
      "[250/00449] train_loss: 0.013855\n",
      "[250/00499] train_loss: 0.013643\n",
      "[250/00549] train_loss: 0.012764\n",
      "[250/00599] train_loss: 0.012946\n",
      "[250/00649] train_loss: 0.013691\n",
      "[250/00699] train_loss: 0.013895\n",
      "[250/00749] train_loss: 0.013196\n",
      "[250/00799] train_loss: 0.012807\n",
      "[250/00849] train_loss: 0.013215\n",
      "[250/00899] train_loss: 0.012120\n",
      "[250/00949] train_loss: 0.012661\n",
      "[250/00999] train_loss: 0.012742\n",
      "[250/01049] train_loss: 0.012416\n",
      "[250/01099] train_loss: 0.012938\n",
      "[250/01149] train_loss: 0.014345\n",
      "[250/01199] train_loss: 0.012671\n",
      "[251/00023] train_loss: 0.013557\n",
      "[251/00073] train_loss: 0.013264\n",
      "[251/00123] train_loss: 0.014486\n",
      "[251/00173] train_loss: 0.013276\n",
      "[251/00223] train_loss: 0.013142\n",
      "[251/00273] train_loss: 0.013274\n",
      "[251/00323] train_loss: 0.012745\n",
      "[251/00373] train_loss: 0.013442\n",
      "[251/00423] train_loss: 0.013195\n",
      "[251/00473] train_loss: 0.014346\n",
      "[251/00523] train_loss: 0.012918\n",
      "[251/00573] train_loss: 0.013168\n",
      "[251/00623] train_loss: 0.013225\n",
      "[251/00673] train_loss: 0.013195\n",
      "[251/00723] train_loss: 0.013132\n",
      "[251/00773] train_loss: 0.013786\n",
      "[251/00823] train_loss: 0.013308\n",
      "[251/00873] train_loss: 0.012480\n",
      "[251/00923] train_loss: 0.013537\n",
      "[251/00973] train_loss: 0.012605\n",
      "[251/01023] train_loss: 0.013253\n",
      "[251/01073] train_loss: 0.012457\n",
      "[251/01123] train_loss: 0.012280\n",
      "[251/01173] train_loss: 0.013000\n",
      "[251/01223] train_loss: 0.013448\n",
      "[252/00047] train_loss: 0.014208\n",
      "[252/00097] train_loss: 0.013617\n",
      "[252/00147] train_loss: 0.013671\n",
      "[252/00197] train_loss: 0.014168\n",
      "[252/00247] train_loss: 0.012871\n",
      "[252/00297] train_loss: 0.013045\n",
      "[252/00347] train_loss: 0.013711\n",
      "[252/00397] train_loss: 0.012871\n",
      "[252/00447] train_loss: 0.012724\n",
      "[252/00497] train_loss: 0.013232\n",
      "[252/00547] train_loss: 0.012822\n",
      "[252/00597] train_loss: 0.012426\n",
      "[252/00647] train_loss: 0.012753\n",
      "[252/00697] train_loss: 0.012921\n",
      "[252/00747] train_loss: 0.012400\n",
      "[252/00797] train_loss: 0.012519\n",
      "[252/00847] train_loss: 0.013020\n",
      "[252/00897] train_loss: 0.012886\n",
      "[252/00947] train_loss: 0.012811\n",
      "[252/00997] train_loss: 0.012406\n",
      "[252/01047] train_loss: 0.012494\n",
      "[252/01097] train_loss: 0.012793\n",
      "[252/01147] train_loss: 0.012873\n",
      "[252/01197] train_loss: 0.013835\n",
      "[253/00021] train_loss: 0.013899\n",
      "[253/00071] train_loss: 0.014116\n",
      "[253/00121] train_loss: 0.013309\n",
      "[253/00171] train_loss: 0.013692\n",
      "[253/00221] train_loss: 0.013826\n",
      "[253/00271] train_loss: 0.013271\n",
      "[253/00321] train_loss: 0.013040\n",
      "[253/00371] train_loss: 0.012793\n",
      "[253/00421] train_loss: 0.013247\n",
      "[253/00471] train_loss: 0.013199\n",
      "[253/00521] train_loss: 0.014340\n",
      "[253/00571] train_loss: 0.013063\n",
      "[253/00621] train_loss: 0.012943\n",
      "[253/00671] train_loss: 0.013864\n",
      "[253/00721] train_loss: 0.013162\n",
      "[253/00771] train_loss: 0.012638\n",
      "[253/00821] train_loss: 0.013294\n",
      "[253/00871] train_loss: 0.013572\n",
      "[253/00921] train_loss: 0.012668\n",
      "[253/00971] train_loss: 0.013066\n",
      "[253/01021] train_loss: 0.012740\n",
      "[253/01071] train_loss: 0.012793\n",
      "[253/01121] train_loss: 0.013084\n",
      "[253/01171] train_loss: 0.013274\n",
      "[253/01221] train_loss: 0.012523\n",
      "[254/00045] train_loss: 0.014544\n",
      "[254/00095] train_loss: 0.014160\n",
      "[254/00145] train_loss: 0.013473\n",
      "[254/00195] train_loss: 0.014439\n",
      "[254/00245] train_loss: 0.012784\n",
      "[254/00295] train_loss: 0.013272\n",
      "[254/00345] train_loss: 0.013293\n",
      "[254/00395] train_loss: 0.013164\n",
      "[254/00445] train_loss: 0.013031\n",
      "[254/00495] train_loss: 0.013382\n",
      "[254/00545] train_loss: 0.012488\n",
      "[254/00595] train_loss: 0.012994\n",
      "[254/00645] train_loss: 0.012747\n",
      "[254/00695] train_loss: 0.013283\n",
      "[254/00745] train_loss: 0.013618\n",
      "[254/00795] train_loss: 0.012874\n",
      "[254/00845] train_loss: 0.013043\n",
      "[254/00895] train_loss: 0.013153\n",
      "[254/00945] train_loss: 0.012995\n",
      "[254/00995] train_loss: 0.012694\n",
      "[254/01045] train_loss: 0.012615\n",
      "[254/01095] train_loss: 0.013655\n",
      "[254/01145] train_loss: 0.012228\n",
      "[254/01195] train_loss: 0.013193\n",
      "[255/00019] train_loss: 0.013266\n",
      "[255/00069] train_loss: 0.013534\n",
      "[255/00119] train_loss: 0.013947\n",
      "[255/00169] train_loss: 0.014219\n",
      "[255/00219] train_loss: 0.013517\n",
      "[255/00269] train_loss: 0.013331\n",
      "[255/00319] train_loss: 0.012800\n",
      "[255/00369] train_loss: 0.013989\n",
      "[255/00419] train_loss: 0.013004\n",
      "[255/00469] train_loss: 0.013255\n",
      "[255/00519] train_loss: 0.013183\n",
      "[255/00569] train_loss: 0.013015\n",
      "[255/00619] train_loss: 0.012969\n",
      "[255/00669] train_loss: 0.012399\n",
      "[255/00719] train_loss: 0.013369\n",
      "[255/00769] train_loss: 0.012021\n",
      "[255/00819] train_loss: 0.012774\n",
      "[255/00869] train_loss: 0.012526\n",
      "[255/00919] train_loss: 0.014501\n",
      "[255/00969] train_loss: 0.012585\n",
      "[255/01019] train_loss: 0.012711\n",
      "[255/01069] train_loss: 0.013138\n",
      "[255/01119] train_loss: 0.012411\n",
      "[255/01169] train_loss: 0.012456\n",
      "[255/01219] train_loss: 0.012894\n",
      "[256/00043] train_loss: 0.013626\n",
      "[256/00093] train_loss: 0.013569\n",
      "[256/00143] train_loss: 0.013401\n",
      "[256/00193] train_loss: 0.013641\n",
      "[256/00243] train_loss: 0.013004\n",
      "[256/00293] train_loss: 0.014177\n",
      "[256/00343] train_loss: 0.013512\n",
      "[256/00393] train_loss: 0.013479\n",
      "[256/00443] train_loss: 0.013047\n",
      "[256/00493] train_loss: 0.013667\n",
      "[256/00543] train_loss: 0.013383\n",
      "[256/00593] train_loss: 0.013242\n",
      "[256/00643] train_loss: 0.012858\n",
      "[256/00693] train_loss: 0.012609\n",
      "[256/00743] train_loss: 0.013394\n",
      "[256/00793] train_loss: 0.012858\n",
      "[256/00843] train_loss: 0.013244\n",
      "[256/00893] train_loss: 0.012308\n",
      "[256/00943] train_loss: 0.012664\n",
      "[256/00993] train_loss: 0.013112\n",
      "[256/01043] train_loss: 0.012967\n",
      "[256/01093] train_loss: 0.012251\n",
      "[256/01143] train_loss: 0.013381\n",
      "[256/01193] train_loss: 0.012681\n",
      "[257/00017] train_loss: 0.012758\n",
      "[257/00067] train_loss: 0.013205\n",
      "[257/00117] train_loss: 0.013950\n",
      "[257/00167] train_loss: 0.013468\n",
      "[257/00217] train_loss: 0.013061\n",
      "[257/00267] train_loss: 0.013862\n",
      "[257/00317] train_loss: 0.012628\n",
      "[257/00367] train_loss: 0.013741\n",
      "[257/00417] train_loss: 0.012755\n",
      "[257/00467] train_loss: 0.013411\n",
      "[257/00517] train_loss: 0.013121\n",
      "[257/00567] train_loss: 0.012794\n",
      "[257/00617] train_loss: 0.012970\n",
      "[257/00667] train_loss: 0.013318\n",
      "[257/00717] train_loss: 0.012427\n",
      "[257/00767] train_loss: 0.013377\n",
      "[257/00817] train_loss: 0.013464\n",
      "[257/00867] train_loss: 0.013375\n",
      "[257/00917] train_loss: 0.012751\n",
      "[257/00967] train_loss: 0.012539\n",
      "[257/01017] train_loss: 0.013174\n",
      "[257/01067] train_loss: 0.012090\n",
      "[257/01117] train_loss: 0.012585\n",
      "[257/01167] train_loss: 0.013155\n",
      "[257/01217] train_loss: 0.012506\n",
      "[258/00041] train_loss: 0.014947\n",
      "[258/00091] train_loss: 0.013634\n",
      "[258/00141] train_loss: 0.013094\n",
      "[258/00191] train_loss: 0.013760\n",
      "[258/00241] train_loss: 0.012774\n",
      "[258/00291] train_loss: 0.013165\n",
      "[258/00341] train_loss: 0.012980\n",
      "[258/00391] train_loss: 0.013136\n",
      "[258/00441] train_loss: 0.014338\n",
      "[258/00491] train_loss: 0.013124\n",
      "[258/00541] train_loss: 0.013157\n",
      "[258/00591] train_loss: 0.013269\n",
      "[258/00641] train_loss: 0.013286\n",
      "[258/00691] train_loss: 0.013277\n",
      "[258/00741] train_loss: 0.012495\n",
      "[258/00791] train_loss: 0.012740\n",
      "[258/00841] train_loss: 0.014296\n",
      "[258/00891] train_loss: 0.013189\n",
      "[258/00941] train_loss: 0.012838\n",
      "[258/00991] train_loss: 0.012854\n",
      "[258/01041] train_loss: 0.013012\n",
      "[258/01091] train_loss: 0.012572\n",
      "[258/01141] train_loss: 0.012894\n",
      "[258/01191] train_loss: 0.012621\n",
      "[259/00015] train_loss: 0.013484\n",
      "[259/00065] train_loss: 0.013463\n",
      "[259/00115] train_loss: 0.013125\n",
      "[259/00165] train_loss: 0.013488\n",
      "[259/00215] train_loss: 0.013570\n",
      "[259/00265] train_loss: 0.014001\n",
      "[259/00315] train_loss: 0.013639\n",
      "[259/00365] train_loss: 0.013007\n",
      "[259/00415] train_loss: 0.012929\n",
      "[259/00465] train_loss: 0.013229\n",
      "[259/00515] train_loss: 0.013049\n",
      "[259/00565] train_loss: 0.012720\n",
      "[259/00615] train_loss: 0.012974\n",
      "[259/00665] train_loss: 0.013308\n",
      "[259/00715] train_loss: 0.012823\n",
      "[259/00765] train_loss: 0.012562\n",
      "[259/00815] train_loss: 0.012590\n",
      "[259/00865] train_loss: 0.012875\n",
      "[259/00915] train_loss: 0.012669\n",
      "[259/00965] train_loss: 0.013029\n",
      "[259/01015] train_loss: 0.013018\n",
      "[259/01065] train_loss: 0.012475\n",
      "[259/01115] train_loss: 0.013333\n",
      "[259/01165] train_loss: 0.012928\n",
      "[259/01215] train_loss: 0.012141\n",
      "[260/00039] train_loss: 0.013571\n",
      "[260/00089] train_loss: 0.013362\n",
      "[260/00139] train_loss: 0.013840\n",
      "[260/00189] train_loss: 0.013416\n",
      "[260/00239] train_loss: 0.013620\n",
      "[260/00289] train_loss: 0.012641\n",
      "[260/00339] train_loss: 0.013141\n",
      "[260/00389] train_loss: 0.013085\n",
      "[260/00439] train_loss: 0.012380\n",
      "[260/00489] train_loss: 0.012611\n",
      "[260/00539] train_loss: 0.013269\n",
      "[260/00589] train_loss: 0.013108\n",
      "[260/00639] train_loss: 0.013764\n",
      "[260/00689] train_loss: 0.012973\n",
      "[260/00739] train_loss: 0.013075\n",
      "[260/00789] train_loss: 0.012321\n",
      "[260/00839] train_loss: 0.012917\n",
      "[260/00889] train_loss: 0.012880\n",
      "[260/00939] train_loss: 0.012557\n",
      "[260/00989] train_loss: 0.012274\n",
      "[260/01039] train_loss: 0.013312\n",
      "[260/01089] train_loss: 0.013754\n",
      "[260/01139] train_loss: 0.013340\n",
      "[260/01189] train_loss: 0.012048\n",
      "[261/00013] train_loss: 0.013094\n",
      "[261/00063] train_loss: 0.013535\n",
      "[261/00113] train_loss: 0.013799\n",
      "[261/00163] train_loss: 0.013331\n",
      "[261/00213] train_loss: 0.012658\n",
      "[261/00263] train_loss: 0.012405\n",
      "[261/00313] train_loss: 0.013540\n",
      "[261/00363] train_loss: 0.013810\n",
      "[261/00413] train_loss: 0.013542\n",
      "[261/00463] train_loss: 0.013511\n",
      "[261/00513] train_loss: 0.012607\n",
      "[261/00563] train_loss: 0.012965\n",
      "[261/00613] train_loss: 0.012497\n",
      "[261/00663] train_loss: 0.012901\n",
      "[261/00713] train_loss: 0.013292\n",
      "[261/00763] train_loss: 0.012015\n",
      "[261/00813] train_loss: 0.012580\n",
      "[261/00863] train_loss: 0.013718\n",
      "[261/00913] train_loss: 0.013227\n",
      "[261/00963] train_loss: 0.012937\n",
      "[261/01013] train_loss: 0.013247\n",
      "[261/01063] train_loss: 0.013453\n",
      "[261/01113] train_loss: 0.012828\n",
      "[261/01163] train_loss: 0.012496\n",
      "[261/01213] train_loss: 0.012751\n",
      "[262/00037] train_loss: 0.013598\n",
      "[262/00087] train_loss: 0.012563\n",
      "[262/00137] train_loss: 0.013212\n",
      "[262/00187] train_loss: 0.014517\n",
      "[262/00237] train_loss: 0.014228\n",
      "[262/00287] train_loss: 0.012550\n",
      "[262/00337] train_loss: 0.013584\n",
      "[262/00387] train_loss: 0.012953\n",
      "[262/00437] train_loss: 0.012542\n",
      "[262/00487] train_loss: 0.013469\n",
      "[262/00537] train_loss: 0.012804\n",
      "[262/00587] train_loss: 0.012859\n",
      "[262/00637] train_loss: 0.012801\n",
      "[262/00687] train_loss: 0.013656\n",
      "[262/00737] train_loss: 0.013407\n",
      "[262/00787] train_loss: 0.013174\n",
      "[262/00837] train_loss: 0.013275\n",
      "[262/00887] train_loss: 0.012468\n",
      "[262/00937] train_loss: 0.013127\n",
      "[262/00987] train_loss: 0.013001\n",
      "[262/01037] train_loss: 0.013730\n",
      "[262/01087] train_loss: 0.012672\n",
      "[262/01137] train_loss: 0.012938\n",
      "[262/01187] train_loss: 0.012405\n",
      "[263/00011] train_loss: 0.012748\n",
      "[263/00061] train_loss: 0.014028\n",
      "[263/00111] train_loss: 0.014203\n",
      "[263/00161] train_loss: 0.013899\n",
      "[263/00211] train_loss: 0.012829\n",
      "[263/00261] train_loss: 0.012586\n",
      "[263/00311] train_loss: 0.013633\n",
      "[263/00361] train_loss: 0.013331\n",
      "[263/00411] train_loss: 0.012964\n",
      "[263/00461] train_loss: 0.012873\n",
      "[263/00511] train_loss: 0.012254\n",
      "[263/00561] train_loss: 0.013393\n",
      "[263/00611] train_loss: 0.012562\n",
      "[263/00661] train_loss: 0.013098\n",
      "[263/00711] train_loss: 0.013193\n",
      "[263/00761] train_loss: 0.012749\n",
      "[263/00811] train_loss: 0.012661\n",
      "[263/00861] train_loss: 0.013370\n",
      "[263/00911] train_loss: 0.012893\n",
      "[263/00961] train_loss: 0.013004\n",
      "[263/01011] train_loss: 0.012831\n",
      "[263/01061] train_loss: 0.012909\n",
      "[263/01111] train_loss: 0.012482\n",
      "[263/01161] train_loss: 0.011928\n",
      "[263/01211] train_loss: 0.012880\n",
      "[264/00035] train_loss: 0.013071\n",
      "[264/00085] train_loss: 0.013896\n",
      "[264/00135] train_loss: 0.013756\n",
      "[264/00185] train_loss: 0.014141\n",
      "[264/00235] train_loss: 0.014050\n",
      "[264/00285] train_loss: 0.013663\n",
      "[264/00335] train_loss: 0.013368\n",
      "[264/00385] train_loss: 0.013193\n",
      "[264/00435] train_loss: 0.013357\n",
      "[264/00485] train_loss: 0.012860\n",
      "[264/00535] train_loss: 0.012330\n",
      "[264/00585] train_loss: 0.012653\n",
      "[264/00635] train_loss: 0.013520\n",
      "[264/00685] train_loss: 0.013002\n",
      "[264/00735] train_loss: 0.012682\n",
      "[264/00785] train_loss: 0.012975\n",
      "[264/00835] train_loss: 0.012411\n",
      "[264/00885] train_loss: 0.012097\n",
      "[264/00935] train_loss: 0.013468\n",
      "[264/00985] train_loss: 0.012850\n",
      "[264/01035] train_loss: 0.012577\n",
      "[264/01085] train_loss: 0.012600\n",
      "[264/01135] train_loss: 0.011836\n",
      "[264/01185] train_loss: 0.013455\n",
      "[265/00009] train_loss: 0.013115\n",
      "[265/00059] train_loss: 0.013911\n",
      "[265/00109] train_loss: 0.013509\n",
      "[265/00159] train_loss: 0.013464\n",
      "[265/00209] train_loss: 0.012775\n",
      "[265/00259] train_loss: 0.013284\n",
      "[265/00309] train_loss: 0.013516\n",
      "[265/00359] train_loss: 0.012830\n",
      "[265/00409] train_loss: 0.013283\n",
      "[265/00459] train_loss: 0.012946\n",
      "[265/00509] train_loss: 0.012546\n",
      "[265/00559] train_loss: 0.012209\n",
      "[265/00609] train_loss: 0.013119\n",
      "[265/00659] train_loss: 0.012988\n",
      "[265/00709] train_loss: 0.012341\n",
      "[265/00759] train_loss: 0.012926\n",
      "[265/00809] train_loss: 0.012519\n",
      "[265/00859] train_loss: 0.012670\n",
      "[265/00909] train_loss: 0.013808\n",
      "[265/00959] train_loss: 0.013014\n",
      "[265/01009] train_loss: 0.012943\n",
      "[265/01059] train_loss: 0.012714\n",
      "[265/01109] train_loss: 0.012160\n",
      "[265/01159] train_loss: 0.012360\n",
      "[265/01209] train_loss: 0.012639\n",
      "[266/00033] train_loss: 0.013816\n",
      "[266/00083] train_loss: 0.014709\n",
      "[266/00133] train_loss: 0.014065\n",
      "[266/00183] train_loss: 0.013561\n",
      "[266/00233] train_loss: 0.013923\n",
      "[266/00283] train_loss: 0.013667\n",
      "[266/00333] train_loss: 0.012185\n",
      "[266/00383] train_loss: 0.012831\n",
      "[266/00433] train_loss: 0.014271\n",
      "[266/00483] train_loss: 0.013783\n",
      "[266/00533] train_loss: 0.013312\n",
      "[266/00583] train_loss: 0.013462\n",
      "[266/00633] train_loss: 0.013357\n",
      "[266/00683] train_loss: 0.013577\n",
      "[266/00733] train_loss: 0.012614\n",
      "[266/00783] train_loss: 0.012670\n",
      "[266/00833] train_loss: 0.012417\n",
      "[266/00883] train_loss: 0.013279\n",
      "[266/00933] train_loss: 0.012067\n",
      "[266/00983] train_loss: 0.013418\n",
      "[266/01033] train_loss: 0.013083\n",
      "[266/01083] train_loss: 0.012480\n",
      "[266/01133] train_loss: 0.012600\n",
      "[266/01183] train_loss: 0.012419\n",
      "[267/00007] train_loss: 0.012332\n",
      "[267/00057] train_loss: 0.014188\n",
      "[267/00107] train_loss: 0.013242\n",
      "[267/00157] train_loss: 0.013023\n",
      "[267/00207] train_loss: 0.013755\n",
      "[267/00257] train_loss: 0.013930\n",
      "[267/00307] train_loss: 0.013636\n",
      "[267/00357] train_loss: 0.012976\n",
      "[267/00407] train_loss: 0.012908\n",
      "[267/00457] train_loss: 0.013871\n",
      "[267/00507] train_loss: 0.014370\n",
      "[267/00557] train_loss: 0.013968\n",
      "[267/00607] train_loss: 0.012335\n",
      "[267/00657] train_loss: 0.012954\n",
      "[267/00707] train_loss: 0.013012\n",
      "[267/00757] train_loss: 0.013435\n",
      "[267/00807] train_loss: 0.012363\n",
      "[267/00857] train_loss: 0.012101\n",
      "[267/00907] train_loss: 0.012558\n",
      "[267/00957] train_loss: 0.013686\n",
      "[267/01007] train_loss: 0.011842\n",
      "[267/01057] train_loss: 0.012522\n",
      "[267/01107] train_loss: 0.012746\n",
      "[267/01157] train_loss: 0.012307\n",
      "[267/01207] train_loss: 0.012649\n",
      "[268/00031] train_loss: 0.013579\n",
      "[268/00081] train_loss: 0.013692\n",
      "[268/00131] train_loss: 0.013316\n",
      "[268/00181] train_loss: 0.013129\n",
      "[268/00231] train_loss: 0.013086\n",
      "[268/00281] train_loss: 0.013495\n",
      "[268/00331] train_loss: 0.012463\n",
      "[268/00381] train_loss: 0.013116\n",
      "[268/00431] train_loss: 0.013511\n",
      "[268/00481] train_loss: 0.013683\n",
      "[268/00531] train_loss: 0.012222\n",
      "[268/00581] train_loss: 0.013214\n",
      "[268/00631] train_loss: 0.012738\n",
      "[268/00681] train_loss: 0.013142\n",
      "[268/00731] train_loss: 0.013175\n",
      "[268/00781] train_loss: 0.013001\n",
      "[268/00831] train_loss: 0.012733\n",
      "[268/00881] train_loss: 0.013115\n",
      "[268/00931] train_loss: 0.012858\n",
      "[268/00981] train_loss: 0.012680\n",
      "[268/01031] train_loss: 0.011980\n",
      "[268/01081] train_loss: 0.012425\n",
      "[268/01131] train_loss: 0.012506\n",
      "[268/01181] train_loss: 0.012919\n",
      "[269/00005] train_loss: 0.013039\n",
      "[269/00055] train_loss: 0.014020\n",
      "[269/00105] train_loss: 0.013324\n",
      "[269/00155] train_loss: 0.013739\n",
      "[269/00205] train_loss: 0.012735\n",
      "[269/00255] train_loss: 0.012966\n",
      "[269/00305] train_loss: 0.012983\n",
      "[269/00355] train_loss: 0.012456\n",
      "[269/00405] train_loss: 0.013480\n",
      "[269/00455] train_loss: 0.012538\n",
      "[269/00505] train_loss: 0.012553\n",
      "[269/00555] train_loss: 0.012816\n",
      "[269/00605] train_loss: 0.013184\n",
      "[269/00655] train_loss: 0.013593\n",
      "[269/00705] train_loss: 0.012514\n",
      "[269/00755] train_loss: 0.013285\n",
      "[269/00805] train_loss: 0.012724\n",
      "[269/00855] train_loss: 0.012576\n",
      "[269/00905] train_loss: 0.013949\n",
      "[269/00955] train_loss: 0.012756\n",
      "[269/01005] train_loss: 0.012413\n",
      "[269/01055] train_loss: 0.013587\n",
      "[269/01105] train_loss: 0.012966\n",
      "[269/01155] train_loss: 0.012751\n",
      "[269/01205] train_loss: 0.012955\n",
      "[270/00029] train_loss: 0.013306\n",
      "[270/00079] train_loss: 0.013514\n",
      "[270/00129] train_loss: 0.013702\n",
      "[270/00179] train_loss: 0.013483\n",
      "[270/00229] train_loss: 0.013735\n",
      "[270/00279] train_loss: 0.013853\n",
      "[270/00329] train_loss: 0.013645\n",
      "[270/00379] train_loss: 0.013063\n",
      "[270/00429] train_loss: 0.012819\n",
      "[270/00479] train_loss: 0.012503\n",
      "[270/00529] train_loss: 0.012837\n",
      "[270/00579] train_loss: 0.012774\n",
      "[270/00629] train_loss: 0.012760\n",
      "[270/00679] train_loss: 0.012216\n",
      "[270/00729] train_loss: 0.013221\n",
      "[270/00779] train_loss: 0.012208\n",
      "[270/00829] train_loss: 0.013285\n",
      "[270/00879] train_loss: 0.012706\n",
      "[270/00929] train_loss: 0.013465\n",
      "[270/00979] train_loss: 0.013152\n",
      "[270/01029] train_loss: 0.013086\n",
      "[270/01079] train_loss: 0.012665\n",
      "[270/01129] train_loss: 0.012995\n",
      "[270/01179] train_loss: 0.012128\n",
      "[271/00003] train_loss: 0.012343\n",
      "[271/00053] train_loss: 0.013786\n",
      "[271/00103] train_loss: 0.014084\n",
      "[271/00153] train_loss: 0.013742\n",
      "[271/00203] train_loss: 0.013061\n",
      "[271/00253] train_loss: 0.013533\n",
      "[271/00303] train_loss: 0.013400\n",
      "[271/00353] train_loss: 0.013032\n",
      "[271/00403] train_loss: 0.012818\n",
      "[271/00453] train_loss: 0.012376\n",
      "[271/00503] train_loss: 0.012029\n",
      "[271/00553] train_loss: 0.012630\n",
      "[271/00603] train_loss: 0.012560\n",
      "[271/00653] train_loss: 0.012770\n",
      "[271/00703] train_loss: 0.012903\n",
      "[271/00753] train_loss: 0.012948\n",
      "[271/00803] train_loss: 0.013050\n",
      "[271/00853] train_loss: 0.012669\n",
      "[271/00903] train_loss: 0.013165\n",
      "[271/00953] train_loss: 0.012266\n",
      "[271/01003] train_loss: 0.012195\n",
      "[271/01053] train_loss: 0.012438\n",
      "[271/01103] train_loss: 0.012841\n",
      "[271/01153] train_loss: 0.013324\n",
      "[271/01203] train_loss: 0.012164\n",
      "[272/00027] train_loss: 0.012677\n",
      "[272/00077] train_loss: 0.014023\n",
      "[272/00127] train_loss: 0.013020\n",
      "[272/00177] train_loss: 0.013027\n",
      "[272/00227] train_loss: 0.013305\n",
      "[272/00277] train_loss: 0.012877\n",
      "[272/00327] train_loss: 0.013135\n",
      "[272/00377] train_loss: 0.012876\n",
      "[272/00427] train_loss: 0.012952\n",
      "[272/00477] train_loss: 0.013311\n",
      "[272/00527] train_loss: 0.012831\n",
      "[272/00577] train_loss: 0.013463\n",
      "[272/00627] train_loss: 0.013118\n",
      "[272/00677] train_loss: 0.012155\n",
      "[272/00727] train_loss: 0.012633\n",
      "[272/00777] train_loss: 0.012928\n",
      "[272/00827] train_loss: 0.012850\n",
      "[272/00877] train_loss: 0.012637\n",
      "[272/00927] train_loss: 0.012865\n",
      "[272/00977] train_loss: 0.013534\n",
      "[272/01027] train_loss: 0.013272\n",
      "[272/01077] train_loss: 0.012810\n",
      "[272/01127] train_loss: 0.012218\n",
      "[272/01177] train_loss: 0.012764\n",
      "[273/00001] train_loss: 0.013711\n",
      "[273/00051] train_loss: 0.014940\n",
      "[273/00101] train_loss: 0.013688\n",
      "[273/00151] train_loss: 0.013631\n",
      "[273/00201] train_loss: 0.013242\n",
      "[273/00251] train_loss: 0.014020\n",
      "[273/00301] train_loss: 0.012978\n",
      "[273/00351] train_loss: 0.013936\n",
      "[273/00401] train_loss: 0.012763\n",
      "[273/00451] train_loss: 0.013304\n",
      "[273/00501] train_loss: 0.012446\n",
      "[273/00551] train_loss: 0.012621\n",
      "[273/00601] train_loss: 0.013532\n",
      "[273/00651] train_loss: 0.012630\n",
      "[273/00701] train_loss: 0.012311\n",
      "[273/00751] train_loss: 0.012992\n",
      "[273/00801] train_loss: 0.012965\n",
      "[273/00851] train_loss: 0.012646\n",
      "[273/00901] train_loss: 0.012612\n",
      "[273/00951] train_loss: 0.011998\n",
      "[273/01001] train_loss: 0.011969\n",
      "[273/01051] train_loss: 0.012868\n",
      "[273/01101] train_loss: 0.013309\n",
      "[273/01151] train_loss: 0.012564\n",
      "[273/01201] train_loss: 0.012438\n",
      "[274/00025] train_loss: 0.013670\n",
      "[274/00075] train_loss: 0.014392\n",
      "[274/00125] train_loss: 0.012496\n",
      "[274/00175] train_loss: 0.013395\n",
      "[274/00225] train_loss: 0.013300\n",
      "[274/00275] train_loss: 0.013258\n",
      "[274/00325] train_loss: 0.012631\n",
      "[274/00375] train_loss: 0.012930\n",
      "[274/00425] train_loss: 0.013301\n",
      "[274/00475] train_loss: 0.012734\n",
      "[274/00525] train_loss: 0.013006\n",
      "[274/00575] train_loss: 0.013918\n",
      "[274/00625] train_loss: 0.012619\n",
      "[274/00675] train_loss: 0.012844\n",
      "[274/00725] train_loss: 0.013365\n",
      "[274/00775] train_loss: 0.012552\n",
      "[274/00825] train_loss: 0.013113\n",
      "[274/00875] train_loss: 0.012758\n",
      "[274/00925] train_loss: 0.013259\n",
      "[274/00975] train_loss: 0.013286\n",
      "[274/01025] train_loss: 0.012411\n",
      "[274/01075] train_loss: 0.013159\n",
      "[274/01125] train_loss: 0.013069\n",
      "[274/01175] train_loss: 0.012881\n",
      "[274/01225] train_loss: 0.012638\n",
      "[275/00049] train_loss: 0.013919\n",
      "[275/00099] train_loss: 0.013134\n",
      "[275/00149] train_loss: 0.014091\n",
      "[275/00199] train_loss: 0.013098\n",
      "[275/00249] train_loss: 0.013331\n",
      "[275/00299] train_loss: 0.013144\n",
      "[275/00349] train_loss: 0.013157\n",
      "[275/00399] train_loss: 0.013215\n",
      "[275/00449] train_loss: 0.012992\n",
      "[275/00499] train_loss: 0.012821\n",
      "[275/00549] train_loss: 0.012845\n",
      "[275/00599] train_loss: 0.012875\n",
      "[275/00649] train_loss: 0.012884\n",
      "[275/00699] train_loss: 0.012361\n",
      "[275/00749] train_loss: 0.012719\n",
      "[275/00799] train_loss: 0.013435\n",
      "[275/00849] train_loss: 0.012504\n",
      "[275/00899] train_loss: 0.012431\n",
      "[275/00949] train_loss: 0.013125\n",
      "[275/00999] train_loss: 0.012465\n",
      "[275/01049] train_loss: 0.013370\n",
      "[275/01099] train_loss: 0.013816\n",
      "[275/01149] train_loss: 0.013542\n",
      "[275/01199] train_loss: 0.013527\n",
      "[276/00023] train_loss: 0.012540\n",
      "[276/00073] train_loss: 0.013728\n",
      "[276/00123] train_loss: 0.013335\n",
      "[276/00173] train_loss: 0.013187\n",
      "[276/00223] train_loss: 0.013496\n",
      "[276/00273] train_loss: 0.013554\n",
      "[276/00323] train_loss: 0.013038\n",
      "[276/00373] train_loss: 0.013034\n",
      "[276/00423] train_loss: 0.012974\n",
      "[276/00473] train_loss: 0.013213\n",
      "[276/00523] train_loss: 0.013326\n",
      "[276/00573] train_loss: 0.013362\n",
      "[276/00623] train_loss: 0.013035\n",
      "[276/00673] train_loss: 0.012915\n",
      "[276/00723] train_loss: 0.012870\n",
      "[276/00773] train_loss: 0.013161\n",
      "[276/00823] train_loss: 0.012384\n",
      "[276/00873] train_loss: 0.012835\n",
      "[276/00923] train_loss: 0.013110\n",
      "[276/00973] train_loss: 0.012992\n",
      "[276/01023] train_loss: 0.012241\n",
      "[276/01073] train_loss: 0.012559\n",
      "[276/01123] train_loss: 0.012205\n",
      "[276/01173] train_loss: 0.012819\n",
      "[276/01223] train_loss: 0.012535\n",
      "[277/00047] train_loss: 0.013822\n",
      "[277/00097] train_loss: 0.013013\n",
      "[277/00147] train_loss: 0.013597\n",
      "[277/00197] train_loss: 0.013291\n",
      "[277/00247] train_loss: 0.012941\n",
      "[277/00297] train_loss: 0.013211\n",
      "[277/00347] train_loss: 0.013321\n",
      "[277/00397] train_loss: 0.012846\n",
      "[277/00447] train_loss: 0.012550\n",
      "[277/00497] train_loss: 0.013230\n",
      "[277/00547] train_loss: 0.012834\n",
      "[277/00597] train_loss: 0.013051\n",
      "[277/00647] train_loss: 0.013031\n",
      "[277/00697] train_loss: 0.012632\n",
      "[277/00747] train_loss: 0.012944\n",
      "[277/00797] train_loss: 0.013989\n",
      "[277/00847] train_loss: 0.013576\n",
      "[277/00897] train_loss: 0.012536\n",
      "[277/00947] train_loss: 0.012448\n",
      "[277/00997] train_loss: 0.013151\n",
      "[277/01047] train_loss: 0.012924\n",
      "[277/01097] train_loss: 0.012370\n",
      "[277/01147] train_loss: 0.012345\n",
      "[277/01197] train_loss: 0.012097\n",
      "[278/00021] train_loss: 0.012848\n",
      "[278/00071] train_loss: 0.013402\n",
      "[278/00121] train_loss: 0.013754\n",
      "[278/00171] train_loss: 0.013272\n",
      "[278/00221] train_loss: 0.012473\n",
      "[278/00271] train_loss: 0.013134\n",
      "[278/00321] train_loss: 0.012794\n",
      "[278/00371] train_loss: 0.012734\n",
      "[278/00421] train_loss: 0.012890\n",
      "[278/00471] train_loss: 0.012923\n",
      "[278/00521] train_loss: 0.012649\n",
      "[278/00571] train_loss: 0.013753\n",
      "[278/00621] train_loss: 0.012798\n",
      "[278/00671] train_loss: 0.013700\n",
      "[278/00721] train_loss: 0.013466\n",
      "[278/00771] train_loss: 0.012164\n",
      "[278/00821] train_loss: 0.013349\n",
      "[278/00871] train_loss: 0.012884\n",
      "[278/00921] train_loss: 0.012493\n",
      "[278/00971] train_loss: 0.013050\n",
      "[278/01021] train_loss: 0.012440\n",
      "[278/01071] train_loss: 0.013300\n",
      "[278/01121] train_loss: 0.011859\n",
      "[278/01171] train_loss: 0.012320\n",
      "[278/01221] train_loss: 0.012349\n",
      "[279/00045] train_loss: 0.013758\n",
      "[279/00095] train_loss: 0.012482\n",
      "[279/00145] train_loss: 0.013106\n",
      "[279/00195] train_loss: 0.012693\n",
      "[279/00245] train_loss: 0.013280\n",
      "[279/00295] train_loss: 0.012988\n",
      "[279/00345] train_loss: 0.013631\n",
      "[279/00395] train_loss: 0.012732\n",
      "[279/00445] train_loss: 0.013643\n",
      "[279/00495] train_loss: 0.012981\n",
      "[279/00545] train_loss: 0.012797\n",
      "[279/00595] train_loss: 0.013320\n",
      "[279/00645] train_loss: 0.013443\n",
      "[279/00695] train_loss: 0.013914\n",
      "[279/00745] train_loss: 0.012115\n",
      "[279/00795] train_loss: 0.012926\n",
      "[279/00845] train_loss: 0.012694\n",
      "[279/00895] train_loss: 0.012673\n",
      "[279/00945] train_loss: 0.013081\n",
      "[279/00995] train_loss: 0.011869\n",
      "[279/01045] train_loss: 0.013060\n",
      "[279/01095] train_loss: 0.012153\n",
      "[279/01145] train_loss: 0.012850\n",
      "[279/01195] train_loss: 0.012767\n",
      "[280/00019] train_loss: 0.012575\n",
      "[280/00069] train_loss: 0.013753\n",
      "[280/00119] train_loss: 0.012789\n",
      "[280/00169] train_loss: 0.013098\n",
      "[280/00219] train_loss: 0.013341\n",
      "[280/00269] train_loss: 0.013565\n",
      "[280/00319] train_loss: 0.013070\n",
      "[280/00369] train_loss: 0.013001\n",
      "[280/00419] train_loss: 0.012786\n",
      "[280/00469] train_loss: 0.012168\n",
      "[280/00519] train_loss: 0.013398\n",
      "[280/00569] train_loss: 0.012585\n",
      "[280/00619] train_loss: 0.012897\n",
      "[280/00669] train_loss: 0.013229\n",
      "[280/00719] train_loss: 0.012852\n",
      "[280/00769] train_loss: 0.013027\n",
      "[280/00819] train_loss: 0.012939\n",
      "[280/00869] train_loss: 0.012797\n",
      "[280/00919] train_loss: 0.012532\n",
      "[280/00969] train_loss: 0.012773\n",
      "[280/01019] train_loss: 0.013051\n",
      "[280/01069] train_loss: 0.013260\n",
      "[280/01119] train_loss: 0.012900\n",
      "[280/01169] train_loss: 0.012545\n",
      "[280/01219] train_loss: 0.013606\n",
      "[281/00043] train_loss: 0.013682\n",
      "[281/00093] train_loss: 0.013748\n",
      "[281/00143] train_loss: 0.013646\n",
      "[281/00193] train_loss: 0.013877\n",
      "[281/00243] train_loss: 0.012897\n",
      "[281/00293] train_loss: 0.013105\n",
      "[281/00343] train_loss: 0.013664\n",
      "[281/00393] train_loss: 0.013048\n",
      "[281/00443] train_loss: 0.012421\n",
      "[281/00493] train_loss: 0.012620\n",
      "[281/00543] train_loss: 0.013782\n",
      "[281/00593] train_loss: 0.013027\n",
      "[281/00643] train_loss: 0.013128\n",
      "[281/00693] train_loss: 0.012981\n",
      "[281/00743] train_loss: 0.012550\n",
      "[281/00793] train_loss: 0.012246\n",
      "[281/00843] train_loss: 0.012555\n",
      "[281/00893] train_loss: 0.012440\n",
      "[281/00943] train_loss: 0.013039\n",
      "[281/00993] train_loss: 0.012582\n",
      "[281/01043] train_loss: 0.012629\n",
      "[281/01093] train_loss: 0.012164\n",
      "[281/01143] train_loss: 0.012810\n",
      "[281/01193] train_loss: 0.012389\n",
      "[282/00017] train_loss: 0.012788\n",
      "[282/00067] train_loss: 0.013386\n",
      "[282/00117] train_loss: 0.014072\n",
      "[282/00167] train_loss: 0.013632\n",
      "[282/00217] train_loss: 0.013323\n",
      "[282/00267] train_loss: 0.013052\n",
      "[282/00317] train_loss: 0.013654\n",
      "[282/00367] train_loss: 0.013071\n",
      "[282/00417] train_loss: 0.013731\n",
      "[282/00467] train_loss: 0.012670\n",
      "[282/00517] train_loss: 0.012946\n",
      "[282/00567] train_loss: 0.013358\n",
      "[282/00617] train_loss: 0.012263\n",
      "[282/00667] train_loss: 0.012411\n",
      "[282/00717] train_loss: 0.012765\n",
      "[282/00767] train_loss: 0.012853\n",
      "[282/00817] train_loss: 0.012909\n",
      "[282/00867] train_loss: 0.012758\n",
      "[282/00917] train_loss: 0.013328\n",
      "[282/00967] train_loss: 0.012556\n",
      "[282/01017] train_loss: 0.013097\n",
      "[282/01067] train_loss: 0.012519\n",
      "[282/01117] train_loss: 0.012363\n",
      "[282/01167] train_loss: 0.012562\n",
      "[282/01217] train_loss: 0.012430\n",
      "[283/00041] train_loss: 0.013229\n",
      "[283/00091] train_loss: 0.013536\n",
      "[283/00141] train_loss: 0.013571\n",
      "[283/00191] train_loss: 0.013782\n",
      "[283/00241] train_loss: 0.013779\n",
      "[283/00291] train_loss: 0.012454\n",
      "[283/00341] train_loss: 0.012751\n",
      "[283/00391] train_loss: 0.012803\n",
      "[283/00441] train_loss: 0.012974\n",
      "[283/00491] train_loss: 0.013322\n",
      "[283/00541] train_loss: 0.013003\n",
      "[283/00591] train_loss: 0.013703\n",
      "[283/00641] train_loss: 0.012456\n",
      "[283/00691] train_loss: 0.012634\n",
      "[283/00741] train_loss: 0.012579\n",
      "[283/00791] train_loss: 0.012946\n",
      "[283/00841] train_loss: 0.012439\n",
      "[283/00891] train_loss: 0.011923\n",
      "[283/00941] train_loss: 0.012269\n",
      "[283/00991] train_loss: 0.012768\n",
      "[283/01041] train_loss: 0.012575\n",
      "[283/01091] train_loss: 0.012205\n",
      "[283/01141] train_loss: 0.013142\n",
      "[283/01191] train_loss: 0.012669\n",
      "[284/00015] train_loss: 0.012675\n",
      "[284/00065] train_loss: 0.014524\n",
      "[284/00115] train_loss: 0.013905\n",
      "[284/00165] train_loss: 0.012685\n",
      "[284/00215] train_loss: 0.013219\n",
      "[284/00265] train_loss: 0.012875\n",
      "[284/00315] train_loss: 0.013146\n",
      "[284/00365] train_loss: 0.013146\n",
      "[284/00415] train_loss: 0.013142\n",
      "[284/00465] train_loss: 0.014096\n",
      "[284/00515] train_loss: 0.013152\n",
      "[284/00565] train_loss: 0.012932\n",
      "[284/00615] train_loss: 0.012544\n",
      "[284/00665] train_loss: 0.012679\n",
      "[284/00715] train_loss: 0.012479\n",
      "[284/00765] train_loss: 0.013280\n",
      "[284/00815] train_loss: 0.012453\n",
      "[284/00865] train_loss: 0.013213\n",
      "[284/00915] train_loss: 0.012559\n",
      "[284/00965] train_loss: 0.012498\n",
      "[284/01015] train_loss: 0.012716\n",
      "[284/01065] train_loss: 0.012893\n",
      "[284/01115] train_loss: 0.012179\n",
      "[284/01165] train_loss: 0.013284\n",
      "[284/01215] train_loss: 0.012812\n",
      "[285/00039] train_loss: 0.013178\n",
      "[285/00089] train_loss: 0.012493\n",
      "[285/00139] train_loss: 0.013168\n",
      "[285/00189] train_loss: 0.013363\n",
      "[285/00239] train_loss: 0.012575\n",
      "[285/00289] train_loss: 0.012943\n",
      "[285/00339] train_loss: 0.013326\n",
      "[285/00389] train_loss: 0.013533\n",
      "[285/00439] train_loss: 0.012742\n",
      "[285/00489] train_loss: 0.011948\n",
      "[285/00539] train_loss: 0.012873\n",
      "[285/00589] train_loss: 0.013262\n",
      "[285/00639] train_loss: 0.012643\n",
      "[285/00689] train_loss: 0.012995\n",
      "[285/00739] train_loss: 0.012664\n",
      "[285/00789] train_loss: 0.013278\n",
      "[285/00839] train_loss: 0.012758\n",
      "[285/00889] train_loss: 0.012740\n",
      "[285/00939] train_loss: 0.013654\n",
      "[285/00989] train_loss: 0.012665\n",
      "[285/01039] train_loss: 0.012486\n",
      "[285/01089] train_loss: 0.012624\n",
      "[285/01139] train_loss: 0.012558\n",
      "[285/01189] train_loss: 0.012767\n",
      "[286/00013] train_loss: 0.012419\n",
      "[286/00063] train_loss: 0.013781\n",
      "[286/00113] train_loss: 0.012631\n",
      "[286/00163] train_loss: 0.013568\n",
      "[286/00213] train_loss: 0.012674\n",
      "[286/00263] train_loss: 0.013425\n",
      "[286/00313] train_loss: 0.013923\n",
      "[286/00363] train_loss: 0.012852\n",
      "[286/00413] train_loss: 0.012785\n",
      "[286/00463] train_loss: 0.013125\n",
      "[286/00513] train_loss: 0.013016\n",
      "[286/00563] train_loss: 0.012028\n",
      "[286/00613] train_loss: 0.013009\n",
      "[286/00663] train_loss: 0.012820\n",
      "[286/00713] train_loss: 0.013012\n",
      "[286/00763] train_loss: 0.013399\n",
      "[286/00813] train_loss: 0.012552\n",
      "[286/00863] train_loss: 0.012820\n",
      "[286/00913] train_loss: 0.012219\n",
      "[286/00963] train_loss: 0.013516\n",
      "[286/01013] train_loss: 0.013122\n",
      "[286/01063] train_loss: 0.012884\n",
      "[286/01113] train_loss: 0.012531\n",
      "[286/01163] train_loss: 0.012059\n",
      "[286/01213] train_loss: 0.012628\n",
      "[287/00037] train_loss: 0.013582\n",
      "[287/00087] train_loss: 0.013280\n",
      "[287/00137] train_loss: 0.013563\n",
      "[287/00187] train_loss: 0.012795\n",
      "[287/00237] train_loss: 0.013252\n",
      "[287/00287] train_loss: 0.012220\n",
      "[287/00337] train_loss: 0.012571\n",
      "[287/00387] train_loss: 0.012730\n",
      "[287/00437] train_loss: 0.013153\n",
      "[287/00487] train_loss: 0.013110\n",
      "[287/00537] train_loss: 0.012305\n",
      "[287/00587] train_loss: 0.012570\n",
      "[287/00637] train_loss: 0.012442\n",
      "[287/00687] train_loss: 0.013178\n",
      "[287/00737] train_loss: 0.012676\n",
      "[287/00787] train_loss: 0.013246\n",
      "[287/00837] train_loss: 0.012512\n",
      "[287/00887] train_loss: 0.012769\n",
      "[287/00937] train_loss: 0.012428\n",
      "[287/00987] train_loss: 0.012421\n",
      "[287/01037] train_loss: 0.013513\n",
      "[287/01087] train_loss: 0.012670\n",
      "[287/01137] train_loss: 0.013289\n",
      "[287/01187] train_loss: 0.012165\n",
      "[288/00011] train_loss: 0.012462\n",
      "[288/00061] train_loss: 0.013197\n",
      "[288/00111] train_loss: 0.014001\n",
      "[288/00161] train_loss: 0.013318\n",
      "[288/00211] train_loss: 0.012984\n",
      "[288/00261] train_loss: 0.012698\n",
      "[288/00311] train_loss: 0.013010\n",
      "[288/00361] train_loss: 0.013606\n",
      "[288/00411] train_loss: 0.012305\n",
      "[288/00461] train_loss: 0.013581\n",
      "[288/00511] train_loss: 0.013574\n",
      "[288/00561] train_loss: 0.012351\n",
      "[288/00611] train_loss: 0.012449\n",
      "[288/00661] train_loss: 0.013448\n",
      "[288/00711] train_loss: 0.012420\n",
      "[288/00761] train_loss: 0.013383\n",
      "[288/00811] train_loss: 0.012595\n",
      "[288/00861] train_loss: 0.013088\n",
      "[288/00911] train_loss: 0.012884\n",
      "[288/00961] train_loss: 0.012143\n",
      "[288/01011] train_loss: 0.013595\n",
      "[288/01061] train_loss: 0.012481\n",
      "[288/01111] train_loss: 0.013228\n",
      "[288/01161] train_loss: 0.012711\n",
      "[288/01211] train_loss: 0.012568\n",
      "[289/00035] train_loss: 0.013969\n",
      "[289/00085] train_loss: 0.013003\n",
      "[289/00135] train_loss: 0.012862\n",
      "[289/00185] train_loss: 0.012919\n",
      "[289/00235] train_loss: 0.014344\n",
      "[289/00285] train_loss: 0.012574\n",
      "[289/00335] train_loss: 0.013077\n",
      "[289/00385] train_loss: 0.012990\n",
      "[289/00435] train_loss: 0.013034\n",
      "[289/00485] train_loss: 0.012912\n",
      "[289/00535] train_loss: 0.012660\n",
      "[289/00585] train_loss: 0.013063\n",
      "[289/00635] train_loss: 0.012848\n",
      "[289/00685] train_loss: 0.013055\n",
      "[289/00735] train_loss: 0.013079\n",
      "[289/00785] train_loss: 0.012924\n",
      "[289/00835] train_loss: 0.012057\n",
      "[289/00885] train_loss: 0.012420\n",
      "[289/00935] train_loss: 0.012283\n",
      "[289/00985] train_loss: 0.013845\n",
      "[289/01035] train_loss: 0.013151\n",
      "[289/01085] train_loss: 0.012021\n",
      "[289/01135] train_loss: 0.012902\n",
      "[289/01185] train_loss: 0.013022\n",
      "[290/00009] train_loss: 0.011900\n",
      "[290/00059] train_loss: 0.013132\n",
      "[290/00109] train_loss: 0.012990\n",
      "[290/00159] train_loss: 0.013346\n",
      "[290/00209] train_loss: 0.013341\n",
      "[290/00259] train_loss: 0.013137\n",
      "[290/00309] train_loss: 0.012410\n",
      "[290/00359] train_loss: 0.012871\n",
      "[290/00409] train_loss: 0.012863\n",
      "[290/00459] train_loss: 0.013821\n",
      "[290/00509] train_loss: 0.013470\n",
      "[290/00559] train_loss: 0.012670\n",
      "[290/00609] train_loss: 0.012896\n",
      "[290/00659] train_loss: 0.012170\n",
      "[290/00709] train_loss: 0.012368\n",
      "[290/00759] train_loss: 0.012983\n",
      "[290/00809] train_loss: 0.013036\n",
      "[290/00859] train_loss: 0.012926\n",
      "[290/00909] train_loss: 0.013471\n",
      "[290/00959] train_loss: 0.013169\n",
      "[290/01009] train_loss: 0.012701\n",
      "[290/01059] train_loss: 0.012673\n",
      "[290/01109] train_loss: 0.013341\n",
      "[290/01159] train_loss: 0.012853\n",
      "[290/01209] train_loss: 0.012028\n",
      "[291/00033] train_loss: 0.013689\n",
      "[291/00083] train_loss: 0.014001\n",
      "[291/00133] train_loss: 0.013236\n",
      "[291/00183] train_loss: 0.013375\n",
      "[291/00233] train_loss: 0.013376\n",
      "[291/00283] train_loss: 0.012896\n",
      "[291/00333] train_loss: 0.012972\n",
      "[291/00383] train_loss: 0.013257\n",
      "[291/00433] train_loss: 0.012583\n",
      "[291/00483] train_loss: 0.013159\n",
      "[291/00533] train_loss: 0.012421\n",
      "[291/00583] train_loss: 0.012923\n",
      "[291/00633] train_loss: 0.012467\n",
      "[291/00683] train_loss: 0.011908\n",
      "[291/00733] train_loss: 0.012897\n",
      "[291/00783] train_loss: 0.012547\n",
      "[291/00833] train_loss: 0.012827\n",
      "[291/00883] train_loss: 0.012590\n",
      "[291/00933] train_loss: 0.012359\n",
      "[291/00983] train_loss: 0.012535\n",
      "[291/01033] train_loss: 0.012263\n",
      "[291/01083] train_loss: 0.013013\n",
      "[291/01133] train_loss: 0.013201\n",
      "[291/01183] train_loss: 0.013294\n",
      "[292/00007] train_loss: 0.012739\n",
      "[292/00057] train_loss: 0.013368\n",
      "[292/00107] train_loss: 0.013810\n",
      "[292/00157] train_loss: 0.013595\n",
      "[292/00207] train_loss: 0.013077\n",
      "[292/00257] train_loss: 0.013705\n",
      "[292/00307] train_loss: 0.013069\n",
      "[292/00357] train_loss: 0.013590\n",
      "[292/00407] train_loss: 0.012633\n",
      "[292/00457] train_loss: 0.013339\n",
      "[292/00507] train_loss: 0.012645\n",
      "[292/00557] train_loss: 0.012627\n",
      "[292/00607] train_loss: 0.012445\n",
      "[292/00657] train_loss: 0.012428\n",
      "[292/00707] train_loss: 0.013031\n",
      "[292/00757] train_loss: 0.012721\n",
      "[292/00807] train_loss: 0.012634\n",
      "[292/00857] train_loss: 0.012373\n",
      "[292/00907] train_loss: 0.012070\n",
      "[292/00957] train_loss: 0.012309\n",
      "[292/01007] train_loss: 0.012936\n",
      "[292/01057] train_loss: 0.012707\n",
      "[292/01107] train_loss: 0.013327\n",
      "[292/01157] train_loss: 0.012796\n",
      "[292/01207] train_loss: 0.012130\n",
      "[293/00031] train_loss: 0.013183\n",
      "[293/00081] train_loss: 0.013691\n",
      "[293/00131] train_loss: 0.012927\n",
      "[293/00181] train_loss: 0.014143\n",
      "[293/00231] train_loss: 0.012826\n",
      "[293/00281] train_loss: 0.012748\n",
      "[293/00331] train_loss: 0.013512\n",
      "[293/00381] train_loss: 0.012720\n",
      "[293/00431] train_loss: 0.013289\n",
      "[293/00481] train_loss: 0.012895\n",
      "[293/00531] train_loss: 0.013436\n",
      "[293/00581] train_loss: 0.012817\n",
      "[293/00631] train_loss: 0.012932\n",
      "[293/00681] train_loss: 0.012226\n",
      "[293/00731] train_loss: 0.012198\n",
      "[293/00781] train_loss: 0.012740\n",
      "[293/00831] train_loss: 0.012371\n",
      "[293/00881] train_loss: 0.013005\n",
      "[293/00931] train_loss: 0.012683\n",
      "[293/00981] train_loss: 0.012246\n",
      "[293/01031] train_loss: 0.012620\n",
      "[293/01081] train_loss: 0.012489\n",
      "[293/01131] train_loss: 0.012769\n",
      "[293/01181] train_loss: 0.012256\n",
      "[294/00005] train_loss: 0.012198\n",
      "[294/00055] train_loss: 0.013967\n",
      "[294/00105] train_loss: 0.013123\n",
      "[294/00155] train_loss: 0.012855\n",
      "[294/00205] train_loss: 0.012317\n",
      "[294/00255] train_loss: 0.012975\n",
      "[294/00305] train_loss: 0.012727\n",
      "[294/00355] train_loss: 0.012487\n",
      "[294/00405] train_loss: 0.013514\n",
      "[294/00455] train_loss: 0.012930\n",
      "[294/00505] train_loss: 0.012982\n",
      "[294/00555] train_loss: 0.012616\n",
      "[294/00605] train_loss: 0.012819\n",
      "[294/00655] train_loss: 0.013243\n",
      "[294/00705] train_loss: 0.012835\n",
      "[294/00755] train_loss: 0.012664\n",
      "[294/00805] train_loss: 0.013051\n",
      "[294/00855] train_loss: 0.012300\n",
      "[294/00905] train_loss: 0.012716\n",
      "[294/00955] train_loss: 0.013314\n",
      "[294/01005] train_loss: 0.013141\n",
      "[294/01055] train_loss: 0.012541\n",
      "[294/01105] train_loss: 0.013096\n",
      "[294/01155] train_loss: 0.012210\n",
      "[294/01205] train_loss: 0.012619\n",
      "[295/00029] train_loss: 0.013363\n",
      "[295/00079] train_loss: 0.013767\n",
      "[295/00129] train_loss: 0.013628\n",
      "[295/00179] train_loss: 0.012952\n",
      "[295/00229] train_loss: 0.012338\n",
      "[295/00279] train_loss: 0.013160\n",
      "[295/00329] train_loss: 0.013084\n",
      "[295/00379] train_loss: 0.012849\n",
      "[295/00429] train_loss: 0.013003\n",
      "[295/00479] train_loss: 0.013045\n",
      "[295/00529] train_loss: 0.013301\n",
      "[295/00579] train_loss: 0.013429\n",
      "[295/00629] train_loss: 0.012542\n",
      "[295/00679] train_loss: 0.012848\n",
      "[295/00729] train_loss: 0.013161\n",
      "[295/00779] train_loss: 0.013635\n",
      "[295/00829] train_loss: 0.012601\n",
      "[295/00879] train_loss: 0.013256\n",
      "[295/00929] train_loss: 0.012537\n",
      "[295/00979] train_loss: 0.012737\n",
      "[295/01029] train_loss: 0.012347\n",
      "[295/01079] train_loss: 0.012895\n",
      "[295/01129] train_loss: 0.012418\n",
      "[295/01179] train_loss: 0.012817\n",
      "[296/00003] train_loss: 0.012629\n",
      "[296/00053] train_loss: 0.014531\n",
      "[296/00103] train_loss: 0.013733\n",
      "[296/00153] train_loss: 0.013385\n",
      "[296/00203] train_loss: 0.013215\n",
      "[296/00253] train_loss: 0.012450\n",
      "[296/00303] train_loss: 0.012542\n",
      "[296/00353] train_loss: 0.013053\n",
      "[296/00403] train_loss: 0.013522\n",
      "[296/00453] train_loss: 0.013646\n",
      "[296/00503] train_loss: 0.012747\n",
      "[296/00553] train_loss: 0.012615\n",
      "[296/00603] train_loss: 0.012611\n",
      "[296/00653] train_loss: 0.012571\n",
      "[296/00703] train_loss: 0.012475\n",
      "[296/00753] train_loss: 0.012906\n",
      "[296/00803] train_loss: 0.012689\n",
      "[296/00853] train_loss: 0.012269\n",
      "[296/00903] train_loss: 0.011874\n",
      "[296/00953] train_loss: 0.011817\n",
      "[296/01003] train_loss: 0.013194\n",
      "[296/01053] train_loss: 0.012509\n",
      "[296/01103] train_loss: 0.013919\n",
      "[296/01153] train_loss: 0.012373\n",
      "[296/01203] train_loss: 0.013211\n",
      "[297/00027] train_loss: 0.012997\n",
      "[297/00077] train_loss: 0.013738\n",
      "[297/00127] train_loss: 0.013290\n",
      "[297/00177] train_loss: 0.013276\n",
      "[297/00227] train_loss: 0.012416\n",
      "[297/00277] train_loss: 0.013332\n",
      "[297/00327] train_loss: 0.012678\n",
      "[297/00377] train_loss: 0.013098\n",
      "[297/00427] train_loss: 0.013154\n",
      "[297/00477] train_loss: 0.013270\n",
      "[297/00527] train_loss: 0.012845\n",
      "[297/00577] train_loss: 0.012883\n",
      "[297/00627] train_loss: 0.012645\n",
      "[297/00677] train_loss: 0.013029\n",
      "[297/00727] train_loss: 0.012513\n",
      "[297/00777] train_loss: 0.012546\n",
      "[297/00827] train_loss: 0.012824\n",
      "[297/00877] train_loss: 0.012281\n",
      "[297/00927] train_loss: 0.012550\n",
      "[297/00977] train_loss: 0.012757\n",
      "[297/01027] train_loss: 0.012400\n",
      "[297/01077] train_loss: 0.012671\n",
      "[297/01127] train_loss: 0.012826\n",
      "[297/01177] train_loss: 0.012364\n",
      "[298/00001] train_loss: 0.013105\n",
      "[298/00051] train_loss: 0.013869\n",
      "[298/00101] train_loss: 0.012798\n",
      "[298/00151] train_loss: 0.013555\n",
      "[298/00201] train_loss: 0.013852\n",
      "[298/00251] train_loss: 0.013509\n",
      "[298/00301] train_loss: 0.012868\n",
      "[298/00351] train_loss: 0.013166\n",
      "[298/00401] train_loss: 0.013036\n",
      "[298/00451] train_loss: 0.012838\n",
      "[298/00501] train_loss: 0.012488\n",
      "[298/00551] train_loss: 0.012371\n",
      "[298/00601] train_loss: 0.012063\n",
      "[298/00651] train_loss: 0.012589\n",
      "[298/00701] train_loss: 0.012284\n",
      "[298/00751] train_loss: 0.013205\n",
      "[298/00801] train_loss: 0.012571\n",
      "[298/00851] train_loss: 0.013035\n",
      "[298/00901] train_loss: 0.012980\n",
      "[298/00951] train_loss: 0.013501\n",
      "[298/01001] train_loss: 0.012273\n",
      "[298/01051] train_loss: 0.013278\n",
      "[298/01101] train_loss: 0.012178\n",
      "[298/01151] train_loss: 0.012309\n",
      "[298/01201] train_loss: 0.012318\n",
      "[299/00025] train_loss: 0.012961\n",
      "[299/00075] train_loss: 0.013156\n",
      "[299/00125] train_loss: 0.013180\n",
      "[299/00175] train_loss: 0.012794\n",
      "[299/00225] train_loss: 0.013392\n",
      "[299/00275] train_loss: 0.012914\n",
      "[299/00325] train_loss: 0.013300\n",
      "[299/00375] train_loss: 0.013509\n",
      "[299/00425] train_loss: 0.012576\n",
      "[299/00475] train_loss: 0.012941\n",
      "[299/00525] train_loss: 0.013794\n",
      "[299/00575] train_loss: 0.012722\n",
      "[299/00625] train_loss: 0.012432\n",
      "[299/00675] train_loss: 0.012588\n",
      "[299/00725] train_loss: 0.012207\n",
      "[299/00775] train_loss: 0.012556\n",
      "[299/00825] train_loss: 0.012618\n",
      "[299/00875] train_loss: 0.012286\n",
      "[299/00925] train_loss: 0.012867\n",
      "[299/00975] train_loss: 0.012302\n",
      "[299/01025] train_loss: 0.012784\n",
      "[299/01075] train_loss: 0.012580\n",
      "[299/01125] train_loss: 0.012572\n",
      "[299/01175] train_loss: 0.012729\n",
      "[299/01225] train_loss: 0.012655\n",
      "[300/00049] train_loss: 0.013818\n",
      "[300/00099] train_loss: 0.013380\n",
      "[300/00149] train_loss: 0.013254\n",
      "[300/00199] train_loss: 0.012726\n",
      "[300/00249] train_loss: 0.013738\n",
      "[300/00299] train_loss: 0.013315\n",
      "[300/00349] train_loss: 0.012936\n",
      "[300/00399] train_loss: 0.012742\n",
      "[300/00449] train_loss: 0.012622\n",
      "[300/00499] train_loss: 0.013404\n",
      "[300/00549] train_loss: 0.012649\n",
      "[300/00599] train_loss: 0.012634\n",
      "[300/00649] train_loss: 0.013244\n",
      "[300/00699] train_loss: 0.012820\n",
      "[300/00749] train_loss: 0.013203\n",
      "[300/00799] train_loss: 0.012025\n",
      "[300/00849] train_loss: 0.012114\n",
      "[300/00899] train_loss: 0.013016\n",
      "[300/00949] train_loss: 0.012803\n",
      "[300/00999] train_loss: 0.011712\n",
      "[300/01049] train_loss: 0.013224\n",
      "[300/01099] train_loss: 0.013173\n",
      "[300/01149] train_loss: 0.012596\n",
      "[300/01199] train_loss: 0.012556\n",
      "[301/00023] train_loss: 0.013468\n",
      "[301/00073] train_loss: 0.012871\n",
      "[301/00123] train_loss: 0.013443\n",
      "[301/00173] train_loss: 0.013770\n",
      "[301/00223] train_loss: 0.012748\n",
      "[301/00273] train_loss: 0.012964\n",
      "[301/00323] train_loss: 0.012591\n",
      "[301/00373] train_loss: 0.012419\n",
      "[301/00423] train_loss: 0.013405\n",
      "[301/00473] train_loss: 0.012805\n",
      "[301/00523] train_loss: 0.012812\n",
      "[301/00573] train_loss: 0.012497\n",
      "[301/00623] train_loss: 0.012563\n",
      "[301/00673] train_loss: 0.012893\n",
      "[301/00723] train_loss: 0.012776\n",
      "[301/00773] train_loss: 0.012776\n",
      "[301/00823] train_loss: 0.012660\n",
      "[301/00873] train_loss: 0.012612\n",
      "[301/00923] train_loss: 0.012886\n",
      "[301/00973] train_loss: 0.012124\n",
      "[301/01023] train_loss: 0.013205\n",
      "[301/01073] train_loss: 0.013160\n",
      "[301/01123] train_loss: 0.012533\n",
      "[301/01173] train_loss: 0.012903\n",
      "[301/01223] train_loss: 0.012956\n",
      "[302/00047] train_loss: 0.013477\n",
      "[302/00097] train_loss: 0.013485\n",
      "[302/00147] train_loss: 0.013174\n",
      "[302/00197] train_loss: 0.012754\n",
      "[302/00247] train_loss: 0.013750\n",
      "[302/00297] train_loss: 0.012861\n",
      "[302/00347] train_loss: 0.013183\n",
      "[302/00397] train_loss: 0.012905\n",
      "[302/00447] train_loss: 0.014020\n",
      "[302/00497] train_loss: 0.013020\n",
      "[302/00547] train_loss: 0.012395\n",
      "[302/00597] train_loss: 0.013406\n",
      "[302/00647] train_loss: 0.012182\n",
      "[302/00697] train_loss: 0.013227\n",
      "[302/00747] train_loss: 0.012146\n",
      "[302/00797] train_loss: 0.012193\n",
      "[302/00847] train_loss: 0.012535\n",
      "[302/00897] train_loss: 0.013503\n",
      "[302/00947] train_loss: 0.011855\n",
      "[302/00997] train_loss: 0.013111\n",
      "[302/01047] train_loss: 0.012768\n",
      "[302/01097] train_loss: 0.012513\n",
      "[302/01147] train_loss: 0.012004\n",
      "[302/01197] train_loss: 0.013080\n",
      "[303/00021] train_loss: 0.012842\n",
      "[303/00071] train_loss: 0.013604\n",
      "[303/00121] train_loss: 0.013296\n",
      "[303/00171] train_loss: 0.013037\n",
      "[303/00221] train_loss: 0.013410\n",
      "[303/00271] train_loss: 0.012683\n",
      "[303/00321] train_loss: 0.013118\n",
      "[303/00371] train_loss: 0.012717\n",
      "[303/00421] train_loss: 0.012987\n",
      "[303/00471] train_loss: 0.012065\n",
      "[303/00521] train_loss: 0.012208\n",
      "[303/00571] train_loss: 0.013352\n",
      "[303/00621] train_loss: 0.013058\n",
      "[303/00671] train_loss: 0.013237\n",
      "[303/00721] train_loss: 0.012848\n",
      "[303/00771] train_loss: 0.012573\n",
      "[303/00821] train_loss: 0.012807\n",
      "[303/00871] train_loss: 0.011816\n",
      "[303/00921] train_loss: 0.012433\n",
      "[303/00971] train_loss: 0.012936\n",
      "[303/01021] train_loss: 0.012432\n",
      "[303/01071] train_loss: 0.012911\n",
      "[303/01121] train_loss: 0.012487\n",
      "[303/01171] train_loss: 0.012795\n",
      "[303/01221] train_loss: 0.012935\n",
      "[304/00045] train_loss: 0.013355\n",
      "[304/00095] train_loss: 0.013776\n",
      "[304/00145] train_loss: 0.013131\n",
      "[304/00195] train_loss: 0.012774\n",
      "[304/00245] train_loss: 0.013000\n",
      "[304/00295] train_loss: 0.012837\n",
      "[304/00345] train_loss: 0.012685\n",
      "[304/00395] train_loss: 0.012331\n",
      "[304/00445] train_loss: 0.013148\n",
      "[304/00495] train_loss: 0.012733\n",
      "[304/00545] train_loss: 0.012613\n",
      "[304/00595] train_loss: 0.012280\n",
      "[304/00645] train_loss: 0.012006\n",
      "[304/00695] train_loss: 0.013351\n",
      "[304/00745] train_loss: 0.012928\n",
      "[304/00795] train_loss: 0.012578\n",
      "[304/00845] train_loss: 0.013009\n",
      "[304/00895] train_loss: 0.013687\n",
      "[304/00945] train_loss: 0.012523\n",
      "[304/00995] train_loss: 0.012932\n",
      "[304/01045] train_loss: 0.012917\n",
      "[304/01095] train_loss: 0.013346\n",
      "[304/01145] train_loss: 0.012386\n",
      "[304/01195] train_loss: 0.012946\n",
      "[305/00019] train_loss: 0.013404\n",
      "[305/00069] train_loss: 0.013279\n",
      "[305/00119] train_loss: 0.014093\n",
      "[305/00169] train_loss: 0.013062\n",
      "[305/00219] train_loss: 0.012071\n",
      "[305/00269] train_loss: 0.012587\n",
      "[305/00319] train_loss: 0.012938\n",
      "[305/00369] train_loss: 0.012964\n",
      "[305/00419] train_loss: 0.013165\n",
      "[305/00469] train_loss: 0.012115\n",
      "[305/00519] train_loss: 0.013262\n",
      "[305/00569] train_loss: 0.012668\n",
      "[305/00619] train_loss: 0.011821\n",
      "[305/00669] train_loss: 0.013263\n",
      "[305/00719] train_loss: 0.013706\n",
      "[305/00769] train_loss: 0.012718\n",
      "[305/00819] train_loss: 0.012564\n",
      "[305/00869] train_loss: 0.012543\n",
      "[305/00919] train_loss: 0.012952\n",
      "[305/00969] train_loss: 0.012644\n",
      "[305/01019] train_loss: 0.013497\n",
      "[305/01069] train_loss: 0.012688\n",
      "[305/01119] train_loss: 0.012403\n",
      "[305/01169] train_loss: 0.012533\n",
      "[305/01219] train_loss: 0.012225\n",
      "[306/00043] train_loss: 0.013473\n",
      "[306/00093] train_loss: 0.013069\n",
      "[306/00143] train_loss: 0.013430\n",
      "[306/00193] train_loss: 0.013469\n",
      "[306/00243] train_loss: 0.012859\n",
      "[306/00293] train_loss: 0.012676\n",
      "[306/00343] train_loss: 0.013140\n",
      "[306/00393] train_loss: 0.012262\n",
      "[306/00443] train_loss: 0.011933\n",
      "[306/00493] train_loss: 0.012993\n",
      "[306/00543] train_loss: 0.012506\n",
      "[306/00593] train_loss: 0.012074\n",
      "[306/00643] train_loss: 0.012561\n",
      "[306/00693] train_loss: 0.012725\n",
      "[306/00743] train_loss: 0.012893\n",
      "[306/00793] train_loss: 0.012014\n",
      "[306/00843] train_loss: 0.012560\n",
      "[306/00893] train_loss: 0.012663\n",
      "[306/00943] train_loss: 0.012978\n",
      "[306/00993] train_loss: 0.012154\n",
      "[306/01043] train_loss: 0.013142\n",
      "[306/01093] train_loss: 0.012629\n",
      "[306/01143] train_loss: 0.012769\n",
      "[306/01193] train_loss: 0.012633\n",
      "[307/00017] train_loss: 0.012583\n",
      "[307/00067] train_loss: 0.013431\n",
      "[307/00117] train_loss: 0.013485\n",
      "[307/00167] train_loss: 0.012751\n",
      "[307/00217] train_loss: 0.013278\n",
      "[307/00267] train_loss: 0.012297\n",
      "[307/00317] train_loss: 0.012669\n",
      "[307/00367] train_loss: 0.011944\n",
      "[307/00417] train_loss: 0.012276\n",
      "[307/00467] train_loss: 0.012541\n",
      "[307/00517] train_loss: 0.012814\n",
      "[307/00567] train_loss: 0.013180\n",
      "[307/00617] train_loss: 0.012842\n",
      "[307/00667] train_loss: 0.012429\n",
      "[307/00717] train_loss: 0.013005\n",
      "[307/00767] train_loss: 0.013022\n",
      "[307/00817] train_loss: 0.012863\n",
      "[307/00867] train_loss: 0.013595\n",
      "[307/00917] train_loss: 0.012967\n",
      "[307/00967] train_loss: 0.012382\n",
      "[307/01017] train_loss: 0.012493\n",
      "[307/01067] train_loss: 0.013043\n",
      "[307/01117] train_loss: 0.012710\n",
      "[307/01167] train_loss: 0.011579\n",
      "[307/01217] train_loss: 0.011687\n",
      "[308/00041] train_loss: 0.012948\n",
      "[308/00091] train_loss: 0.013845\n",
      "[308/00141] train_loss: 0.014041\n",
      "[308/00191] train_loss: 0.013747\n",
      "[308/00241] train_loss: 0.012869\n",
      "[308/00291] train_loss: 0.013674\n",
      "[308/00341] train_loss: 0.012547\n",
      "[308/00391] train_loss: 0.013446\n",
      "[308/00441] train_loss: 0.013322\n",
      "[308/00491] train_loss: 0.012900\n",
      "[308/00541] train_loss: 0.012758\n",
      "[308/00591] train_loss: 0.012635\n",
      "[308/00641] train_loss: 0.012507\n",
      "[308/00691] train_loss: 0.012572\n",
      "[308/00741] train_loss: 0.012064\n",
      "[308/00791] train_loss: 0.013343\n",
      "[308/00841] train_loss: 0.012253\n",
      "[308/00891] train_loss: 0.012282\n",
      "[308/00941] train_loss: 0.012617\n",
      "[308/00991] train_loss: 0.012543\n",
      "[308/01041] train_loss: 0.012601\n",
      "[308/01091] train_loss: 0.012509\n",
      "[308/01141] train_loss: 0.012640\n",
      "[308/01191] train_loss: 0.012188\n",
      "[309/00015] train_loss: 0.013148\n",
      "[309/00065] train_loss: 0.012842\n",
      "[309/00115] train_loss: 0.012746\n",
      "[309/00165] train_loss: 0.012784\n",
      "[309/00215] train_loss: 0.013465\n",
      "[309/00265] train_loss: 0.012765\n",
      "[309/00315] train_loss: 0.013419\n",
      "[309/00365] train_loss: 0.012143\n",
      "[309/00415] train_loss: 0.013012\n",
      "[309/00465] train_loss: 0.012904\n",
      "[309/00515] train_loss: 0.012934\n",
      "[309/00565] train_loss: 0.012223\n",
      "[309/00615] train_loss: 0.012986\n",
      "[309/00665] train_loss: 0.013081\n",
      "[309/00715] train_loss: 0.012884\n",
      "[309/00765] train_loss: 0.011951\n",
      "[309/00815] train_loss: 0.012391\n",
      "[309/00865] train_loss: 0.013287\n",
      "[309/00915] train_loss: 0.012581\n",
      "[309/00965] train_loss: 0.013148\n",
      "[309/01015] train_loss: 0.012318\n",
      "[309/01065] train_loss: 0.012392\n",
      "[309/01115] train_loss: 0.012909\n",
      "[309/01165] train_loss: 0.012553\n",
      "[309/01215] train_loss: 0.012359\n",
      "[310/00039] train_loss: 0.014249\n",
      "[310/00089] train_loss: 0.013382\n",
      "[310/00139] train_loss: 0.013231\n",
      "[310/00189] train_loss: 0.013065\n",
      "[310/00239] train_loss: 0.012729\n",
      "[310/00289] train_loss: 0.013085\n",
      "[310/00339] train_loss: 0.012804\n",
      "[310/00389] train_loss: 0.013139\n",
      "[310/00439] train_loss: 0.013413\n",
      "[310/00489] train_loss: 0.013478\n",
      "[310/00539] train_loss: 0.013319\n",
      "[310/00589] train_loss: 0.013202\n",
      "[310/00639] train_loss: 0.012612\n",
      "[310/00689] train_loss: 0.012770\n",
      "[310/00739] train_loss: 0.013002\n",
      "[310/00789] train_loss: 0.012590\n",
      "[310/00839] train_loss: 0.011990\n",
      "[310/00889] train_loss: 0.012105\n",
      "[310/00939] train_loss: 0.012687\n",
      "[310/00989] train_loss: 0.012465\n",
      "[310/01039] train_loss: 0.012349\n",
      "[310/01089] train_loss: 0.013336\n",
      "[310/01139] train_loss: 0.012121\n",
      "[310/01189] train_loss: 0.012090\n",
      "[311/00013] train_loss: 0.013206\n",
      "[311/00063] train_loss: 0.013415\n",
      "[311/00113] train_loss: 0.012514\n",
      "[311/00163] train_loss: 0.013695\n",
      "[311/00213] train_loss: 0.012752\n",
      "[311/00263] train_loss: 0.013113\n",
      "[311/00313] train_loss: 0.012931\n",
      "[311/00363] train_loss: 0.012882\n",
      "[311/00413] train_loss: 0.012484\n",
      "[311/00463] train_loss: 0.012838\n",
      "[311/00513] train_loss: 0.012208\n",
      "[311/00563] train_loss: 0.012405\n",
      "[311/00613] train_loss: 0.012194\n",
      "[311/00663] train_loss: 0.013029\n",
      "[311/00713] train_loss: 0.012433\n",
      "[311/00763] train_loss: 0.012613\n",
      "[311/00813] train_loss: 0.012769\n",
      "[311/00863] train_loss: 0.012396\n",
      "[311/00913] train_loss: 0.013081\n",
      "[311/00963] train_loss: 0.013474\n",
      "[311/01013] train_loss: 0.012282\n",
      "[311/01063] train_loss: 0.013308\n",
      "[311/01113] train_loss: 0.012090\n",
      "[311/01163] train_loss: 0.012725\n",
      "[311/01213] train_loss: 0.012109\n",
      "[312/00037] train_loss: 0.013742\n",
      "[312/00087] train_loss: 0.013592\n",
      "[312/00137] train_loss: 0.013829\n",
      "[312/00187] train_loss: 0.012514\n",
      "[312/00237] train_loss: 0.012692\n",
      "[312/00287] train_loss: 0.012650\n",
      "[312/00337] train_loss: 0.013098\n",
      "[312/00387] train_loss: 0.012902\n",
      "[312/00437] train_loss: 0.013306\n",
      "[312/00487] train_loss: 0.012413\n",
      "[312/00537] train_loss: 0.013765\n",
      "[312/00587] train_loss: 0.011933\n",
      "[312/00637] train_loss: 0.012433\n",
      "[312/00687] train_loss: 0.012781\n",
      "[312/00737] train_loss: 0.012886\n",
      "[312/00787] train_loss: 0.012659\n",
      "[312/00837] train_loss: 0.012997\n",
      "[312/00887] train_loss: 0.012368\n",
      "[312/00937] train_loss: 0.012566\n",
      "[312/00987] train_loss: 0.011721\n",
      "[312/01037] train_loss: 0.012965\n",
      "[312/01087] train_loss: 0.012619\n",
      "[312/01137] train_loss: 0.012298\n",
      "[312/01187] train_loss: 0.012683\n",
      "[313/00011] train_loss: 0.013057\n",
      "[313/00061] train_loss: 0.013418\n",
      "[313/00111] train_loss: 0.013522\n",
      "[313/00161] train_loss: 0.012816\n",
      "[313/00211] train_loss: 0.013177\n",
      "[313/00261] train_loss: 0.012890\n",
      "[313/00311] train_loss: 0.013094\n",
      "[313/00361] train_loss: 0.012563\n",
      "[313/00411] train_loss: 0.013436\n",
      "[313/00461] train_loss: 0.011672\n",
      "[313/00511] train_loss: 0.012798\n",
      "[313/00561] train_loss: 0.013039\n",
      "[313/00611] train_loss: 0.013219\n",
      "[313/00661] train_loss: 0.012885\n",
      "[313/00711] train_loss: 0.012153\n",
      "[313/00761] train_loss: 0.012230\n",
      "[313/00811] train_loss: 0.012744\n",
      "[313/00861] train_loss: 0.012474\n",
      "[313/00911] train_loss: 0.012245\n",
      "[313/00961] train_loss: 0.012104\n",
      "[313/01011] train_loss: 0.012503\n",
      "[313/01061] train_loss: 0.012241\n",
      "[313/01111] train_loss: 0.012854\n",
      "[313/01161] train_loss: 0.012870\n",
      "[313/01211] train_loss: 0.012842\n",
      "[314/00035] train_loss: 0.013135\n",
      "[314/00085] train_loss: 0.012628\n",
      "[314/00135] train_loss: 0.013195\n",
      "[314/00185] train_loss: 0.012447\n",
      "[314/00235] train_loss: 0.013263\n",
      "[314/00285] train_loss: 0.012155\n",
      "[314/00335] train_loss: 0.012681\n",
      "[314/00385] train_loss: 0.012823\n",
      "[314/00435] train_loss: 0.013169\n",
      "[314/00485] train_loss: 0.013500\n",
      "[314/00535] train_loss: 0.012565\n",
      "[314/00585] train_loss: 0.012349\n",
      "[314/00635] train_loss: 0.012846\n",
      "[314/00685] train_loss: 0.013154\n",
      "[314/00735] train_loss: 0.012954\n",
      "[314/00785] train_loss: 0.012361\n",
      "[314/00835] train_loss: 0.012847\n",
      "[314/00885] train_loss: 0.012566\n",
      "[314/00935] train_loss: 0.012516\n",
      "[314/00985] train_loss: 0.012598\n",
      "[314/01035] train_loss: 0.013774\n",
      "[314/01085] train_loss: 0.012511\n",
      "[314/01135] train_loss: 0.012106\n",
      "[314/01185] train_loss: 0.012653\n",
      "[315/00009] train_loss: 0.012558\n",
      "[315/00059] train_loss: 0.012925\n",
      "[315/00109] train_loss: 0.012889\n",
      "[315/00159] train_loss: 0.014150\n",
      "[315/00209] train_loss: 0.013018\n",
      "[315/00259] train_loss: 0.012944\n",
      "[315/00309] train_loss: 0.012848\n",
      "[315/00359] train_loss: 0.013803\n",
      "[315/00409] train_loss: 0.012161\n",
      "[315/00459] train_loss: 0.012968\n",
      "[315/00509] train_loss: 0.012458\n",
      "[315/00559] train_loss: 0.012720\n",
      "[315/00609] train_loss: 0.012588\n",
      "[315/00659] train_loss: 0.012771\n",
      "[315/00709] train_loss: 0.012112\n",
      "[315/00759] train_loss: 0.013167\n",
      "[315/00809] train_loss: 0.012456\n",
      "[315/00859] train_loss: 0.012962\n",
      "[315/00909] train_loss: 0.012128\n",
      "[315/00959] train_loss: 0.013362\n",
      "[315/01009] train_loss: 0.012046\n",
      "[315/01059] train_loss: 0.012397\n",
      "[315/01109] train_loss: 0.012224\n",
      "[315/01159] train_loss: 0.012320\n",
      "[315/01209] train_loss: 0.012156\n",
      "[316/00033] train_loss: 0.013447\n",
      "[316/00083] train_loss: 0.013215\n",
      "[316/00133] train_loss: 0.013729\n",
      "[316/00183] train_loss: 0.012925\n",
      "[316/00233] train_loss: 0.012591\n",
      "[316/00283] train_loss: 0.012856\n",
      "[316/00333] train_loss: 0.013339\n",
      "[316/00383] train_loss: 0.013082\n",
      "[316/00433] train_loss: 0.012798\n",
      "[316/00483] train_loss: 0.012877\n",
      "[316/00533] train_loss: 0.011996\n",
      "[316/00583] train_loss: 0.012919\n",
      "[316/00633] train_loss: 0.012991\n",
      "[316/00683] train_loss: 0.012380\n",
      "[316/00733] train_loss: 0.012739\n",
      "[316/00783] train_loss: 0.011920\n",
      "[316/00833] train_loss: 0.011927\n",
      "[316/00883] train_loss: 0.012932\n",
      "[316/00933] train_loss: 0.012860\n",
      "[316/00983] train_loss: 0.013148\n",
      "[316/01033] train_loss: 0.012517\n",
      "[316/01083] train_loss: 0.012715\n",
      "[316/01133] train_loss: 0.012587\n",
      "[316/01183] train_loss: 0.012473\n",
      "[317/00007] train_loss: 0.012771\n",
      "[317/00057] train_loss: 0.012936\n",
      "[317/00107] train_loss: 0.013511\n",
      "[317/00157] train_loss: 0.013498\n",
      "[317/00207] train_loss: 0.013134\n",
      "[317/00257] train_loss: 0.012705\n",
      "[317/00307] train_loss: 0.013381\n",
      "[317/00357] train_loss: 0.013227\n",
      "[317/00407] train_loss: 0.013091\n",
      "[317/00457] train_loss: 0.012122\n",
      "[317/00507] train_loss: 0.012891\n",
      "[317/00557] train_loss: 0.013211\n",
      "[317/00607] train_loss: 0.012265\n",
      "[317/00657] train_loss: 0.012745\n",
      "[317/00707] train_loss: 0.012646\n",
      "[317/00757] train_loss: 0.012594\n",
      "[317/00807] train_loss: 0.012227\n",
      "[317/00857] train_loss: 0.012125\n",
      "[317/00907] train_loss: 0.012114\n",
      "[317/00957] train_loss: 0.013130\n",
      "[317/01007] train_loss: 0.012780\n",
      "[317/01057] train_loss: 0.013073\n",
      "[317/01107] train_loss: 0.012554\n",
      "[317/01157] train_loss: 0.012715\n",
      "[317/01207] train_loss: 0.013283\n",
      "[318/00031] train_loss: 0.012453\n",
      "[318/00081] train_loss: 0.013529\n",
      "[318/00131] train_loss: 0.013571\n",
      "[318/00181] train_loss: 0.012413\n",
      "[318/00231] train_loss: 0.013349\n",
      "[318/00281] train_loss: 0.012306\n",
      "[318/00331] train_loss: 0.012445\n",
      "[318/00381] train_loss: 0.012509\n",
      "[318/00431] train_loss: 0.013575\n",
      "[318/00481] train_loss: 0.012162\n",
      "[318/00531] train_loss: 0.012693\n",
      "[318/00581] train_loss: 0.012521\n",
      "[318/00631] train_loss: 0.012419\n",
      "[318/00681] train_loss: 0.013071\n",
      "[318/00731] train_loss: 0.013024\n",
      "[318/00781] train_loss: 0.013900\n",
      "[318/00831] train_loss: 0.012936\n",
      "[318/00881] train_loss: 0.012474\n",
      "[318/00931] train_loss: 0.011851\n",
      "[318/00981] train_loss: 0.012882\n",
      "[318/01031] train_loss: 0.011986\n",
      "[318/01081] train_loss: 0.012199\n",
      "[318/01131] train_loss: 0.012715\n",
      "[318/01181] train_loss: 0.013024\n",
      "[319/00005] train_loss: 0.012469\n",
      "[319/00055] train_loss: 0.013091\n",
      "[319/00105] train_loss: 0.013509\n",
      "[319/00155] train_loss: 0.012206\n",
      "[319/00205] train_loss: 0.012529\n",
      "[319/00255] train_loss: 0.013822\n",
      "[319/00305] train_loss: 0.013368\n",
      "[319/00355] train_loss: 0.013070\n",
      "[319/00405] train_loss: 0.012981\n",
      "[319/00455] train_loss: 0.012892\n",
      "[319/00505] train_loss: 0.013003\n",
      "[319/00555] train_loss: 0.012410\n",
      "[319/00605] train_loss: 0.012287\n",
      "[319/00655] train_loss: 0.011859\n",
      "[319/00705] train_loss: 0.012513\n",
      "[319/00755] train_loss: 0.012656\n",
      "[319/00805] train_loss: 0.013193\n",
      "[319/00855] train_loss: 0.012361\n",
      "[319/00905] train_loss: 0.012232\n",
      "[319/00955] train_loss: 0.012402\n",
      "[319/01005] train_loss: 0.012580\n",
      "[319/01055] train_loss: 0.012504\n",
      "[319/01105] train_loss: 0.012262\n",
      "[319/01155] train_loss: 0.012226\n",
      "[319/01205] train_loss: 0.013082\n",
      "[320/00029] train_loss: 0.013298\n",
      "[320/00079] train_loss: 0.013066\n",
      "[320/00129] train_loss: 0.013255\n",
      "[320/00179] train_loss: 0.012956\n",
      "[320/00229] train_loss: 0.013275\n",
      "[320/00279] train_loss: 0.013594\n",
      "[320/00329] train_loss: 0.013299\n",
      "[320/00379] train_loss: 0.012731\n",
      "[320/00429] train_loss: 0.012293\n",
      "[320/00479] train_loss: 0.012309\n",
      "[320/00529] train_loss: 0.012373\n",
      "[320/00579] train_loss: 0.012182\n",
      "[320/00629] train_loss: 0.012436\n",
      "[320/00679] train_loss: 0.013364\n",
      "[320/00729] train_loss: 0.012719\n",
      "[320/00779] train_loss: 0.011938\n",
      "[320/00829] train_loss: 0.013429\n",
      "[320/00879] train_loss: 0.012989\n",
      "[320/00929] train_loss: 0.012315\n",
      "[320/00979] train_loss: 0.011796\n",
      "[320/01029] train_loss: 0.012514\n",
      "[320/01079] train_loss: 0.012787\n",
      "[320/01129] train_loss: 0.012790\n",
      "[320/01179] train_loss: 0.011837\n",
      "[321/00003] train_loss: 0.012189\n",
      "[321/00053] train_loss: 0.013161\n",
      "[321/00103] train_loss: 0.013398\n",
      "[321/00153] train_loss: 0.012900\n",
      "[321/00203] train_loss: 0.013151\n",
      "[321/00253] train_loss: 0.012700\n",
      "[321/00303] train_loss: 0.012771\n",
      "[321/00353] train_loss: 0.012857\n",
      "[321/00403] train_loss: 0.012984\n",
      "[321/00453] train_loss: 0.012466\n",
      "[321/00503] train_loss: 0.013757\n",
      "[321/00553] train_loss: 0.011882\n",
      "[321/00603] train_loss: 0.012378\n",
      "[321/00653] train_loss: 0.013124\n",
      "[321/00703] train_loss: 0.011988\n",
      "[321/00753] train_loss: 0.012368\n",
      "[321/00803] train_loss: 0.012437\n",
      "[321/00853] train_loss: 0.012547\n",
      "[321/00903] train_loss: 0.012499\n",
      "[321/00953] train_loss: 0.012941\n",
      "[321/01003] train_loss: 0.012825\n",
      "[321/01053] train_loss: 0.012491\n",
      "[321/01103] train_loss: 0.011285\n",
      "[321/01153] train_loss: 0.011832\n",
      "[321/01203] train_loss: 0.013483\n",
      "[322/00027] train_loss: 0.012741\n",
      "[322/00077] train_loss: 0.013440\n",
      "[322/00127] train_loss: 0.013705\n",
      "[322/00177] train_loss: 0.012799\n",
      "[322/00227] train_loss: 0.012360\n",
      "[322/00277] train_loss: 0.012761\n",
      "[322/00327] train_loss: 0.012465\n",
      "[322/00377] train_loss: 0.012615\n",
      "[322/00427] train_loss: 0.012072\n",
      "[322/00477] train_loss: 0.012765\n",
      "[322/00527] train_loss: 0.012447\n",
      "[322/00577] train_loss: 0.012841\n",
      "[322/00627] train_loss: 0.013423\n",
      "[322/00677] train_loss: 0.012843\n",
      "[322/00727] train_loss: 0.013472\n",
      "[322/00777] train_loss: 0.012837\n",
      "[322/00827] train_loss: 0.012568\n",
      "[322/00877] train_loss: 0.012788\n",
      "[322/00927] train_loss: 0.012873\n",
      "[322/00977] train_loss: 0.012704\n",
      "[322/01027] train_loss: 0.012447\n",
      "[322/01077] train_loss: 0.011935\n",
      "[322/01127] train_loss: 0.012819\n",
      "[322/01177] train_loss: 0.013283\n",
      "[323/00001] train_loss: 0.012048\n",
      "[323/00051] train_loss: 0.013146\n",
      "[323/00101] train_loss: 0.013222\n",
      "[323/00151] train_loss: 0.013706\n",
      "[323/00201] train_loss: 0.012988\n",
      "[323/00251] train_loss: 0.012712\n",
      "[323/00301] train_loss: 0.013255\n",
      "[323/00351] train_loss: 0.013113\n",
      "[323/00401] train_loss: 0.012135\n",
      "[323/00451] train_loss: 0.012597\n",
      "[323/00501] train_loss: 0.012281\n",
      "[323/00551] train_loss: 0.012184\n",
      "[323/00601] train_loss: 0.012761\n",
      "[323/00651] train_loss: 0.012795\n",
      "[323/00701] train_loss: 0.011814\n",
      "[323/00751] train_loss: 0.012222\n",
      "[323/00801] train_loss: 0.012041\n",
      "[323/00851] train_loss: 0.012676\n",
      "[323/00901] train_loss: 0.012530\n",
      "[323/00951] train_loss: 0.012927\n",
      "[323/01001] train_loss: 0.013300\n",
      "[323/01051] train_loss: 0.012948\n",
      "[323/01101] train_loss: 0.012783\n",
      "[323/01151] train_loss: 0.011940\n",
      "[323/01201] train_loss: 0.012368\n",
      "[324/00025] train_loss: 0.012944\n",
      "[324/00075] train_loss: 0.013847\n",
      "[324/00125] train_loss: 0.012402\n",
      "[324/00175] train_loss: 0.013301\n",
      "[324/00225] train_loss: 0.013395\n",
      "[324/00275] train_loss: 0.013662\n",
      "[324/00325] train_loss: 0.013502\n",
      "[324/00375] train_loss: 0.013028\n",
      "[324/00425] train_loss: 0.013115\n",
      "[324/00475] train_loss: 0.012496\n",
      "[324/00525] train_loss: 0.012363\n",
      "[324/00575] train_loss: 0.013169\n",
      "[324/00625] train_loss: 0.014082\n",
      "[324/00675] train_loss: 0.012281\n",
      "[324/00725] train_loss: 0.013137\n",
      "[324/00775] train_loss: 0.012860\n",
      "[324/00825] train_loss: 0.011737\n",
      "[324/00875] train_loss: 0.012232\n",
      "[324/00925] train_loss: 0.012176\n",
      "[324/00975] train_loss: 0.013071\n",
      "[324/01025] train_loss: 0.012451\n",
      "[324/01075] train_loss: 0.012054\n",
      "[324/01125] train_loss: 0.012162\n",
      "[324/01175] train_loss: 0.012323\n",
      "[324/01225] train_loss: 0.012000\n",
      "[325/00049] train_loss: 0.012642\n",
      "[325/00099] train_loss: 0.013192\n",
      "[325/00149] train_loss: 0.013717\n",
      "[325/00199] train_loss: 0.012769\n",
      "[325/00249] train_loss: 0.012566\n",
      "[325/00299] train_loss: 0.013835\n",
      "[325/00349] train_loss: 0.011942\n",
      "[325/00399] train_loss: 0.012610\n",
      "[325/00449] train_loss: 0.012670\n",
      "[325/00499] train_loss: 0.012143\n",
      "[325/00549] train_loss: 0.012559\n",
      "[325/00599] train_loss: 0.012838\n",
      "[325/00649] train_loss: 0.012307\n",
      "[325/00699] train_loss: 0.011816\n",
      "[325/00749] train_loss: 0.012433\n",
      "[325/00799] train_loss: 0.012714\n",
      "[325/00849] train_loss: 0.012894\n",
      "[325/00899] train_loss: 0.012700\n",
      "[325/00949] train_loss: 0.012649\n",
      "[325/00999] train_loss: 0.012552\n",
      "[325/01049] train_loss: 0.012146\n",
      "[325/01099] train_loss: 0.012195\n",
      "[325/01149] train_loss: 0.012934\n",
      "[325/01199] train_loss: 0.012062\n",
      "[326/00023] train_loss: 0.013444\n",
      "[326/00073] train_loss: 0.013343\n",
      "[326/00123] train_loss: 0.013685\n",
      "[326/00173] train_loss: 0.012448\n",
      "[326/00223] train_loss: 0.013000\n",
      "[326/00273] train_loss: 0.013378\n",
      "[326/00323] train_loss: 0.012929\n",
      "[326/00373] train_loss: 0.013425\n",
      "[326/00423] train_loss: 0.013495\n",
      "[326/00473] train_loss: 0.013025\n",
      "[326/00523] train_loss: 0.013097\n",
      "[326/00573] train_loss: 0.012581\n",
      "[326/00623] train_loss: 0.013514\n",
      "[326/00673] train_loss: 0.012357\n",
      "[326/00723] train_loss: 0.012949\n",
      "[326/00773] train_loss: 0.012350\n",
      "[326/00823] train_loss: 0.012854\n",
      "[326/00873] train_loss: 0.012555\n",
      "[326/00923] train_loss: 0.012308\n",
      "[326/00973] train_loss: 0.012411\n",
      "[326/01023] train_loss: 0.011922\n",
      "[326/01073] train_loss: 0.011746\n",
      "[326/01123] train_loss: 0.012312\n",
      "[326/01173] train_loss: 0.012109\n",
      "[326/01223] train_loss: 0.012284\n",
      "[327/00047] train_loss: 0.013477\n",
      "[327/00097] train_loss: 0.013066\n",
      "[327/00147] train_loss: 0.012774\n",
      "[327/00197] train_loss: 0.013679\n",
      "[327/00247] train_loss: 0.013561\n",
      "[327/00297] train_loss: 0.012509\n",
      "[327/00347] train_loss: 0.012043\n",
      "[327/00397] train_loss: 0.013079\n",
      "[327/00447] train_loss: 0.013040\n",
      "[327/00497] train_loss: 0.012353\n",
      "[327/00547] train_loss: 0.012659\n",
      "[327/00597] train_loss: 0.012084\n",
      "[327/00647] train_loss: 0.012513\n",
      "[327/00697] train_loss: 0.012364\n",
      "[327/00747] train_loss: 0.013279\n",
      "[327/00797] train_loss: 0.013058\n",
      "[327/00847] train_loss: 0.013051\n",
      "[327/00897] train_loss: 0.012707\n",
      "[327/00947] train_loss: 0.012434\n",
      "[327/00997] train_loss: 0.013055\n",
      "[327/01047] train_loss: 0.012512\n",
      "[327/01097] train_loss: 0.012255\n",
      "[327/01147] train_loss: 0.011844\n",
      "[327/01197] train_loss: 0.012335\n",
      "[328/00021] train_loss: 0.013966\n",
      "[328/00071] train_loss: 0.013912\n",
      "[328/00121] train_loss: 0.013144\n",
      "[328/00171] train_loss: 0.012349\n",
      "[328/00221] train_loss: 0.012357\n",
      "[328/00271] train_loss: 0.012490\n",
      "[328/00321] train_loss: 0.012563\n",
      "[328/00371] train_loss: 0.012475\n",
      "[328/00421] train_loss: 0.012704\n",
      "[328/00471] train_loss: 0.012848\n",
      "[328/00521] train_loss: 0.012606\n",
      "[328/00571] train_loss: 0.012616\n",
      "[328/00621] train_loss: 0.012911\n",
      "[328/00671] train_loss: 0.012929\n",
      "[328/00721] train_loss: 0.012082\n",
      "[328/00771] train_loss: 0.012252\n",
      "[328/00821] train_loss: 0.012877\n",
      "[328/00871] train_loss: 0.012731\n",
      "[328/00921] train_loss: 0.011553\n",
      "[328/00971] train_loss: 0.012415\n",
      "[328/01021] train_loss: 0.013167\n",
      "[328/01071] train_loss: 0.012174\n",
      "[328/01121] train_loss: 0.011785\n",
      "[328/01171] train_loss: 0.013284\n",
      "[328/01221] train_loss: 0.012353\n",
      "[329/00045] train_loss: 0.013229\n",
      "[329/00095] train_loss: 0.012770\n",
      "[329/00145] train_loss: 0.013103\n",
      "[329/00195] train_loss: 0.014429\n",
      "[329/00245] train_loss: 0.012673\n",
      "[329/00295] train_loss: 0.012733\n",
      "[329/00345] train_loss: 0.012214\n",
      "[329/00395] train_loss: 0.013182\n",
      "[329/00445] train_loss: 0.013136\n",
      "[329/00495] train_loss: 0.012782\n",
      "[329/00545] train_loss: 0.012857\n",
      "[329/00595] train_loss: 0.012552\n",
      "[329/00645] train_loss: 0.011925\n",
      "[329/00695] train_loss: 0.011574\n",
      "[329/00745] train_loss: 0.012826\n",
      "[329/00795] train_loss: 0.012739\n",
      "[329/00845] train_loss: 0.013723\n",
      "[329/00895] train_loss: 0.012729\n",
      "[329/00945] train_loss: 0.012071\n",
      "[329/00995] train_loss: 0.011655\n",
      "[329/01045] train_loss: 0.012612\n",
      "[329/01095] train_loss: 0.012024\n",
      "[329/01145] train_loss: 0.012015\n",
      "[329/01195] train_loss: 0.013142\n",
      "[330/00019] train_loss: 0.012820\n",
      "[330/00069] train_loss: 0.013742\n",
      "[330/00119] train_loss: 0.013018\n",
      "[330/00169] train_loss: 0.013329\n",
      "[330/00219] train_loss: 0.012702\n",
      "[330/00269] train_loss: 0.013012\n",
      "[330/00319] train_loss: 0.012399\n",
      "[330/00369] train_loss: 0.012909\n",
      "[330/00419] train_loss: 0.012624\n",
      "[330/00469] train_loss: 0.013166\n",
      "[330/00519] train_loss: 0.012719\n",
      "[330/00569] train_loss: 0.012481\n",
      "[330/00619] train_loss: 0.012305\n",
      "[330/00669] train_loss: 0.012953\n",
      "[330/00719] train_loss: 0.012605\n",
      "[330/00769] train_loss: 0.011964\n",
      "[330/00819] train_loss: 0.012079\n",
      "[330/00869] train_loss: 0.013035\n",
      "[330/00919] train_loss: 0.012532\n",
      "[330/00969] train_loss: 0.012587\n",
      "[330/01019] train_loss: 0.012782\n",
      "[330/01069] train_loss: 0.012793\n",
      "[330/01119] train_loss: 0.012422\n",
      "[330/01169] train_loss: 0.011813\n",
      "[330/01219] train_loss: 0.012470\n",
      "[331/00043] train_loss: 0.012811\n",
      "[331/00093] train_loss: 0.012922\n",
      "[331/00143] train_loss: 0.013511\n",
      "[331/00193] train_loss: 0.013695\n",
      "[331/00243] train_loss: 0.012862\n",
      "[331/00293] train_loss: 0.012291\n",
      "[331/00343] train_loss: 0.012986\n",
      "[331/00393] train_loss: 0.012371\n",
      "[331/00443] train_loss: 0.012723\n",
      "[331/00493] train_loss: 0.012682\n",
      "[331/00543] train_loss: 0.012558\n",
      "[331/00593] train_loss: 0.012783\n",
      "[331/00643] train_loss: 0.012060\n",
      "[331/00693] train_loss: 0.012296\n",
      "[331/00743] train_loss: 0.012727\n",
      "[331/00793] train_loss: 0.012090\n",
      "[331/00843] train_loss: 0.012475\n",
      "[331/00893] train_loss: 0.012229\n",
      "[331/00943] train_loss: 0.012404\n",
      "[331/00993] train_loss: 0.012576\n",
      "[331/01043] train_loss: 0.013076\n",
      "[331/01093] train_loss: 0.013517\n",
      "[331/01143] train_loss: 0.012294\n",
      "[331/01193] train_loss: 0.013001\n",
      "[332/00017] train_loss: 0.012355\n",
      "[332/00067] train_loss: 0.013315\n",
      "[332/00117] train_loss: 0.013127\n",
      "[332/00167] train_loss: 0.012403\n",
      "[332/00217] train_loss: 0.012642\n",
      "[332/00267] train_loss: 0.013083\n",
      "[332/00317] train_loss: 0.011970\n",
      "[332/00367] train_loss: 0.012847\n",
      "[332/00417] train_loss: 0.012842\n",
      "[332/00467] train_loss: 0.013021\n",
      "[332/00517] train_loss: 0.012393\n",
      "[332/00567] train_loss: 0.012329\n",
      "[332/00617] train_loss: 0.012131\n",
      "[332/00667] train_loss: 0.013076\n",
      "[332/00717] train_loss: 0.012672\n",
      "[332/00767] train_loss: 0.012823\n",
      "[332/00817] train_loss: 0.013073\n",
      "[332/00867] train_loss: 0.012771\n",
      "[332/00917] train_loss: 0.013759\n",
      "[332/00967] train_loss: 0.012479\n",
      "[332/01017] train_loss: 0.012437\n",
      "[332/01067] train_loss: 0.012429\n",
      "[332/01117] train_loss: 0.012851\n",
      "[332/01167] train_loss: 0.012379\n",
      "[332/01217] train_loss: 0.012211\n",
      "[333/00041] train_loss: 0.012663\n",
      "[333/00091] train_loss: 0.013284\n",
      "[333/00141] train_loss: 0.012590\n",
      "[333/00191] train_loss: 0.011850\n",
      "[333/00241] train_loss: 0.013374\n",
      "[333/00291] train_loss: 0.013189\n",
      "[333/00341] train_loss: 0.013394\n",
      "[333/00391] train_loss: 0.013137\n",
      "[333/00441] train_loss: 0.013351\n",
      "[333/00491] train_loss: 0.012783\n",
      "[333/00541] train_loss: 0.012196\n",
      "[333/00591] train_loss: 0.012724\n",
      "[333/00641] train_loss: 0.012909\n",
      "[333/00691] train_loss: 0.012820\n",
      "[333/00741] train_loss: 0.011339\n",
      "[333/00791] train_loss: 0.012073\n",
      "[333/00841] train_loss: 0.012157\n",
      "[333/00891] train_loss: 0.012920\n",
      "[333/00941] train_loss: 0.012768\n",
      "[333/00991] train_loss: 0.012186\n",
      "[333/01041] train_loss: 0.011982\n",
      "[333/01091] train_loss: 0.012657\n",
      "[333/01141] train_loss: 0.012254\n",
      "[333/01191] train_loss: 0.013292\n",
      "[334/00015] train_loss: 0.013110\n",
      "[334/00065] train_loss: 0.013269\n",
      "[334/00115] train_loss: 0.013617\n",
      "[334/00165] train_loss: 0.013019\n",
      "[334/00215] train_loss: 0.012808\n",
      "[334/00265] train_loss: 0.013118\n",
      "[334/00315] train_loss: 0.012389\n",
      "[334/00365] train_loss: 0.012464\n",
      "[334/00415] train_loss: 0.012848\n",
      "[334/00465] train_loss: 0.012984\n",
      "[334/00515] train_loss: 0.012564\n",
      "[334/00565] train_loss: 0.012088\n",
      "[334/00615] train_loss: 0.013783\n",
      "[334/00665] train_loss: 0.012407\n",
      "[334/00715] train_loss: 0.012151\n",
      "[334/00765] train_loss: 0.013043\n",
      "[334/00815] train_loss: 0.011841\n",
      "[334/00865] train_loss: 0.012961\n",
      "[334/00915] train_loss: 0.013546\n",
      "[334/00965] train_loss: 0.012196\n",
      "[334/01015] train_loss: 0.013065\n",
      "[334/01065] train_loss: 0.012221\n",
      "[334/01115] train_loss: 0.012318\n",
      "[334/01165] train_loss: 0.012148\n",
      "[334/01215] train_loss: 0.012225\n",
      "[335/00039] train_loss: 0.013516\n",
      "[335/00089] train_loss: 0.013964\n",
      "[335/00139] train_loss: 0.012916\n",
      "[335/00189] train_loss: 0.013042\n",
      "[335/00239] train_loss: 0.013087\n",
      "[335/00289] train_loss: 0.012678\n",
      "[335/00339] train_loss: 0.012918\n",
      "[335/00389] train_loss: 0.012399\n",
      "[335/00439] train_loss: 0.012615\n",
      "[335/00489] train_loss: 0.012517\n",
      "[335/00539] train_loss: 0.012927\n",
      "[335/00589] train_loss: 0.012976\n",
      "[335/00639] train_loss: 0.011956\n",
      "[335/00689] train_loss: 0.013301\n",
      "[335/00739] train_loss: 0.012697\n",
      "[335/00789] train_loss: 0.012798\n",
      "[335/00839] train_loss: 0.012990\n",
      "[335/00889] train_loss: 0.011804\n",
      "[335/00939] train_loss: 0.012327\n",
      "[335/00989] train_loss: 0.012379\n",
      "[335/01039] train_loss: 0.012750\n",
      "[335/01089] train_loss: 0.012742\n",
      "[335/01139] train_loss: 0.012283\n",
      "[335/01189] train_loss: 0.012810\n",
      "[336/00013] train_loss: 0.012808\n",
      "[336/00063] train_loss: 0.013551\n",
      "[336/00113] train_loss: 0.012779\n",
      "[336/00163] train_loss: 0.013362\n",
      "[336/00213] train_loss: 0.012799\n",
      "[336/00263] train_loss: 0.012803\n",
      "[336/00313] train_loss: 0.012151\n",
      "[336/00363] train_loss: 0.013461\n",
      "[336/00413] train_loss: 0.013365\n",
      "[336/00463] train_loss: 0.012482\n",
      "[336/00513] train_loss: 0.012574\n",
      "[336/00563] train_loss: 0.012397\n",
      "[336/00613] train_loss: 0.012686\n",
      "[336/00663] train_loss: 0.011965\n",
      "[336/00713] train_loss: 0.012370\n",
      "[336/00763] train_loss: 0.012288\n",
      "[336/00813] train_loss: 0.011799\n",
      "[336/00863] train_loss: 0.012957\n",
      "[336/00913] train_loss: 0.013249\n",
      "[336/00963] train_loss: 0.013273\n",
      "[336/01013] train_loss: 0.012553\n",
      "[336/01063] train_loss: 0.011920\n",
      "[336/01113] train_loss: 0.011881\n",
      "[336/01163] train_loss: 0.012442\n",
      "[336/01213] train_loss: 0.012110\n",
      "[337/00037] train_loss: 0.013112\n",
      "[337/00087] train_loss: 0.012581\n",
      "[337/00137] train_loss: 0.012833\n",
      "[337/00187] train_loss: 0.014341\n",
      "[337/00237] train_loss: 0.012713\n",
      "[337/00287] train_loss: 0.012921\n",
      "[337/00337] train_loss: 0.012640\n",
      "[337/00387] train_loss: 0.013495\n",
      "[337/00437] train_loss: 0.012359\n",
      "[337/00487] train_loss: 0.012767\n",
      "[337/00537] train_loss: 0.013005\n",
      "[337/00587] train_loss: 0.012975\n",
      "[337/00637] train_loss: 0.012648\n",
      "[337/00687] train_loss: 0.011740\n",
      "[337/00737] train_loss: 0.012755\n",
      "[337/00787] train_loss: 0.012517\n",
      "[337/00837] train_loss: 0.012307\n",
      "[337/00887] train_loss: 0.011815\n",
      "[337/00937] train_loss: 0.012641\n",
      "[337/00987] train_loss: 0.012396\n",
      "[337/01037] train_loss: 0.012568\n",
      "[337/01087] train_loss: 0.013215\n",
      "[337/01137] train_loss: 0.012247\n",
      "[337/01187] train_loss: 0.012620\n",
      "[338/00011] train_loss: 0.012195\n",
      "[338/00061] train_loss: 0.013261\n",
      "[338/00111] train_loss: 0.013238\n",
      "[338/00161] train_loss: 0.013055\n",
      "[338/00211] train_loss: 0.013243\n",
      "[338/00261] train_loss: 0.012973\n",
      "[338/00311] train_loss: 0.012791\n",
      "[338/00361] train_loss: 0.011561\n",
      "[338/00411] train_loss: 0.012708\n",
      "[338/00461] train_loss: 0.013054\n",
      "[338/00511] train_loss: 0.012140\n",
      "[338/00561] train_loss: 0.012550\n",
      "[338/00611] train_loss: 0.012424\n",
      "[338/00661] train_loss: 0.012399\n",
      "[338/00711] train_loss: 0.012523\n",
      "[338/00761] train_loss: 0.012886\n",
      "[338/00811] train_loss: 0.012960\n",
      "[338/00861] train_loss: 0.012005\n",
      "[338/00911] train_loss: 0.012305\n",
      "[338/00961] train_loss: 0.013095\n",
      "[338/01011] train_loss: 0.012238\n",
      "[338/01061] train_loss: 0.012849\n",
      "[338/01111] train_loss: 0.012279\n",
      "[338/01161] train_loss: 0.012128\n",
      "[338/01211] train_loss: 0.012639\n",
      "[339/00035] train_loss: 0.013224\n",
      "[339/00085] train_loss: 0.013298\n",
      "[339/00135] train_loss: 0.012040\n",
      "[339/00185] train_loss: 0.012710\n",
      "[339/00235] train_loss: 0.012324\n",
      "[339/00285] train_loss: 0.012733\n",
      "[339/00335] train_loss: 0.013214\n",
      "[339/00385] train_loss: 0.012775\n",
      "[339/00435] train_loss: 0.012848\n",
      "[339/00485] train_loss: 0.013431\n",
      "[339/00535] train_loss: 0.012095\n",
      "[339/00585] train_loss: 0.012954\n",
      "[339/00635] train_loss: 0.012481\n",
      "[339/00685] train_loss: 0.012916\n",
      "[339/00735] train_loss: 0.012452\n",
      "[339/00785] train_loss: 0.012415\n",
      "[339/00835] train_loss: 0.012521\n",
      "[339/00885] train_loss: 0.012362\n",
      "[339/00935] train_loss: 0.012808\n",
      "[339/00985] train_loss: 0.012912\n",
      "[339/01035] train_loss: 0.012653\n",
      "[339/01085] train_loss: 0.012399\n",
      "[339/01135] train_loss: 0.012572\n",
      "[339/01185] train_loss: 0.011947\n",
      "[340/00009] train_loss: 0.011792\n",
      "[340/00059] train_loss: 0.013920\n",
      "[340/00109] train_loss: 0.013616\n",
      "[340/00159] train_loss: 0.013117\n",
      "[340/00209] train_loss: 0.012252\n",
      "[340/00259] train_loss: 0.012705\n",
      "[340/00309] train_loss: 0.012728\n",
      "[340/00359] train_loss: 0.013046\n",
      "[340/00409] train_loss: 0.012663\n",
      "[340/00459] train_loss: 0.012757\n",
      "[340/00509] train_loss: 0.012712\n",
      "[340/00559] train_loss: 0.013436\n",
      "[340/00609] train_loss: 0.013211\n",
      "[340/00659] train_loss: 0.012499\n",
      "[340/00709] train_loss: 0.011564\n",
      "[340/00759] train_loss: 0.012893\n",
      "[340/00809] train_loss: 0.012480\n",
      "[340/00859] train_loss: 0.012822\n",
      "[340/00909] train_loss: 0.012330\n",
      "[340/00959] train_loss: 0.012080\n",
      "[340/01009] train_loss: 0.012044\n",
      "[340/01059] train_loss: 0.011876\n",
      "[340/01109] train_loss: 0.011934\n",
      "[340/01159] train_loss: 0.011815\n",
      "[340/01209] train_loss: 0.011912\n",
      "[341/00033] train_loss: 0.013520\n",
      "[341/00083] train_loss: 0.013355\n",
      "[341/00133] train_loss: 0.013442\n",
      "[341/00183] train_loss: 0.012375\n",
      "[341/00233] train_loss: 0.012611\n",
      "[341/00283] train_loss: 0.012190\n",
      "[341/00333] train_loss: 0.012462\n",
      "[341/00383] train_loss: 0.012935\n",
      "[341/00433] train_loss: 0.012602\n",
      "[341/00483] train_loss: 0.012385\n",
      "[341/00533] train_loss: 0.013058\n",
      "[341/00583] train_loss: 0.012586\n",
      "[341/00633] train_loss: 0.012366\n",
      "[341/00683] train_loss: 0.012399\n",
      "[341/00733] train_loss: 0.012694\n",
      "[341/00783] train_loss: 0.012576\n",
      "[341/00833] train_loss: 0.012522\n",
      "[341/00883] train_loss: 0.012373\n",
      "[341/00933] train_loss: 0.012611\n",
      "[341/00983] train_loss: 0.012087\n",
      "[341/01033] train_loss: 0.012952\n",
      "[341/01083] train_loss: 0.012341\n",
      "[341/01133] train_loss: 0.013014\n",
      "[341/01183] train_loss: 0.012742\n",
      "[342/00007] train_loss: 0.012543\n",
      "[342/00057] train_loss: 0.014052\n",
      "[342/00107] train_loss: 0.012562\n",
      "[342/00157] train_loss: 0.012689\n",
      "[342/00207] train_loss: 0.013765\n",
      "[342/00257] train_loss: 0.013282\n",
      "[342/00307] train_loss: 0.013369\n",
      "[342/00357] train_loss: 0.013506\n",
      "[342/00407] train_loss: 0.013066\n",
      "[342/00457] train_loss: 0.012904\n",
      "[342/00507] train_loss: 0.012827\n",
      "[342/00557] train_loss: 0.012886\n",
      "[342/00607] train_loss: 0.012232\n",
      "[342/00657] train_loss: 0.012320\n",
      "[342/00707] train_loss: 0.012541\n",
      "[342/00757] train_loss: 0.012397\n",
      "[342/00807] train_loss: 0.012448\n",
      "[342/00857] train_loss: 0.012660\n",
      "[342/00907] train_loss: 0.012402\n",
      "[342/00957] train_loss: 0.012917\n",
      "[342/01007] train_loss: 0.011978\n",
      "[342/01057] train_loss: 0.011928\n",
      "[342/01107] train_loss: 0.011744\n",
      "[342/01157] train_loss: 0.012251\n",
      "[342/01207] train_loss: 0.012084\n",
      "[343/00031] train_loss: 0.013348\n",
      "[343/00081] train_loss: 0.012856\n",
      "[343/00131] train_loss: 0.012553\n",
      "[343/00181] train_loss: 0.012998\n",
      "[343/00231] train_loss: 0.012879\n",
      "[343/00281] train_loss: 0.013696\n",
      "[343/00331] train_loss: 0.013061\n",
      "[343/00381] train_loss: 0.011773\n",
      "[343/00431] train_loss: 0.012197\n",
      "[343/00481] train_loss: 0.012476\n",
      "[343/00531] train_loss: 0.012835\n",
      "[343/00581] train_loss: 0.012672\n",
      "[343/00631] train_loss: 0.012266\n",
      "[343/00681] train_loss: 0.013083\n",
      "[343/00731] train_loss: 0.012705\n",
      "[343/00781] train_loss: 0.012481\n",
      "[343/00831] train_loss: 0.012853\n",
      "[343/00881] train_loss: 0.012991\n",
      "[343/00931] train_loss: 0.011900\n",
      "[343/00981] train_loss: 0.012139\n",
      "[343/01031] train_loss: 0.012181\n",
      "[343/01081] train_loss: 0.012922\n",
      "[343/01131] train_loss: 0.012343\n",
      "[343/01181] train_loss: 0.012588\n",
      "[344/00005] train_loss: 0.012849\n",
      "[344/00055] train_loss: 0.013649\n",
      "[344/00105] train_loss: 0.012882\n",
      "[344/00155] train_loss: 0.013565\n",
      "[344/00205] train_loss: 0.013173\n",
      "[344/00255] train_loss: 0.013108\n",
      "[344/00305] train_loss: 0.012903\n",
      "[344/00355] train_loss: 0.013026\n",
      "[344/00405] train_loss: 0.012341\n",
      "[344/00455] train_loss: 0.013010\n",
      "[344/00505] train_loss: 0.013055\n",
      "[344/00555] train_loss: 0.012509\n",
      "[344/00605] train_loss: 0.012256\n",
      "[344/00655] train_loss: 0.012372\n",
      "[344/00705] train_loss: 0.011864\n",
      "[344/00755] train_loss: 0.012485\n",
      "[344/00805] train_loss: 0.012230\n",
      "[344/00855] train_loss: 0.012427\n",
      "[344/00905] train_loss: 0.012274\n",
      "[344/00955] train_loss: 0.012955\n",
      "[344/01005] train_loss: 0.012399\n",
      "[344/01055] train_loss: 0.012385\n",
      "[344/01105] train_loss: 0.011867\n",
      "[344/01155] train_loss: 0.012973\n",
      "[344/01205] train_loss: 0.012476\n",
      "[345/00029] train_loss: 0.012141\n",
      "[345/00079] train_loss: 0.013184\n",
      "[345/00129] train_loss: 0.013334\n",
      "[345/00179] train_loss: 0.012930\n",
      "[345/00229] train_loss: 0.011943\n",
      "[345/00279] train_loss: 0.012757\n",
      "[345/00329] train_loss: 0.012541\n",
      "[345/00379] train_loss: 0.012699\n",
      "[345/00429] train_loss: 0.013922\n",
      "[345/00479] train_loss: 0.012732\n",
      "[345/00529] train_loss: 0.013362\n",
      "[345/00579] train_loss: 0.012725\n",
      "[345/00629] train_loss: 0.013296\n",
      "[345/00679] train_loss: 0.012249\n",
      "[345/00729] train_loss: 0.013470\n",
      "[345/00779] train_loss: 0.013140\n",
      "[345/00829] train_loss: 0.012623\n",
      "[345/00879] train_loss: 0.011683\n",
      "[345/00929] train_loss: 0.012938\n",
      "[345/00979] train_loss: 0.012838\n",
      "[345/01029] train_loss: 0.012575\n",
      "[345/01079] train_loss: 0.012202\n",
      "[345/01129] train_loss: 0.011752\n",
      "[345/01179] train_loss: 0.012065\n",
      "[346/00003] train_loss: 0.011323\n",
      "[346/00053] train_loss: 0.013538\n",
      "[346/00103] train_loss: 0.013778\n",
      "[346/00153] train_loss: 0.013330\n",
      "[346/00203] train_loss: 0.012915\n",
      "[346/00253] train_loss: 0.012450\n",
      "[346/00303] train_loss: 0.012199\n",
      "[346/00353] train_loss: 0.012580\n",
      "[346/00403] train_loss: 0.012409\n",
      "[346/00453] train_loss: 0.011953\n",
      "[346/00503] train_loss: 0.012222\n",
      "[346/00553] train_loss: 0.013149\n",
      "[346/00603] train_loss: 0.012786\n",
      "[346/00653] train_loss: 0.012546\n",
      "[346/00703] train_loss: 0.012533\n",
      "[346/00753] train_loss: 0.012027\n",
      "[346/00803] train_loss: 0.012354\n",
      "[346/00853] train_loss: 0.012647\n",
      "[346/00903] train_loss: 0.012560\n",
      "[346/00953] train_loss: 0.012925\n",
      "[346/01003] train_loss: 0.013113\n",
      "[346/01053] train_loss: 0.012199\n",
      "[346/01103] train_loss: 0.012426\n",
      "[346/01153] train_loss: 0.011767\n",
      "[346/01203] train_loss: 0.012674\n",
      "[347/00027] train_loss: 0.012785\n",
      "[347/00077] train_loss: 0.013595\n",
      "[347/00127] train_loss: 0.012739\n",
      "[347/00177] train_loss: 0.012735\n",
      "[347/00227] train_loss: 0.012943\n",
      "[347/00277] train_loss: 0.012776\n",
      "[347/00327] train_loss: 0.013047\n",
      "[347/00377] train_loss: 0.012361\n",
      "[347/00427] train_loss: 0.013236\n",
      "[347/00477] train_loss: 0.013163\n",
      "[347/00527] train_loss: 0.013008\n",
      "[347/00577] train_loss: 0.011910\n",
      "[347/00627] train_loss: 0.012103\n",
      "[347/00677] train_loss: 0.012097\n",
      "[347/00727] train_loss: 0.012754\n",
      "[347/00777] train_loss: 0.014052\n",
      "[347/00827] train_loss: 0.012784\n",
      "[347/00877] train_loss: 0.012108\n",
      "[347/00927] train_loss: 0.012739\n",
      "[347/00977] train_loss: 0.012131\n",
      "[347/01027] train_loss: 0.012737\n",
      "[347/01077] train_loss: 0.011854\n",
      "[347/01127] train_loss: 0.012432\n",
      "[347/01177] train_loss: 0.012581\n",
      "[348/00001] train_loss: 0.012822\n",
      "[348/00051] train_loss: 0.013201\n",
      "[348/00101] train_loss: 0.012347\n",
      "[348/00151] train_loss: 0.013308\n",
      "[348/00201] train_loss: 0.013590\n",
      "[348/00251] train_loss: 0.012803\n",
      "[348/00301] train_loss: 0.012707\n",
      "[348/00351] train_loss: 0.012471\n",
      "[348/00401] train_loss: 0.013439\n",
      "[348/00451] train_loss: 0.012394\n",
      "[348/00501] train_loss: 0.012625\n",
      "[348/00551] train_loss: 0.011932\n",
      "[348/00601] train_loss: 0.012163\n",
      "[348/00651] train_loss: 0.012840\n",
      "[348/00701] train_loss: 0.012349\n",
      "[348/00751] train_loss: 0.012432\n",
      "[348/00801] train_loss: 0.013110\n",
      "[348/00851] train_loss: 0.012380\n",
      "[348/00901] train_loss: 0.012283\n",
      "[348/00951] train_loss: 0.012531\n",
      "[348/01001] train_loss: 0.012107\n",
      "[348/01051] train_loss: 0.012834\n",
      "[348/01101] train_loss: 0.012126\n",
      "[348/01151] train_loss: 0.012802\n",
      "[348/01201] train_loss: 0.011947\n",
      "[349/00025] train_loss: 0.012076\n",
      "[349/00075] train_loss: 0.012817\n",
      "[349/00125] train_loss: 0.013187\n",
      "[349/00175] train_loss: 0.012767\n",
      "[349/00225] train_loss: 0.013198\n",
      "[349/00275] train_loss: 0.012661\n",
      "[349/00325] train_loss: 0.012905\n",
      "[349/00375] train_loss: 0.012099\n",
      "[349/00425] train_loss: 0.012404\n",
      "[349/00475] train_loss: 0.012778\n",
      "[349/00525] train_loss: 0.012942\n",
      "[349/00575] train_loss: 0.012890\n",
      "[349/00625] train_loss: 0.012924\n",
      "[349/00675] train_loss: 0.012422\n",
      "[349/00725] train_loss: 0.012326\n",
      "[349/00775] train_loss: 0.013078\n",
      "[349/00825] train_loss: 0.011818\n",
      "[349/00875] train_loss: 0.011814\n",
      "[349/00925] train_loss: 0.012954\n",
      "[349/00975] train_loss: 0.012067\n",
      "[349/01025] train_loss: 0.012864\n",
      "[349/01075] train_loss: 0.012381\n",
      "[349/01125] train_loss: 0.012660\n",
      "[349/01175] train_loss: 0.012136\n",
      "[349/01225] train_loss: 0.012710\n",
      "[350/00049] train_loss: 0.013368\n",
      "[350/00099] train_loss: 0.012677\n",
      "[350/00149] train_loss: 0.013774\n",
      "[350/00199] train_loss: 0.013114\n",
      "[350/00249] train_loss: 0.012813\n",
      "[350/00299] train_loss: 0.012203\n",
      "[350/00349] train_loss: 0.013019\n",
      "[350/00399] train_loss: 0.012402\n",
      "[350/00449] train_loss: 0.013797\n",
      "[350/00499] train_loss: 0.012949\n",
      "[350/00549] train_loss: 0.012137\n",
      "[350/00599] train_loss: 0.012233\n",
      "[350/00649] train_loss: 0.011961\n",
      "[350/00699] train_loss: 0.012289\n",
      "[350/00749] train_loss: 0.012861\n",
      "[350/00799] train_loss: 0.012331\n",
      "[350/00849] train_loss: 0.011910\n",
      "[350/00899] train_loss: 0.013629\n",
      "[350/00949] train_loss: 0.012139\n",
      "[350/00999] train_loss: 0.013159\n",
      "[350/01049] train_loss: 0.011983\n",
      "[350/01099] train_loss: 0.013013\n",
      "[350/01149] train_loss: 0.012285\n",
      "[350/01199] train_loss: 0.012651\n",
      "[351/00023] train_loss: 0.012814\n",
      "[351/00073] train_loss: 0.013315\n",
      "[351/00123] train_loss: 0.013612\n",
      "[351/00173] train_loss: 0.012833\n",
      "[351/00223] train_loss: 0.013449\n",
      "[351/00273] train_loss: 0.012557\n",
      "[351/00323] train_loss: 0.013349\n",
      "[351/00373] train_loss: 0.013015\n",
      "[351/00423] train_loss: 0.012238\n",
      "[351/00473] train_loss: 0.012647\n",
      "[351/00523] train_loss: 0.012073\n",
      "[351/00573] train_loss: 0.012361\n",
      "[351/00623] train_loss: 0.012605\n",
      "[351/00673] train_loss: 0.012078\n",
      "[351/00723] train_loss: 0.012555\n",
      "[351/00773] train_loss: 0.012086\n",
      "[351/00823] train_loss: 0.011475\n",
      "[351/00873] train_loss: 0.012317\n",
      "[351/00923] train_loss: 0.012572\n",
      "[351/00973] train_loss: 0.012275\n",
      "[351/01023] train_loss: 0.013269\n",
      "[351/01073] train_loss: 0.012486\n",
      "[351/01123] train_loss: 0.012301\n",
      "[351/01173] train_loss: 0.013366\n",
      "[351/01223] train_loss: 0.012775\n",
      "[352/00047] train_loss: 0.013379\n",
      "[352/00097] train_loss: 0.012331\n",
      "[352/00147] train_loss: 0.013074\n",
      "[352/00197] train_loss: 0.012816\n",
      "[352/00247] train_loss: 0.013179\n",
      "[352/00297] train_loss: 0.012640\n",
      "[352/00347] train_loss: 0.012482\n",
      "[352/00397] train_loss: 0.012779\n",
      "[352/00447] train_loss: 0.013282\n",
      "[352/00497] train_loss: 0.012301\n",
      "[352/00547] train_loss: 0.012403\n",
      "[352/00597] train_loss: 0.012756\n",
      "[352/00647] train_loss: 0.012862\n",
      "[352/00697] train_loss: 0.012022\n",
      "[352/00747] train_loss: 0.012445\n",
      "[352/00797] train_loss: 0.012979\n",
      "[352/00847] train_loss: 0.012522\n",
      "[352/00897] train_loss: 0.012751\n",
      "[352/00947] train_loss: 0.012547\n",
      "[352/00997] train_loss: 0.011515\n",
      "[352/01047] train_loss: 0.012901\n",
      "[352/01097] train_loss: 0.012467\n",
      "[352/01147] train_loss: 0.012589\n",
      "[352/01197] train_loss: 0.012086\n",
      "[353/00021] train_loss: 0.013067\n",
      "[353/00071] train_loss: 0.013339\n",
      "[353/00121] train_loss: 0.012592\n",
      "[353/00171] train_loss: 0.013733\n",
      "[353/00221] train_loss: 0.013103\n",
      "[353/00271] train_loss: 0.012516\n",
      "[353/00321] train_loss: 0.012748\n",
      "[353/00371] train_loss: 0.012807\n",
      "[353/00421] train_loss: 0.013140\n",
      "[353/00471] train_loss: 0.012462\n",
      "[353/00521] train_loss: 0.011906\n",
      "[353/00571] train_loss: 0.012053\n",
      "[353/00621] train_loss: 0.012760\n",
      "[353/00671] train_loss: 0.012064\n",
      "[353/00721] train_loss: 0.012823\n",
      "[353/00771] train_loss: 0.012506\n",
      "[353/00821] train_loss: 0.013141\n",
      "[353/00871] train_loss: 0.012133\n",
      "[353/00921] train_loss: 0.012161\n",
      "[353/00971] train_loss: 0.011910\n",
      "[353/01021] train_loss: 0.011720\n",
      "[353/01071] train_loss: 0.012214\n",
      "[353/01121] train_loss: 0.012520\n",
      "[353/01171] train_loss: 0.012284\n",
      "[353/01221] train_loss: 0.011781\n",
      "[354/00045] train_loss: 0.013082\n",
      "[354/00095] train_loss: 0.013099\n",
      "[354/00145] train_loss: 0.012163\n",
      "[354/00195] train_loss: 0.012768\n",
      "[354/00245] train_loss: 0.012870\n",
      "[354/00295] train_loss: 0.013094\n",
      "[354/00345] train_loss: 0.012531\n",
      "[354/00395] train_loss: 0.012918\n",
      "[354/00445] train_loss: 0.012282\n",
      "[354/00495] train_loss: 0.012741\n",
      "[354/00545] train_loss: 0.013245\n",
      "[354/00595] train_loss: 0.013009\n",
      "[354/00645] train_loss: 0.012633\n",
      "[354/00695] train_loss: 0.012482\n",
      "[354/00745] train_loss: 0.012805\n",
      "[354/00795] train_loss: 0.012442\n",
      "[354/00845] train_loss: 0.012049\n",
      "[354/00895] train_loss: 0.011668\n",
      "[354/00945] train_loss: 0.011842\n",
      "[354/00995] train_loss: 0.013353\n",
      "[354/01045] train_loss: 0.011944\n",
      "[354/01095] train_loss: 0.012706\n",
      "[354/01145] train_loss: 0.012738\n",
      "[354/01195] train_loss: 0.012345\n",
      "[355/00019] train_loss: 0.012394\n",
      "[355/00069] train_loss: 0.013510\n",
      "[355/00119] train_loss: 0.013776\n",
      "[355/00169] train_loss: 0.012929\n",
      "[355/00219] train_loss: 0.012115\n",
      "[355/00269] train_loss: 0.012363\n",
      "[355/00319] train_loss: 0.013469\n",
      "[355/00369] train_loss: 0.012713\n",
      "[355/00419] train_loss: 0.012804\n",
      "[355/00469] train_loss: 0.011845\n",
      "[355/00519] train_loss: 0.011645\n",
      "[355/00569] train_loss: 0.011698\n",
      "[355/00619] train_loss: 0.012379\n",
      "[355/00669] train_loss: 0.012616\n",
      "[355/00719] train_loss: 0.012439\n",
      "[355/00769] train_loss: 0.012660\n",
      "[355/00819] train_loss: 0.012533\n",
      "[355/00869] train_loss: 0.012633\n",
      "[355/00919] train_loss: 0.012365\n",
      "[355/00969] train_loss: 0.012665\n",
      "[355/01019] train_loss: 0.012401\n",
      "[355/01069] train_loss: 0.012489\n",
      "[355/01119] train_loss: 0.012166\n",
      "[355/01169] train_loss: 0.011940\n",
      "[355/01219] train_loss: 0.012479\n",
      "[356/00043] train_loss: 0.013051\n",
      "[356/00093] train_loss: 0.012727\n",
      "[356/00143] train_loss: 0.012327\n",
      "[356/00193] train_loss: 0.013310\n",
      "[356/00243] train_loss: 0.012692\n",
      "[356/00293] train_loss: 0.013064\n",
      "[356/00343] train_loss: 0.012212\n",
      "[356/00393] train_loss: 0.012081\n",
      "[356/00443] train_loss: 0.013377\n",
      "[356/00493] train_loss: 0.012177\n",
      "[356/00543] train_loss: 0.012381\n",
      "[356/00593] train_loss: 0.012255\n",
      "[356/00643] train_loss: 0.012600\n",
      "[356/00693] train_loss: 0.013103\n",
      "[356/00743] train_loss: 0.012805\n",
      "[356/00793] train_loss: 0.012430\n",
      "[356/00843] train_loss: 0.012637\n",
      "[356/00893] train_loss: 0.012415\n",
      "[356/00943] train_loss: 0.012314\n",
      "[356/00993] train_loss: 0.012600\n",
      "[356/01043] train_loss: 0.012299\n",
      "[356/01093] train_loss: 0.012795\n",
      "[356/01143] train_loss: 0.012773\n",
      "[356/01193] train_loss: 0.012073\n",
      "[357/00017] train_loss: 0.012941\n",
      "[357/00067] train_loss: 0.013151\n",
      "[357/00117] train_loss: 0.013418\n",
      "[357/00167] train_loss: 0.013083\n",
      "[357/00217] train_loss: 0.012900\n",
      "[357/00267] train_loss: 0.012420\n",
      "[357/00317] train_loss: 0.012667\n",
      "[357/00367] train_loss: 0.012542\n",
      "[357/00417] train_loss: 0.012966\n",
      "[357/00467] train_loss: 0.013096\n",
      "[357/00517] train_loss: 0.012717\n",
      "[357/00567] train_loss: 0.012812\n",
      "[357/00617] train_loss: 0.012679\n",
      "[357/00667] train_loss: 0.012306\n",
      "[357/00717] train_loss: 0.012708\n",
      "[357/00767] train_loss: 0.013039\n",
      "[357/00817] train_loss: 0.012262\n",
      "[357/00867] train_loss: 0.012234\n",
      "[357/00917] train_loss: 0.012204\n",
      "[357/00967] train_loss: 0.012571\n",
      "[357/01017] train_loss: 0.012299\n",
      "[357/01067] train_loss: 0.012494\n",
      "[357/01117] train_loss: 0.011570\n",
      "[357/01167] train_loss: 0.012444\n",
      "[357/01217] train_loss: 0.012714\n",
      "[358/00041] train_loss: 0.013109\n",
      "[358/00091] train_loss: 0.012709\n",
      "[358/00141] train_loss: 0.012469\n",
      "[358/00191] train_loss: 0.012100\n",
      "[358/00241] train_loss: 0.012268\n",
      "[358/00291] train_loss: 0.012638\n",
      "[358/00341] train_loss: 0.012229\n",
      "[358/00391] train_loss: 0.012879\n",
      "[358/00441] train_loss: 0.011749\n",
      "[358/00491] train_loss: 0.012522\n",
      "[358/00541] train_loss: 0.012851\n",
      "[358/00591] train_loss: 0.012481\n",
      "[358/00641] train_loss: 0.012856\n",
      "[358/00691] train_loss: 0.012505\n",
      "[358/00741] train_loss: 0.012790\n",
      "[358/00791] train_loss: 0.013265\n",
      "[358/00841] train_loss: 0.012193\n",
      "[358/00891] train_loss: 0.012993\n",
      "[358/00941] train_loss: 0.012061\n",
      "[358/00991] train_loss: 0.012166\n",
      "[358/01041] train_loss: 0.012461\n",
      "[358/01091] train_loss: 0.012734\n",
      "[358/01141] train_loss: 0.013534\n",
      "[358/01191] train_loss: 0.012456\n",
      "[359/00015] train_loss: 0.012497\n",
      "[359/00065] train_loss: 0.013345\n",
      "[359/00115] train_loss: 0.012619\n",
      "[359/00165] train_loss: 0.013048\n",
      "[359/00215] train_loss: 0.012475\n",
      "[359/00265] train_loss: 0.013458\n",
      "[359/00315] train_loss: 0.012192\n",
      "[359/00365] train_loss: 0.012477\n",
      "[359/00415] train_loss: 0.012615\n",
      "[359/00465] train_loss: 0.012149\n",
      "[359/00515] train_loss: 0.012527\n",
      "[359/00565] train_loss: 0.012503\n",
      "[359/00615] train_loss: 0.012852\n",
      "[359/00665] train_loss: 0.012698\n",
      "[359/00715] train_loss: 0.012229\n",
      "[359/00765] train_loss: 0.012566\n",
      "[359/00815] train_loss: 0.012580\n",
      "[359/00865] train_loss: 0.011960\n",
      "[359/00915] train_loss: 0.012517\n",
      "[359/00965] train_loss: 0.012318\n",
      "[359/01015] train_loss: 0.012300\n",
      "[359/01065] train_loss: 0.012192\n",
      "[359/01115] train_loss: 0.011781\n",
      "[359/01165] train_loss: 0.012197\n",
      "[359/01215] train_loss: 0.012197\n",
      "[360/00039] train_loss: 0.012792\n",
      "[360/00089] train_loss: 0.012815\n",
      "[360/00139] train_loss: 0.012755\n",
      "[360/00189] train_loss: 0.013100\n",
      "[360/00239] train_loss: 0.012734\n",
      "[360/00289] train_loss: 0.012648\n",
      "[360/00339] train_loss: 0.012664\n",
      "[360/00389] train_loss: 0.012925\n",
      "[360/00439] train_loss: 0.012892\n",
      "[360/00489] train_loss: 0.012340\n",
      "[360/00539] train_loss: 0.012918\n",
      "[360/00589] train_loss: 0.012871\n",
      "[360/00639] train_loss: 0.012544\n",
      "[360/00689] train_loss: 0.012658\n",
      "[360/00739] train_loss: 0.012528\n",
      "[360/00789] train_loss: 0.012102\n",
      "[360/00839] train_loss: 0.012555\n",
      "[360/00889] train_loss: 0.012176\n",
      "[360/00939] train_loss: 0.013010\n",
      "[360/00989] train_loss: 0.011722\n",
      "[360/01039] train_loss: 0.012272\n",
      "[360/01089] train_loss: 0.012357\n",
      "[360/01139] train_loss: 0.012289\n",
      "[360/01189] train_loss: 0.012690\n",
      "[361/00013] train_loss: 0.012081\n",
      "[361/00063] train_loss: 0.013209\n",
      "[361/00113] train_loss: 0.012931\n",
      "[361/00163] train_loss: 0.012577\n",
      "[361/00213] train_loss: 0.012049\n",
      "[361/00263] train_loss: 0.012485\n",
      "[361/00313] train_loss: 0.012055\n",
      "[361/00363] train_loss: 0.013120\n",
      "[361/00413] train_loss: 0.012641\n",
      "[361/00463] train_loss: 0.012523\n",
      "[361/00513] train_loss: 0.012595\n",
      "[361/00563] train_loss: 0.012474\n",
      "[361/00613] train_loss: 0.012295\n",
      "[361/00663] train_loss: 0.012194\n",
      "[361/00713] train_loss: 0.013173\n",
      "[361/00763] train_loss: 0.012569\n",
      "[361/00813] train_loss: 0.012634\n",
      "[361/00863] train_loss: 0.013367\n",
      "[361/00913] train_loss: 0.011797\n",
      "[361/00963] train_loss: 0.012444\n",
      "[361/01013] train_loss: 0.012785\n",
      "[361/01063] train_loss: 0.012248\n",
      "[361/01113] train_loss: 0.011739\n",
      "[361/01163] train_loss: 0.012841\n",
      "[361/01213] train_loss: 0.012186\n",
      "[362/00037] train_loss: 0.013526\n",
      "[362/00087] train_loss: 0.013265\n",
      "[362/00137] train_loss: 0.012610\n",
      "[362/00187] train_loss: 0.013257\n",
      "[362/00237] train_loss: 0.013338\n",
      "[362/00287] train_loss: 0.013472\n",
      "[362/00337] train_loss: 0.013193\n",
      "[362/00387] train_loss: 0.012342\n",
      "[362/00437] train_loss: 0.012220\n",
      "[362/00487] train_loss: 0.013217\n",
      "[362/00537] train_loss: 0.012545\n",
      "[362/00587] train_loss: 0.012155\n",
      "[362/00637] train_loss: 0.012342\n",
      "[362/00687] train_loss: 0.012082\n",
      "[362/00737] train_loss: 0.012321\n",
      "[362/00787] train_loss: 0.013178\n",
      "[362/00837] train_loss: 0.012559\n",
      "[362/00887] train_loss: 0.011619\n",
      "[362/00937] train_loss: 0.012964\n",
      "[362/00987] train_loss: 0.011962\n",
      "[362/01037] train_loss: 0.012105\n",
      "[362/01087] train_loss: 0.012114\n",
      "[362/01137] train_loss: 0.012510\n",
      "[362/01187] train_loss: 0.011572\n",
      "[363/00011] train_loss: 0.012127\n",
      "[363/00061] train_loss: 0.013516\n",
      "[363/00111] train_loss: 0.013250\n",
      "[363/00161] train_loss: 0.012209\n",
      "[363/00211] train_loss: 0.013196\n",
      "[363/00261] train_loss: 0.012317\n",
      "[363/00311] train_loss: 0.012077\n",
      "[363/00361] train_loss: 0.012576\n",
      "[363/00411] train_loss: 0.012364\n",
      "[363/00461] train_loss: 0.013009\n",
      "[363/00511] train_loss: 0.012495\n",
      "[363/00561] train_loss: 0.011885\n",
      "[363/00611] train_loss: 0.013182\n",
      "[363/00661] train_loss: 0.012054\n",
      "[363/00711] train_loss: 0.013306\n",
      "[363/00761] train_loss: 0.012556\n",
      "[363/00811] train_loss: 0.012411\n",
      "[363/00861] train_loss: 0.012714\n",
      "[363/00911] train_loss: 0.011959\n",
      "[363/00961] train_loss: 0.012141\n",
      "[363/01011] train_loss: 0.012946\n",
      "[363/01061] train_loss: 0.012034\n",
      "[363/01111] train_loss: 0.012415\n",
      "[363/01161] train_loss: 0.012652\n",
      "[363/01211] train_loss: 0.013115\n",
      "[364/00035] train_loss: 0.012704\n",
      "[364/00085] train_loss: 0.013286\n",
      "[364/00135] train_loss: 0.013040\n",
      "[364/00185] train_loss: 0.012548\n",
      "[364/00235] train_loss: 0.011969\n",
      "[364/00285] train_loss: 0.013203\n",
      "[364/00335] train_loss: 0.012813\n",
      "[364/00385] train_loss: 0.012680\n",
      "[364/00435] train_loss: 0.012337\n",
      "[364/00485] train_loss: 0.013488\n",
      "[364/00535] train_loss: 0.012513\n",
      "[364/00585] train_loss: 0.012287\n",
      "[364/00635] train_loss: 0.012810\n",
      "[364/00685] train_loss: 0.012734\n",
      "[364/00735] train_loss: 0.012763\n",
      "[364/00785] train_loss: 0.012974\n",
      "[364/00835] train_loss: 0.012155\n",
      "[364/00885] train_loss: 0.012113\n",
      "[364/00935] train_loss: 0.011835\n",
      "[364/00985] train_loss: 0.013030\n",
      "[364/01035] train_loss: 0.012138\n",
      "[364/01085] train_loss: 0.012061\n",
      "[364/01135] train_loss: 0.012105\n",
      "[364/01185] train_loss: 0.012046\n",
      "[365/00009] train_loss: 0.012607\n",
      "[365/00059] train_loss: 0.013555\n",
      "[365/00109] train_loss: 0.012944\n",
      "[365/00159] train_loss: 0.013088\n",
      "[365/00209] train_loss: 0.013067\n",
      "[365/00259] train_loss: 0.013355\n",
      "[365/00309] train_loss: 0.012393\n",
      "[365/00359] train_loss: 0.012372\n",
      "[365/00409] train_loss: 0.012156\n",
      "[365/00459] train_loss: 0.012744\n",
      "[365/00509] train_loss: 0.011920\n",
      "[365/00559] train_loss: 0.012808\n",
      "[365/00609] train_loss: 0.012989\n",
      "[365/00659] train_loss: 0.012155\n",
      "[365/00709] train_loss: 0.012898\n",
      "[365/00759] train_loss: 0.012871\n",
      "[365/00809] train_loss: 0.011748\n",
      "[365/00859] train_loss: 0.012657\n",
      "[365/00909] train_loss: 0.012196\n",
      "[365/00959] train_loss: 0.012394\n",
      "[365/01009] train_loss: 0.012518\n",
      "[365/01059] train_loss: 0.012276\n",
      "[365/01109] train_loss: 0.012330\n",
      "[365/01159] train_loss: 0.012836\n",
      "[365/01209] train_loss: 0.012484\n",
      "[366/00033] train_loss: 0.013046\n",
      "[366/00083] train_loss: 0.013404\n",
      "[366/00133] train_loss: 0.013216\n",
      "[366/00183] train_loss: 0.012862\n",
      "[366/00233] train_loss: 0.012424\n",
      "[366/00283] train_loss: 0.012363\n",
      "[366/00333] train_loss: 0.012300\n",
      "[366/00383] train_loss: 0.012303\n",
      "[366/00433] train_loss: 0.012787\n",
      "[366/00483] train_loss: 0.012987\n",
      "[366/00533] train_loss: 0.011991\n",
      "[366/00583] train_loss: 0.012823\n",
      "[366/00633] train_loss: 0.012445\n",
      "[366/00683] train_loss: 0.011848\n",
      "[366/00733] train_loss: 0.011911\n",
      "[366/00783] train_loss: 0.012009\n",
      "[366/00833] train_loss: 0.012132\n",
      "[366/00883] train_loss: 0.013020\n",
      "[366/00933] train_loss: 0.012999\n",
      "[366/00983] train_loss: 0.012506\n",
      "[366/01033] train_loss: 0.013125\n",
      "[366/01083] train_loss: 0.012247\n",
      "[366/01133] train_loss: 0.012444\n",
      "[366/01183] train_loss: 0.012524\n",
      "[367/00007] train_loss: 0.012322\n",
      "[367/00057] train_loss: 0.012919\n",
      "[367/00107] train_loss: 0.012917\n",
      "[367/00157] train_loss: 0.013159\n",
      "[367/00207] train_loss: 0.012356\n",
      "[367/00257] train_loss: 0.012669\n",
      "[367/00307] train_loss: 0.012756\n",
      "[367/00357] train_loss: 0.013482\n",
      "[367/00407] train_loss: 0.012672\n",
      "[367/00457] train_loss: 0.011766\n",
      "[367/00507] train_loss: 0.011788\n",
      "[367/00557] train_loss: 0.012684\n",
      "[367/00607] train_loss: 0.012330\n",
      "[367/00657] train_loss: 0.012125\n",
      "[367/00707] train_loss: 0.012524\n",
      "[367/00757] train_loss: 0.012603\n",
      "[367/00807] train_loss: 0.012518\n",
      "[367/00857] train_loss: 0.012550\n",
      "[367/00907] train_loss: 0.012001\n",
      "[367/00957] train_loss: 0.012296\n",
      "[367/01007] train_loss: 0.012608\n",
      "[367/01057] train_loss: 0.012970\n",
      "[367/01107] train_loss: 0.013373\n",
      "[367/01157] train_loss: 0.012612\n",
      "[367/01207] train_loss: 0.012675\n",
      "[368/00031] train_loss: 0.013019\n",
      "[368/00081] train_loss: 0.013369\n",
      "[368/00131] train_loss: 0.011948\n",
      "[368/00181] train_loss: 0.012441\n",
      "[368/00231] train_loss: 0.012483\n",
      "[368/00281] train_loss: 0.013406\n",
      "[368/00331] train_loss: 0.012523\n",
      "[368/00381] train_loss: 0.012118\n",
      "[368/00431] train_loss: 0.012618\n",
      "[368/00481] train_loss: 0.012528\n",
      "[368/00531] train_loss: 0.012920\n",
      "[368/00581] train_loss: 0.013184\n",
      "[368/00631] train_loss: 0.012289\n",
      "[368/00681] train_loss: 0.013192\n",
      "[368/00731] train_loss: 0.013322\n",
      "[368/00781] train_loss: 0.011640\n",
      "[368/00831] train_loss: 0.011882\n",
      "[368/00881] train_loss: 0.012780\n",
      "[368/00931] train_loss: 0.012745\n",
      "[368/00981] train_loss: 0.012741\n",
      "[368/01031] train_loss: 0.011656\n",
      "[368/01081] train_loss: 0.011866\n",
      "[368/01131] train_loss: 0.012465\n",
      "[368/01181] train_loss: 0.012106\n",
      "[369/00005] train_loss: 0.012829\n",
      "[369/00055] train_loss: 0.013772\n",
      "[369/00105] train_loss: 0.012220\n",
      "[369/00155] train_loss: 0.012678\n",
      "[369/00205] train_loss: 0.013013\n",
      "[369/00255] train_loss: 0.011939\n",
      "[369/00305] train_loss: 0.012716\n",
      "[369/00355] train_loss: 0.012708\n",
      "[369/00405] train_loss: 0.012401\n",
      "[369/00455] train_loss: 0.012517\n",
      "[369/00505] train_loss: 0.012609\n",
      "[369/00555] train_loss: 0.012652\n",
      "[369/00605] train_loss: 0.012898\n",
      "[369/00655] train_loss: 0.012863\n",
      "[369/00705] train_loss: 0.011723\n",
      "[369/00755] train_loss: 0.012596\n",
      "[369/00805] train_loss: 0.012859\n",
      "[369/00855] train_loss: 0.012903\n",
      "[369/00905] train_loss: 0.012893\n",
      "[369/00955] train_loss: 0.012330\n",
      "[369/01005] train_loss: 0.011837\n",
      "[369/01055] train_loss: 0.012137\n",
      "[369/01105] train_loss: 0.011959\n",
      "[369/01155] train_loss: 0.012058\n",
      "[369/01205] train_loss: 0.012427\n",
      "[370/00029] train_loss: 0.013574\n",
      "[370/00079] train_loss: 0.013832\n",
      "[370/00129] train_loss: 0.012441\n",
      "[370/00179] train_loss: 0.012754\n",
      "[370/00229] train_loss: 0.013160\n",
      "[370/00279] train_loss: 0.012038\n",
      "[370/00329] train_loss: 0.013231\n",
      "[370/00379] train_loss: 0.012769\n",
      "[370/00429] train_loss: 0.012857\n",
      "[370/00479] train_loss: 0.012645\n",
      "[370/00529] train_loss: 0.011832\n",
      "[370/00579] train_loss: 0.013039\n",
      "[370/00629] train_loss: 0.012588\n",
      "[370/00679] train_loss: 0.012258\n",
      "[370/00729] train_loss: 0.012902\n",
      "[370/00779] train_loss: 0.012478\n",
      "[370/00829] train_loss: 0.012690\n",
      "[370/00879] train_loss: 0.012504\n",
      "[370/00929] train_loss: 0.011890\n",
      "[370/00979] train_loss: 0.012368\n",
      "[370/01029] train_loss: 0.011943\n",
      "[370/01079] train_loss: 0.013058\n",
      "[370/01129] train_loss: 0.012639\n",
      "[370/01179] train_loss: 0.012362\n",
      "[371/00003] train_loss: 0.011848\n",
      "[371/00053] train_loss: 0.013511\n",
      "[371/00103] train_loss: 0.013080\n",
      "[371/00153] train_loss: 0.012524\n",
      "[371/00203] train_loss: 0.012649\n",
      "[371/00253] train_loss: 0.012598\n",
      "[371/00303] train_loss: 0.012605\n",
      "[371/00353] train_loss: 0.012194\n",
      "[371/00403] train_loss: 0.011945\n",
      "[371/00453] train_loss: 0.012625\n",
      "[371/00503] train_loss: 0.012399\n",
      "[371/00553] train_loss: 0.012437\n",
      "[371/00603] train_loss: 0.012585\n",
      "[371/00653] train_loss: 0.012687\n",
      "[371/00703] train_loss: 0.012441\n",
      "[371/00753] train_loss: 0.012073\n",
      "[371/00803] train_loss: 0.012125\n",
      "[371/00853] train_loss: 0.011995\n",
      "[371/00903] train_loss: 0.012425\n",
      "[371/00953] train_loss: 0.011900\n",
      "[371/01003] train_loss: 0.013381\n",
      "[371/01053] train_loss: 0.012963\n",
      "[371/01103] train_loss: 0.012396\n",
      "[371/01153] train_loss: 0.013051\n",
      "[371/01203] train_loss: 0.012045\n",
      "[372/00027] train_loss: 0.012721\n",
      "[372/00077] train_loss: 0.013313\n",
      "[372/00127] train_loss: 0.012802\n",
      "[372/00177] train_loss: 0.013191\n",
      "[372/00227] train_loss: 0.012760\n",
      "[372/00277] train_loss: 0.013597\n",
      "[372/00327] train_loss: 0.012685\n",
      "[372/00377] train_loss: 0.012112\n",
      "[372/00427] train_loss: 0.012308\n",
      "[372/00477] train_loss: 0.012275\n",
      "[372/00527] train_loss: 0.012136\n",
      "[372/00577] train_loss: 0.012462\n",
      "[372/00627] train_loss: 0.012500\n",
      "[372/00677] train_loss: 0.011870\n",
      "[372/00727] train_loss: 0.011707\n",
      "[372/00777] train_loss: 0.011850\n",
      "[372/00827] train_loss: 0.011844\n",
      "[372/00877] train_loss: 0.013014\n",
      "[372/00927] train_loss: 0.012391\n",
      "[372/00977] train_loss: 0.012358\n",
      "[372/01027] train_loss: 0.011749\n",
      "[372/01077] train_loss: 0.012001\n",
      "[372/01127] train_loss: 0.012979\n",
      "[372/01177] train_loss: 0.012860\n",
      "[373/00001] train_loss: 0.012032\n",
      "[373/00051] train_loss: 0.013499\n",
      "[373/00101] train_loss: 0.013132\n",
      "[373/00151] train_loss: 0.012997\n",
      "[373/00201] train_loss: 0.012860\n",
      "[373/00251] train_loss: 0.013514\n",
      "[373/00301] train_loss: 0.012422\n",
      "[373/00351] train_loss: 0.013202\n",
      "[373/00401] train_loss: 0.012072\n",
      "[373/00451] train_loss: 0.012294\n",
      "[373/00501] train_loss: 0.013078\n",
      "[373/00551] train_loss: 0.012212\n",
      "[373/00601] train_loss: 0.011800\n",
      "[373/00651] train_loss: 0.012432\n",
      "[373/00701] train_loss: 0.011863\n",
      "[373/00751] train_loss: 0.012876\n",
      "[373/00801] train_loss: 0.012811\n",
      "[373/00851] train_loss: 0.011789\n",
      "[373/00901] train_loss: 0.012021\n",
      "[373/00951] train_loss: 0.012697\n",
      "[373/01001] train_loss: 0.012138\n",
      "[373/01051] train_loss: 0.012987\n",
      "[373/01101] train_loss: 0.012411\n",
      "[373/01151] train_loss: 0.011876\n",
      "[373/01201] train_loss: 0.011488\n",
      "[374/00025] train_loss: 0.012296\n",
      "[374/00075] train_loss: 0.013474\n",
      "[374/00125] train_loss: 0.013302\n",
      "[374/00175] train_loss: 0.012730\n",
      "[374/00225] train_loss: 0.012972\n",
      "[374/00275] train_loss: 0.013018\n",
      "[374/00325] train_loss: 0.012068\n",
      "[374/00375] train_loss: 0.012342\n",
      "[374/00425] train_loss: 0.012641\n",
      "[374/00475] train_loss: 0.012581\n",
      "[374/00525] train_loss: 0.012836\n",
      "[374/00575] train_loss: 0.011896\n",
      "[374/00625] train_loss: 0.012210\n",
      "[374/00675] train_loss: 0.013234\n",
      "[374/00725] train_loss: 0.011897\n",
      "[374/00775] train_loss: 0.011683\n",
      "[374/00825] train_loss: 0.012229\n",
      "[374/00875] train_loss: 0.013341\n",
      "[374/00925] train_loss: 0.012250\n",
      "[374/00975] train_loss: 0.012113\n",
      "[374/01025] train_loss: 0.012376\n",
      "[374/01075] train_loss: 0.012478\n",
      "[374/01125] train_loss: 0.011807\n",
      "[374/01175] train_loss: 0.012596\n",
      "[374/01225] train_loss: 0.012862\n",
      "[375/00049] train_loss: 0.013597\n",
      "[375/00099] train_loss: 0.013681\n",
      "[375/00149] train_loss: 0.012842\n",
      "[375/00199] train_loss: 0.012646\n",
      "[375/00249] train_loss: 0.012836\n",
      "[375/00299] train_loss: 0.012552\n",
      "[375/00349] train_loss: 0.012991\n",
      "[375/00399] train_loss: 0.012381\n",
      "[375/00449] train_loss: 0.012763\n",
      "[375/00499] train_loss: 0.012932\n",
      "[375/00549] train_loss: 0.011937\n",
      "[375/00599] train_loss: 0.012569\n",
      "[375/00649] train_loss: 0.012144\n",
      "[375/00699] train_loss: 0.012429\n",
      "[375/00749] train_loss: 0.012717\n",
      "[375/00799] train_loss: 0.013081\n",
      "[375/00849] train_loss: 0.012168\n",
      "[375/00899] train_loss: 0.012192\n",
      "[375/00949] train_loss: 0.011892\n",
      "[375/00999] train_loss: 0.012890\n",
      "[375/01049] train_loss: 0.011276\n",
      "[375/01099] train_loss: 0.012614\n",
      "[375/01149] train_loss: 0.012036\n",
      "[375/01199] train_loss: 0.012111\n",
      "[376/00023] train_loss: 0.012450\n",
      "[376/00073] train_loss: 0.013312\n",
      "[376/00123] train_loss: 0.012951\n",
      "[376/00173] train_loss: 0.012776\n",
      "[376/00223] train_loss: 0.012708\n",
      "[376/00273] train_loss: 0.012563\n",
      "[376/00323] train_loss: 0.012938\n",
      "[376/00373] train_loss: 0.012537\n",
      "[376/00423] train_loss: 0.012450\n",
      "[376/00473] train_loss: 0.012253\n",
      "[376/00523] train_loss: 0.012480\n",
      "[376/00573] train_loss: 0.011882\n",
      "[376/00623] train_loss: 0.012373\n",
      "[376/00673] train_loss: 0.012053\n",
      "[376/00723] train_loss: 0.012555\n",
      "[376/00773] train_loss: 0.012455\n",
      "[376/00823] train_loss: 0.012318\n",
      "[376/00873] train_loss: 0.012711\n",
      "[376/00923] train_loss: 0.012410\n",
      "[376/00973] train_loss: 0.012119\n",
      "[376/01023] train_loss: 0.012508\n",
      "[376/01073] train_loss: 0.012439\n",
      "[376/01123] train_loss: 0.012633\n",
      "[376/01173] train_loss: 0.012058\n",
      "[376/01223] train_loss: 0.011592\n",
      "[377/00047] train_loss: 0.014072\n",
      "[377/00097] train_loss: 0.013021\n",
      "[377/00147] train_loss: 0.012642\n",
      "[377/00197] train_loss: 0.012815\n",
      "[377/00247] train_loss: 0.013660\n",
      "[377/00297] train_loss: 0.012447\n",
      "[377/00347] train_loss: 0.012523\n",
      "[377/00397] train_loss: 0.012849\n",
      "[377/00447] train_loss: 0.013308\n",
      "[377/00497] train_loss: 0.011829\n",
      "[377/00547] train_loss: 0.012712\n",
      "[377/00597] train_loss: 0.011972\n",
      "[377/00647] train_loss: 0.012529\n",
      "[377/00697] train_loss: 0.012383\n",
      "[377/00747] train_loss: 0.012608\n",
      "[377/00797] train_loss: 0.012623\n",
      "[377/00847] train_loss: 0.011925\n",
      "[377/00897] train_loss: 0.012485\n",
      "[377/00947] train_loss: 0.012903\n",
      "[377/00997] train_loss: 0.012540\n",
      "[377/01047] train_loss: 0.011787\n",
      "[377/01097] train_loss: 0.012209\n",
      "[377/01147] train_loss: 0.012407\n",
      "[377/01197] train_loss: 0.011804\n",
      "[378/00021] train_loss: 0.012791\n",
      "[378/00071] train_loss: 0.013482\n",
      "[378/00121] train_loss: 0.012458\n",
      "[378/00171] train_loss: 0.012735\n",
      "[378/00221] train_loss: 0.012426\n",
      "[378/00271] train_loss: 0.012542\n",
      "[378/00321] train_loss: 0.013189\n",
      "[378/00371] train_loss: 0.012225\n",
      "[378/00421] train_loss: 0.012093\n",
      "[378/00471] train_loss: 0.012484\n",
      "[378/00521] train_loss: 0.012191\n",
      "[378/00571] train_loss: 0.012371\n",
      "[378/00621] train_loss: 0.012600\n",
      "[378/00671] train_loss: 0.012369\n",
      "[378/00721] train_loss: 0.012046\n",
      "[378/00771] train_loss: 0.012229\n",
      "[378/00821] train_loss: 0.012355\n",
      "[378/00871] train_loss: 0.012364\n",
      "[378/00921] train_loss: 0.011968\n",
      "[378/00971] train_loss: 0.012347\n",
      "[378/01021] train_loss: 0.012438\n",
      "[378/01071] train_loss: 0.011585\n",
      "[378/01121] train_loss: 0.012639\n",
      "[378/01171] train_loss: 0.012128\n",
      "[378/01221] train_loss: 0.011951\n",
      "[379/00045] train_loss: 0.013108\n",
      "[379/00095] train_loss: 0.013493\n",
      "[379/00145] train_loss: 0.013764\n",
      "[379/00195] train_loss: 0.012988\n",
      "[379/00245] train_loss: 0.013052\n",
      "[379/00295] train_loss: 0.012512\n",
      "[379/00345] train_loss: 0.012580\n",
      "[379/00395] train_loss: 0.012618\n",
      "[379/00445] train_loss: 0.013387\n",
      "[379/00495] train_loss: 0.012063\n",
      "[379/00545] train_loss: 0.012552\n",
      "[379/00595] train_loss: 0.012407\n",
      "[379/00645] train_loss: 0.011761\n",
      "[379/00695] train_loss: 0.012410\n",
      "[379/00745] train_loss: 0.012735\n",
      "[379/00795] train_loss: 0.012476\n",
      "[379/00845] train_loss: 0.012521\n",
      "[379/00895] train_loss: 0.012085\n",
      "[379/00945] train_loss: 0.012343\n",
      "[379/00995] train_loss: 0.012342\n",
      "[379/01045] train_loss: 0.012067\n",
      "[379/01095] train_loss: 0.012201\n",
      "[379/01145] train_loss: 0.013157\n",
      "[379/01195] train_loss: 0.012037\n",
      "[380/00019] train_loss: 0.013364\n",
      "[380/00069] train_loss: 0.013122\n",
      "[380/00119] train_loss: 0.011935\n",
      "[380/00169] train_loss: 0.013304\n",
      "[380/00219] train_loss: 0.012606\n",
      "[380/00269] train_loss: 0.012770\n",
      "[380/00319] train_loss: 0.012168\n",
      "[380/00369] train_loss: 0.012141\n",
      "[380/00419] train_loss: 0.012486\n",
      "[380/00469] train_loss: 0.012138\n",
      "[380/00519] train_loss: 0.012777\n",
      "[380/00569] train_loss: 0.012349\n",
      "[380/00619] train_loss: 0.011465\n",
      "[380/00669] train_loss: 0.012486\n",
      "[380/00719] train_loss: 0.011901\n",
      "[380/00769] train_loss: 0.013063\n",
      "[380/00819] train_loss: 0.012287\n",
      "[380/00869] train_loss: 0.012498\n",
      "[380/00919] train_loss: 0.012380\n",
      "[380/00969] train_loss: 0.013098\n",
      "[380/01019] train_loss: 0.012434\n",
      "[380/01069] train_loss: 0.012481\n",
      "[380/01119] train_loss: 0.012536\n",
      "[380/01169] train_loss: 0.011978\n",
      "[380/01219] train_loss: 0.012452\n",
      "[381/00043] train_loss: 0.013384\n",
      "[381/00093] train_loss: 0.012993\n",
      "[381/00143] train_loss: 0.013440\n",
      "[381/00193] train_loss: 0.012582\n",
      "[381/00243] train_loss: 0.011836\n",
      "[381/00293] train_loss: 0.012924\n",
      "[381/00343] train_loss: 0.012894\n",
      "[381/00393] train_loss: 0.012524\n",
      "[381/00443] train_loss: 0.012893\n",
      "[381/00493] train_loss: 0.012394\n",
      "[381/00543] train_loss: 0.012125\n",
      "[381/00593] train_loss: 0.013136\n",
      "[381/00643] train_loss: 0.012441\n",
      "[381/00693] train_loss: 0.012564\n",
      "[381/00743] train_loss: 0.012517\n",
      "[381/00793] train_loss: 0.012770\n",
      "[381/00843] train_loss: 0.011982\n",
      "[381/00893] train_loss: 0.012683\n",
      "[381/00943] train_loss: 0.012094\n",
      "[381/00993] train_loss: 0.012057\n",
      "[381/01043] train_loss: 0.012818\n",
      "[381/01093] train_loss: 0.012377\n",
      "[381/01143] train_loss: 0.012318\n",
      "[381/01193] train_loss: 0.012730\n",
      "[382/00017] train_loss: 0.012725\n",
      "[382/00067] train_loss: 0.013633\n",
      "[382/00117] train_loss: 0.013451\n",
      "[382/00167] train_loss: 0.012744\n",
      "[382/00217] train_loss: 0.013003\n",
      "[382/00267] train_loss: 0.012785\n",
      "[382/00317] train_loss: 0.013169\n",
      "[382/00367] train_loss: 0.012504\n",
      "[382/00417] train_loss: 0.012559\n",
      "[382/00467] train_loss: 0.012346\n",
      "[382/00517] train_loss: 0.012172\n",
      "[382/00567] train_loss: 0.012826\n",
      "[382/00617] train_loss: 0.011720\n",
      "[382/00667] train_loss: 0.012915\n",
      "[382/00717] train_loss: 0.012380\n",
      "[382/00767] train_loss: 0.012008\n",
      "[382/00817] train_loss: 0.012683\n",
      "[382/00867] train_loss: 0.012236\n",
      "[382/00917] train_loss: 0.012772\n",
      "[382/00967] train_loss: 0.012258\n",
      "[382/01017] train_loss: 0.012152\n",
      "[382/01067] train_loss: 0.012041\n",
      "[382/01117] train_loss: 0.012316\n",
      "[382/01167] train_loss: 0.011822\n",
      "[382/01217] train_loss: 0.011512\n",
      "[383/00041] train_loss: 0.012796\n",
      "[383/00091] train_loss: 0.013342\n",
      "[383/00141] train_loss: 0.012146\n",
      "[383/00191] train_loss: 0.012310\n",
      "[383/00241] train_loss: 0.012664\n",
      "[383/00291] train_loss: 0.012752\n",
      "[383/00341] train_loss: 0.012003\n",
      "[383/00391] train_loss: 0.012814\n",
      "[383/00441] train_loss: 0.012845\n",
      "[383/00491] train_loss: 0.012211\n",
      "[383/00541] train_loss: 0.012317\n",
      "[383/00591] train_loss: 0.012513\n",
      "[383/00641] train_loss: 0.011889\n",
      "[383/00691] train_loss: 0.011876\n",
      "[383/00741] train_loss: 0.011904\n",
      "[383/00791] train_loss: 0.011800\n",
      "[383/00841] train_loss: 0.012216\n",
      "[383/00891] train_loss: 0.012704\n",
      "[383/00941] train_loss: 0.012526\n",
      "[383/00991] train_loss: 0.012496\n",
      "[383/01041] train_loss: 0.013013\n",
      "[383/01091] train_loss: 0.012281\n",
      "[383/01141] train_loss: 0.012423\n",
      "[383/01191] train_loss: 0.012784\n",
      "[384/00015] train_loss: 0.012817\n",
      "[384/00065] train_loss: 0.013084\n",
      "[384/00115] train_loss: 0.013409\n",
      "[384/00165] train_loss: 0.012644\n",
      "[384/00215] train_loss: 0.013106\n",
      "[384/00265] train_loss: 0.013045\n",
      "[384/00315] train_loss: 0.012145\n",
      "[384/00365] train_loss: 0.012754\n",
      "[384/00415] train_loss: 0.012458\n",
      "[384/00465] train_loss: 0.012677\n",
      "[384/00515] train_loss: 0.013044\n",
      "[384/00565] train_loss: 0.012443\n",
      "[384/00615] train_loss: 0.011563\n",
      "[384/00665] train_loss: 0.012163\n",
      "[384/00715] train_loss: 0.012031\n",
      "[384/00765] train_loss: 0.012412\n",
      "[384/00815] train_loss: 0.012315\n",
      "[384/00865] train_loss: 0.012513\n",
      "[384/00915] train_loss: 0.013064\n",
      "[384/00965] train_loss: 0.012619\n",
      "[384/01015] train_loss: 0.012175\n",
      "[384/01065] train_loss: 0.011862\n",
      "[384/01115] train_loss: 0.011933\n",
      "[384/01165] train_loss: 0.012220\n",
      "[384/01215] train_loss: 0.012592\n",
      "[385/00039] train_loss: 0.012691\n",
      "[385/00089] train_loss: 0.012834\n",
      "[385/00139] train_loss: 0.013094\n",
      "[385/00189] train_loss: 0.012044\n",
      "[385/00239] train_loss: 0.012479\n",
      "[385/00289] train_loss: 0.012203\n",
      "[385/00339] train_loss: 0.012702\n",
      "[385/00389] train_loss: 0.012869\n",
      "[385/00439] train_loss: 0.012073\n",
      "[385/00489] train_loss: 0.012887\n",
      "[385/00539] train_loss: 0.012201\n",
      "[385/00589] train_loss: 0.012751\n",
      "[385/00639] train_loss: 0.011867\n",
      "[385/00689] train_loss: 0.012947\n",
      "[385/00739] train_loss: 0.012257\n",
      "[385/00789] train_loss: 0.011887\n",
      "[385/00839] train_loss: 0.011888\n",
      "[385/00889] train_loss: 0.012328\n",
      "[385/00939] train_loss: 0.013340\n",
      "[385/00989] train_loss: 0.012255\n",
      "[385/01039] train_loss: 0.011889\n",
      "[385/01089] train_loss: 0.012991\n",
      "[385/01139] train_loss: 0.012571\n",
      "[385/01189] train_loss: 0.011785\n",
      "[386/00013] train_loss: 0.012165\n",
      "[386/00063] train_loss: 0.013499\n",
      "[386/00113] train_loss: 0.012709\n",
      "[386/00163] train_loss: 0.012915\n",
      "[386/00213] train_loss: 0.013108\n",
      "[386/00263] train_loss: 0.013231\n",
      "[386/00313] train_loss: 0.012272\n",
      "[386/00363] train_loss: 0.012614\n",
      "[386/00413] train_loss: 0.012585\n",
      "[386/00463] train_loss: 0.011636\n",
      "[386/00513] train_loss: 0.012604\n",
      "[386/00563] train_loss: 0.011360\n",
      "[386/00613] train_loss: 0.011925\n",
      "[386/00663] train_loss: 0.013093\n",
      "[386/00713] train_loss: 0.013000\n",
      "[386/00763] train_loss: 0.011893\n",
      "[386/00813] train_loss: 0.012819\n",
      "[386/00863] train_loss: 0.012161\n",
      "[386/00913] train_loss: 0.012058\n",
      "[386/00963] train_loss: 0.012904\n",
      "[386/01013] train_loss: 0.012290\n",
      "[386/01063] train_loss: 0.012411\n",
      "[386/01113] train_loss: 0.012504\n",
      "[386/01163] train_loss: 0.012191\n",
      "[386/01213] train_loss: 0.012509\n",
      "[387/00037] train_loss: 0.013347\n",
      "[387/00087] train_loss: 0.013516\n",
      "[387/00137] train_loss: 0.012521\n",
      "[387/00187] train_loss: 0.013007\n",
      "[387/00237] train_loss: 0.012720\n",
      "[387/00287] train_loss: 0.012959\n",
      "[387/00337] train_loss: 0.012768\n",
      "[387/00387] train_loss: 0.012597\n",
      "[387/00437] train_loss: 0.012007\n",
      "[387/00487] train_loss: 0.012475\n",
      "[387/00537] train_loss: 0.012836\n",
      "[387/00587] train_loss: 0.012181\n",
      "[387/00637] train_loss: 0.012446\n",
      "[387/00687] train_loss: 0.012086\n",
      "[387/00737] train_loss: 0.011354\n",
      "[387/00787] train_loss: 0.012095\n",
      "[387/00837] train_loss: 0.012100\n",
      "[387/00887] train_loss: 0.011746\n",
      "[387/00937] train_loss: 0.011431\n",
      "[387/00987] train_loss: 0.011996\n",
      "[387/01037] train_loss: 0.012789\n",
      "[387/01087] train_loss: 0.012187\n",
      "[387/01137] train_loss: 0.012163\n",
      "[387/01187] train_loss: 0.012390\n",
      "[388/00011] train_loss: 0.012852\n",
      "[388/00061] train_loss: 0.013274\n",
      "[388/00111] train_loss: 0.012714\n",
      "[388/00161] train_loss: 0.012856\n",
      "[388/00211] train_loss: 0.013415\n",
      "[388/00261] train_loss: 0.012884\n",
      "[388/00311] train_loss: 0.012537\n",
      "[388/00361] train_loss: 0.012486\n",
      "[388/00411] train_loss: 0.012740\n",
      "[388/00461] train_loss: 0.012532\n",
      "[388/00511] train_loss: 0.012746\n",
      "[388/00561] train_loss: 0.011931\n",
      "[388/00611] train_loss: 0.013005\n",
      "[388/00661] train_loss: 0.011595\n",
      "[388/00711] train_loss: 0.012764\n",
      "[388/00761] train_loss: 0.011429\n",
      "[388/00811] train_loss: 0.012333\n",
      "[388/00861] train_loss: 0.012425\n",
      "[388/00911] train_loss: 0.012517\n",
      "[388/00961] train_loss: 0.012205\n",
      "[388/01011] train_loss: 0.012686\n",
      "[388/01061] train_loss: 0.012652\n",
      "[388/01111] train_loss: 0.011921\n",
      "[388/01161] train_loss: 0.012256\n",
      "[388/01211] train_loss: 0.011816\n",
      "[389/00035] train_loss: 0.012437\n",
      "[389/00085] train_loss: 0.012350\n",
      "[389/00135] train_loss: 0.012612\n",
      "[389/00185] train_loss: 0.012182\n",
      "[389/00235] train_loss: 0.012499\n",
      "[389/00285] train_loss: 0.012092\n",
      "[389/00335] train_loss: 0.012813\n",
      "[389/00385] train_loss: 0.013070\n",
      "[389/00435] train_loss: 0.012876\n",
      "[389/00485] train_loss: 0.012186\n",
      "[389/00535] train_loss: 0.012546\n",
      "[389/00585] train_loss: 0.012786\n",
      "[389/00635] train_loss: 0.012088\n",
      "[389/00685] train_loss: 0.012172\n",
      "[389/00735] train_loss: 0.012873\n",
      "[389/00785] train_loss: 0.012999\n",
      "[389/00835] train_loss: 0.012240\n",
      "[389/00885] train_loss: 0.012681\n",
      "[389/00935] train_loss: 0.012188\n",
      "[389/00985] train_loss: 0.011919\n",
      "[389/01035] train_loss: 0.012283\n",
      "[389/01085] train_loss: 0.012271\n",
      "[389/01135] train_loss: 0.012309\n",
      "[389/01185] train_loss: 0.011968\n",
      "[390/00009] train_loss: 0.012344\n",
      "[390/00059] train_loss: 0.012525\n",
      "[390/00109] train_loss: 0.013427\n",
      "[390/00159] train_loss: 0.012633\n",
      "[390/00209] train_loss: 0.013263\n",
      "[390/00259] train_loss: 0.012415\n",
      "[390/00309] train_loss: 0.012858\n",
      "[390/00359] train_loss: 0.012612\n",
      "[390/00409] train_loss: 0.012224\n",
      "[390/00459] train_loss: 0.012467\n",
      "[390/00509] train_loss: 0.011754\n",
      "[390/00559] train_loss: 0.012531\n",
      "[390/00609] train_loss: 0.012713\n",
      "[390/00659] train_loss: 0.011558\n",
      "[390/00709] train_loss: 0.012498\n",
      "[390/00759] train_loss: 0.012835\n",
      "[390/00809] train_loss: 0.012002\n",
      "[390/00859] train_loss: 0.012291\n",
      "[390/00909] train_loss: 0.011812\n",
      "[390/00959] train_loss: 0.012091\n",
      "[390/01009] train_loss: 0.012574\n",
      "[390/01059] train_loss: 0.012033\n",
      "[390/01109] train_loss: 0.013277\n",
      "[390/01159] train_loss: 0.012327\n",
      "[390/01209] train_loss: 0.011682\n",
      "[391/00033] train_loss: 0.013212\n",
      "[391/00083] train_loss: 0.012696\n",
      "[391/00133] train_loss: 0.013950\n",
      "[391/00183] train_loss: 0.012775\n",
      "[391/00233] train_loss: 0.013145\n",
      "[391/00283] train_loss: 0.012996\n",
      "[391/00333] train_loss: 0.011897\n",
      "[391/00383] train_loss: 0.012623\n",
      "[391/00433] train_loss: 0.011995\n",
      "[391/00483] train_loss: 0.012653\n",
      "[391/00533] train_loss: 0.012704\n",
      "[391/00583] train_loss: 0.012522\n",
      "[391/00633] train_loss: 0.012189\n",
      "[391/00683] train_loss: 0.012599\n",
      "[391/00733] train_loss: 0.012186\n",
      "[391/00783] train_loss: 0.013108\n",
      "[391/00833] train_loss: 0.013145\n",
      "[391/00883] train_loss: 0.011795\n",
      "[391/00933] train_loss: 0.011874\n",
      "[391/00983] train_loss: 0.011536\n",
      "[391/01033] train_loss: 0.011926\n",
      "[391/01083] train_loss: 0.011537\n",
      "[391/01133] train_loss: 0.011924\n",
      "[391/01183] train_loss: 0.011773\n",
      "[392/00007] train_loss: 0.012662\n",
      "[392/00057] train_loss: 0.012884\n",
      "[392/00107] train_loss: 0.012900\n",
      "[392/00157] train_loss: 0.013104\n",
      "[392/00207] train_loss: 0.012420\n",
      "[392/00257] train_loss: 0.011815\n",
      "[392/00307] train_loss: 0.013040\n",
      "[392/00357] train_loss: 0.012621\n",
      "[392/00407] train_loss: 0.012080\n",
      "[392/00457] train_loss: 0.013053\n",
      "[392/00507] train_loss: 0.011915\n",
      "[392/00557] train_loss: 0.012648\n",
      "[392/00607] train_loss: 0.012107\n",
      "[392/00657] train_loss: 0.012122\n",
      "[392/00707] train_loss: 0.012681\n",
      "[392/00757] train_loss: 0.012453\n",
      "[392/00807] train_loss: 0.012218\n",
      "[392/00857] train_loss: 0.012520\n",
      "[392/00907] train_loss: 0.012023\n",
      "[392/00957] train_loss: 0.012789\n",
      "[392/01007] train_loss: 0.011894\n",
      "[392/01057] train_loss: 0.012809\n",
      "[392/01107] train_loss: 0.011709\n",
      "[392/01157] train_loss: 0.012523\n",
      "[392/01207] train_loss: 0.012482\n",
      "[393/00031] train_loss: 0.013162\n",
      "[393/00081] train_loss: 0.013252\n",
      "[393/00131] train_loss: 0.013506\n",
      "[393/00181] train_loss: 0.012861\n",
      "[393/00231] train_loss: 0.012810\n",
      "[393/00281] train_loss: 0.012685\n",
      "[393/00331] train_loss: 0.012972\n",
      "[393/00381] train_loss: 0.012228\n",
      "[393/00431] train_loss: 0.012651\n",
      "[393/00481] train_loss: 0.012260\n",
      "[393/00531] train_loss: 0.012581\n",
      "[393/00581] train_loss: 0.012534\n",
      "[393/00631] train_loss: 0.011408\n",
      "[393/00681] train_loss: 0.012501\n",
      "[393/00731] train_loss: 0.012026\n",
      "[393/00781] train_loss: 0.012105\n",
      "[393/00831] train_loss: 0.012684\n",
      "[393/00881] train_loss: 0.011965\n",
      "[393/00931] train_loss: 0.012149\n",
      "[393/00981] train_loss: 0.012269\n",
      "[393/01031] train_loss: 0.012285\n",
      "[393/01081] train_loss: 0.012175\n",
      "[393/01131] train_loss: 0.012772\n",
      "[393/01181] train_loss: 0.011998\n",
      "[394/00005] train_loss: 0.011836\n",
      "[394/00055] train_loss: 0.013182\n",
      "[394/00105] train_loss: 0.012533\n",
      "[394/00155] train_loss: 0.012723\n",
      "[394/00205] train_loss: 0.012786\n",
      "[394/00255] train_loss: 0.012652\n",
      "[394/00305] train_loss: 0.012316\n",
      "[394/00355] train_loss: 0.012518\n",
      "[394/00405] train_loss: 0.013018\n",
      "[394/00455] train_loss: 0.013195\n",
      "[394/00505] train_loss: 0.012595\n",
      "[394/00555] train_loss: 0.013052\n",
      "[394/00605] train_loss: 0.012456\n",
      "[394/00655] train_loss: 0.012199\n",
      "[394/00705] train_loss: 0.011952\n",
      "[394/00755] train_loss: 0.012663\n",
      "[394/00805] train_loss: 0.012171\n",
      "[394/00855] train_loss: 0.012570\n",
      "[394/00905] train_loss: 0.012017\n",
      "[394/00955] train_loss: 0.012187\n",
      "[394/01005] train_loss: 0.012710\n",
      "[394/01055] train_loss: 0.011587\n",
      "[394/01105] train_loss: 0.011488\n",
      "[394/01155] train_loss: 0.011644\n",
      "[394/01205] train_loss: 0.012408\n",
      "[395/00029] train_loss: 0.012956\n",
      "[395/00079] train_loss: 0.012930\n",
      "[395/00129] train_loss: 0.012616\n",
      "[395/00179] train_loss: 0.012553\n",
      "[395/00229] train_loss: 0.012806\n",
      "[395/00279] train_loss: 0.013211\n",
      "[395/00329] train_loss: 0.013031\n",
      "[395/00379] train_loss: 0.012083\n",
      "[395/00429] train_loss: 0.012264\n",
      "[395/00479] train_loss: 0.012088\n",
      "[395/00529] train_loss: 0.012655\n",
      "[395/00579] train_loss: 0.012230\n",
      "[395/00629] train_loss: 0.012792\n",
      "[395/00679] train_loss: 0.012821\n",
      "[395/00729] train_loss: 0.012450\n",
      "[395/00779] train_loss: 0.012141\n",
      "[395/00829] train_loss: 0.011677\n",
      "[395/00879] train_loss: 0.012415\n",
      "[395/00929] train_loss: 0.012078\n",
      "[395/00979] train_loss: 0.012159\n",
      "[395/01029] train_loss: 0.011676\n",
      "[395/01079] train_loss: 0.012916\n",
      "[395/01129] train_loss: 0.012151\n",
      "[395/01179] train_loss: 0.012363\n",
      "[396/00003] train_loss: 0.012914\n",
      "[396/00053] train_loss: 0.013280\n",
      "[396/00103] train_loss: 0.012562\n",
      "[396/00153] train_loss: 0.013419\n",
      "[396/00203] train_loss: 0.012446\n",
      "[396/00253] train_loss: 0.012732\n",
      "[396/00303] train_loss: 0.012770\n",
      "[396/00353] train_loss: 0.012730\n",
      "[396/00403] train_loss: 0.012251\n",
      "[396/00453] train_loss: 0.012307\n",
      "[396/00503] train_loss: 0.012704\n",
      "[396/00553] train_loss: 0.012890\n",
      "[396/00603] train_loss: 0.012940\n",
      "[396/00653] train_loss: 0.012767\n",
      "[396/00703] train_loss: 0.011680\n",
      "[396/00753] train_loss: 0.012357\n",
      "[396/00803] train_loss: 0.011824\n",
      "[396/00853] train_loss: 0.012083\n",
      "[396/00903] train_loss: 0.012389\n",
      "[396/00953] train_loss: 0.012618\n",
      "[396/01003] train_loss: 0.012476\n",
      "[396/01053] train_loss: 0.012400\n",
      "[396/01103] train_loss: 0.012206\n",
      "[396/01153] train_loss: 0.012275\n",
      "[396/01203] train_loss: 0.011182\n",
      "[397/00027] train_loss: 0.012433\n",
      "[397/00077] train_loss: 0.012752\n",
      "[397/00127] train_loss: 0.013163\n",
      "[397/00177] train_loss: 0.012138\n",
      "[397/00227] train_loss: 0.012467\n",
      "[397/00277] train_loss: 0.012800\n",
      "[397/00327] train_loss: 0.012380\n",
      "[397/00377] train_loss: 0.012412\n",
      "[397/00427] train_loss: 0.011899\n",
      "[397/00477] train_loss: 0.012739\n",
      "[397/00527] train_loss: 0.012364\n",
      "[397/00577] train_loss: 0.012451\n",
      "[397/00627] train_loss: 0.012099\n",
      "[397/00677] train_loss: 0.012413\n",
      "[397/00727] train_loss: 0.011854\n",
      "[397/00777] train_loss: 0.013225\n",
      "[397/00827] train_loss: 0.012782\n",
      "[397/00877] train_loss: 0.012123\n",
      "[397/00927] train_loss: 0.012650\n",
      "[397/00977] train_loss: 0.012385\n",
      "[397/01027] train_loss: 0.011882\n",
      "[397/01077] train_loss: 0.012456\n",
      "[397/01127] train_loss: 0.013482\n",
      "[397/01177] train_loss: 0.012482\n",
      "[398/00001] train_loss: 0.012010\n",
      "[398/00051] train_loss: 0.012810\n",
      "[398/00101] train_loss: 0.013180\n",
      "[398/00151] train_loss: 0.012652\n",
      "[398/00201] train_loss: 0.011985\n",
      "[398/00251] train_loss: 0.012625\n",
      "[398/00301] train_loss: 0.012625\n",
      "[398/00351] train_loss: 0.012750\n",
      "[398/00401] train_loss: 0.012425\n",
      "[398/00451] train_loss: 0.011895\n",
      "[398/00501] train_loss: 0.012097\n",
      "[398/00551] train_loss: 0.011902\n",
      "[398/00601] train_loss: 0.013119\n",
      "[398/00651] train_loss: 0.012133\n",
      "[398/00701] train_loss: 0.011918\n",
      "[398/00751] train_loss: 0.012329\n",
      "[398/00801] train_loss: 0.012410\n",
      "[398/00851] train_loss: 0.012664\n",
      "[398/00901] train_loss: 0.012084\n",
      "[398/00951] train_loss: 0.011971\n",
      "[398/01001] train_loss: 0.011732\n",
      "[398/01051] train_loss: 0.012251\n",
      "[398/01101] train_loss: 0.012699\n",
      "[398/01151] train_loss: 0.011598\n",
      "[398/01201] train_loss: 0.012490\n",
      "[399/00025] train_loss: 0.012512\n",
      "[399/00075] train_loss: 0.013025\n",
      "[399/00125] train_loss: 0.012444\n",
      "[399/00175] train_loss: 0.013379\n",
      "[399/00225] train_loss: 0.012877\n",
      "[399/00275] train_loss: 0.012734\n",
      "[399/00325] train_loss: 0.013707\n",
      "[399/00375] train_loss: 0.011802\n",
      "[399/00425] train_loss: 0.012454\n",
      "[399/00475] train_loss: 0.012085\n",
      "[399/00525] train_loss: 0.012934\n",
      "[399/00575] train_loss: 0.011998\n",
      "[399/00625] train_loss: 0.012913\n",
      "[399/00675] train_loss: 0.011990\n",
      "[399/00725] train_loss: 0.011717\n",
      "[399/00775] train_loss: 0.012513\n",
      "[399/00825] train_loss: 0.011644\n",
      "[399/00875] train_loss: 0.012114\n",
      "[399/00925] train_loss: 0.012598\n",
      "[399/00975] train_loss: 0.011846\n",
      "[399/01025] train_loss: 0.012512\n",
      "[399/01075] train_loss: 0.012101\n",
      "[399/01125] train_loss: 0.011980\n",
      "[399/01175] train_loss: 0.012480\n",
      "[399/01225] train_loss: 0.012155\n",
      "[400/00049] train_loss: 0.013492\n",
      "[400/00099] train_loss: 0.013409\n",
      "[400/00149] train_loss: 0.013198\n",
      "[400/00199] train_loss: 0.012663\n",
      "[400/00249] train_loss: 0.012702\n",
      "[400/00299] train_loss: 0.012463\n",
      "[400/00349] train_loss: 0.012715\n",
      "[400/00399] train_loss: 0.012608\n",
      "[400/00449] train_loss: 0.012679\n",
      "[400/00499] train_loss: 0.012240\n",
      "[400/00549] train_loss: 0.012717\n",
      "[400/00599] train_loss: 0.012070\n",
      "[400/00649] train_loss: 0.012612\n",
      "[400/00699] train_loss: 0.012295\n",
      "[400/00749] train_loss: 0.013070\n",
      "[400/00799] train_loss: 0.011600\n",
      "[400/00849] train_loss: 0.012783\n",
      "[400/00899] train_loss: 0.012562\n",
      "[400/00949] train_loss: 0.012203\n",
      "[400/00999] train_loss: 0.011984\n",
      "[400/01049] train_loss: 0.011927\n",
      "[400/01099] train_loss: 0.012283\n",
      "[400/01149] train_loss: 0.012974\n",
      "[400/01199] train_loss: 0.011636\n",
      "[401/00023] train_loss: 0.012253\n",
      "[401/00073] train_loss: 0.013280\n",
      "[401/00123] train_loss: 0.012947\n",
      "[401/00173] train_loss: 0.012982\n",
      "[401/00223] train_loss: 0.012646\n",
      "[401/00273] train_loss: 0.013209\n",
      "[401/00323] train_loss: 0.013230\n",
      "[401/00373] train_loss: 0.012717\n",
      "[401/00423] train_loss: 0.012613\n",
      "[401/00473] train_loss: 0.012505\n",
      "[401/00523] train_loss: 0.012355\n",
      "[401/00573] train_loss: 0.011795\n",
      "[401/00623] train_loss: 0.011722\n",
      "[401/00673] train_loss: 0.011572\n",
      "[401/00723] train_loss: 0.012055\n",
      "[401/00773] train_loss: 0.011931\n",
      "[401/00823] train_loss: 0.012734\n",
      "[401/00873] train_loss: 0.011852\n",
      "[401/00923] train_loss: 0.012721\n",
      "[401/00973] train_loss: 0.011867\n",
      "[401/01023] train_loss: 0.012263\n",
      "[401/01073] train_loss: 0.012368\n",
      "[401/01123] train_loss: 0.012704\n",
      "[401/01173] train_loss: 0.012317\n",
      "[401/01223] train_loss: 0.012665\n",
      "[402/00047] train_loss: 0.012430\n",
      "[402/00097] train_loss: 0.013289\n",
      "[402/00147] train_loss: 0.012545\n",
      "[402/00197] train_loss: 0.012138\n",
      "[402/00247] train_loss: 0.012128\n",
      "[402/00297] train_loss: 0.012200\n",
      "[402/00347] train_loss: 0.013076\n",
      "[402/00397] train_loss: 0.012664\n",
      "[402/00447] train_loss: 0.012419\n",
      "[402/00497] train_loss: 0.012856\n",
      "[402/00547] train_loss: 0.013136\n",
      "[402/00597] train_loss: 0.012623\n",
      "[402/00647] train_loss: 0.011590\n",
      "[402/00697] train_loss: 0.011497\n",
      "[402/00747] train_loss: 0.011954\n",
      "[402/00797] train_loss: 0.011771\n",
      "[402/00847] train_loss: 0.012664\n",
      "[402/00897] train_loss: 0.012583\n",
      "[402/00947] train_loss: 0.012193\n",
      "[402/00997] train_loss: 0.012023\n",
      "[402/01047] train_loss: 0.012563\n",
      "[402/01097] train_loss: 0.012409\n",
      "[402/01147] train_loss: 0.012753\n",
      "[402/01197] train_loss: 0.011994\n",
      "[403/00021] train_loss: 0.012120\n",
      "[403/00071] train_loss: 0.012993\n",
      "[403/00121] train_loss: 0.012885\n",
      "[403/00171] train_loss: 0.012706\n",
      "[403/00221] train_loss: 0.012479\n",
      "[403/00271] train_loss: 0.011848\n",
      "[403/00321] train_loss: 0.012474\n",
      "[403/00371] train_loss: 0.012389\n",
      "[403/00421] train_loss: 0.011906\n",
      "[403/00471] train_loss: 0.012563\n",
      "[403/00521] train_loss: 0.013091\n",
      "[403/00571] train_loss: 0.012396\n",
      "[403/00621] train_loss: 0.012021\n",
      "[403/00671] train_loss: 0.011934\n",
      "[403/00721] train_loss: 0.011957\n",
      "[403/00771] train_loss: 0.012029\n",
      "[403/00821] train_loss: 0.012917\n",
      "[403/00871] train_loss: 0.012662\n",
      "[403/00921] train_loss: 0.012078\n",
      "[403/00971] train_loss: 0.011609\n",
      "[403/01021] train_loss: 0.011681\n",
      "[403/01071] train_loss: 0.012510\n",
      "[403/01121] train_loss: 0.012521\n",
      "[403/01171] train_loss: 0.011894\n",
      "[403/01221] train_loss: 0.012763\n",
      "[404/00045] train_loss: 0.013085\n",
      "[404/00095] train_loss: 0.013271\n",
      "[404/00145] train_loss: 0.012726\n",
      "[404/00195] train_loss: 0.012916\n",
      "[404/00245] train_loss: 0.011893\n",
      "[404/00295] train_loss: 0.012136\n",
      "[404/00345] train_loss: 0.012118\n",
      "[404/00395] train_loss: 0.012350\n",
      "[404/00445] train_loss: 0.012164\n",
      "[404/00495] train_loss: 0.012320\n",
      "[404/00545] train_loss: 0.012608\n",
      "[404/00595] train_loss: 0.013192\n",
      "[404/00645] train_loss: 0.011927\n",
      "[404/00695] train_loss: 0.012841\n",
      "[404/00745] train_loss: 0.012770\n",
      "[404/00795] train_loss: 0.012530\n",
      "[404/00845] train_loss: 0.011904\n",
      "[404/00895] train_loss: 0.012280\n",
      "[404/00945] train_loss: 0.012843\n",
      "[404/00995] train_loss: 0.012977\n",
      "[404/01045] train_loss: 0.012271\n",
      "[404/01095] train_loss: 0.012446\n",
      "[404/01145] train_loss: 0.011502\n",
      "[404/01195] train_loss: 0.012564\n",
      "[405/00019] train_loss: 0.012418\n",
      "[405/00069] train_loss: 0.013280\n",
      "[405/00119] train_loss: 0.012764\n",
      "[405/00169] train_loss: 0.011974\n",
      "[405/00219] train_loss: 0.011875\n",
      "[405/00269] train_loss: 0.012460\n",
      "[405/00319] train_loss: 0.012134\n",
      "[405/00369] train_loss: 0.011943\n",
      "[405/00419] train_loss: 0.012063\n",
      "[405/00469] train_loss: 0.011988\n",
      "[405/00519] train_loss: 0.012518\n",
      "[405/00569] train_loss: 0.012270\n",
      "[405/00619] train_loss: 0.012368\n",
      "[405/00669] train_loss: 0.012033\n",
      "[405/00719] train_loss: 0.012246\n",
      "[405/00769] train_loss: 0.011395\n",
      "[405/00819] train_loss: 0.012524\n",
      "[405/00869] train_loss: 0.011821\n",
      "[405/00919] train_loss: 0.012588\n",
      "[405/00969] train_loss: 0.012188\n",
      "[405/01019] train_loss: 0.012707\n",
      "[405/01069] train_loss: 0.012173\n",
      "[405/01119] train_loss: 0.012104\n",
      "[405/01169] train_loss: 0.012424\n",
      "[405/01219] train_loss: 0.012403\n",
      "[406/00043] train_loss: 0.012946\n",
      "[406/00093] train_loss: 0.012543\n",
      "[406/00143] train_loss: 0.013007\n",
      "[406/00193] train_loss: 0.011907\n",
      "[406/00243] train_loss: 0.012861\n",
      "[406/00293] train_loss: 0.013262\n",
      "[406/00343] train_loss: 0.012143\n",
      "[406/00393] train_loss: 0.012697\n",
      "[406/00443] train_loss: 0.012670\n",
      "[406/00493] train_loss: 0.012100\n",
      "[406/00543] train_loss: 0.012172\n",
      "[406/00593] train_loss: 0.012264\n",
      "[406/00643] train_loss: 0.011955\n",
      "[406/00693] train_loss: 0.012784\n",
      "[406/00743] train_loss: 0.012056\n",
      "[406/00793] train_loss: 0.012503\n",
      "[406/00843] train_loss: 0.012075\n",
      "[406/00893] train_loss: 0.011657\n",
      "[406/00943] train_loss: 0.012095\n",
      "[406/00993] train_loss: 0.012435\n",
      "[406/01043] train_loss: 0.012189\n",
      "[406/01093] train_loss: 0.012679\n",
      "[406/01143] train_loss: 0.012904\n",
      "[406/01193] train_loss: 0.012426\n",
      "[407/00017] train_loss: 0.012424\n",
      "[407/00067] train_loss: 0.012804\n",
      "[407/00117] train_loss: 0.013213\n",
      "[407/00167] train_loss: 0.013001\n",
      "[407/00217] train_loss: 0.012608\n",
      "[407/00267] train_loss: 0.012150\n",
      "[407/00317] train_loss: 0.012300\n",
      "[407/00367] train_loss: 0.012938\n",
      "[407/00417] train_loss: 0.012698\n",
      "[407/00467] train_loss: 0.012649\n",
      "[407/00517] train_loss: 0.011657\n",
      "[407/00567] train_loss: 0.012994\n",
      "[407/00617] train_loss: 0.012405\n",
      "[407/00667] train_loss: 0.012327\n",
      "[407/00717] train_loss: 0.012022\n",
      "[407/00767] train_loss: 0.012431\n",
      "[407/00817] train_loss: 0.012523\n",
      "[407/00867] train_loss: 0.012682\n",
      "[407/00917] train_loss: 0.011370\n",
      "[407/00967] train_loss: 0.011821\n",
      "[407/01017] train_loss: 0.011955\n",
      "[407/01067] train_loss: 0.012713\n",
      "[407/01117] train_loss: 0.012195\n",
      "[407/01167] train_loss: 0.011889\n",
      "[407/01217] train_loss: 0.012383\n",
      "[408/00041] train_loss: 0.013397\n",
      "[408/00091] train_loss: 0.013364\n",
      "[408/00141] train_loss: 0.012331\n",
      "[408/00191] train_loss: 0.012974\n",
      "[408/00241] train_loss: 0.012352\n",
      "[408/00291] train_loss: 0.012741\n",
      "[408/00341] train_loss: 0.012076\n",
      "[408/00391] train_loss: 0.012581\n",
      "[408/00441] train_loss: 0.012638\n",
      "[408/00491] train_loss: 0.012852\n",
      "[408/00541] train_loss: 0.012103\n",
      "[408/00591] train_loss: 0.012001\n",
      "[408/00641] train_loss: 0.012253\n",
      "[408/00691] train_loss: 0.012382\n",
      "[408/00741] train_loss: 0.012063\n",
      "[408/00791] train_loss: 0.012158\n",
      "[408/00841] train_loss: 0.012035\n",
      "[408/00891] train_loss: 0.012088\n",
      "[408/00941] train_loss: 0.012713\n",
      "[408/00991] train_loss: 0.011991\n",
      "[408/01041] train_loss: 0.012310\n",
      "[408/01091] train_loss: 0.011856\n",
      "[408/01141] train_loss: 0.012687\n",
      "[408/01191] train_loss: 0.011489\n",
      "[409/00015] train_loss: 0.011739\n",
      "[409/00065] train_loss: 0.013491\n",
      "[409/00115] train_loss: 0.012177\n",
      "[409/00165] train_loss: 0.013365\n",
      "[409/00215] train_loss: 0.012988\n",
      "[409/00265] train_loss: 0.012840\n",
      "[409/00315] train_loss: 0.012492\n",
      "[409/00365] train_loss: 0.012579\n",
      "[409/00415] train_loss: 0.012036\n",
      "[409/00465] train_loss: 0.011778\n",
      "[409/00515] train_loss: 0.012344\n",
      "[409/00565] train_loss: 0.013230\n",
      "[409/00615] train_loss: 0.012238\n",
      "[409/00665] train_loss: 0.012432\n",
      "[409/00715] train_loss: 0.011727\n",
      "[409/00765] train_loss: 0.011928\n",
      "[409/00815] train_loss: 0.011926\n",
      "[409/00865] train_loss: 0.012678\n",
      "[409/00915] train_loss: 0.012391\n",
      "[409/00965] train_loss: 0.012029\n",
      "[409/01015] train_loss: 0.011796\n",
      "[409/01065] train_loss: 0.012443\n",
      "[409/01115] train_loss: 0.012638\n",
      "[409/01165] train_loss: 0.012055\n",
      "[409/01215] train_loss: 0.013301\n",
      "[410/00039] train_loss: 0.012798\n",
      "[410/00089] train_loss: 0.012453\n",
      "[410/00139] train_loss: 0.012864\n",
      "[410/00189] train_loss: 0.012724\n",
      "[410/00239] train_loss: 0.011757\n",
      "[410/00289] train_loss: 0.011723\n",
      "[410/00339] train_loss: 0.012842\n",
      "[410/00389] train_loss: 0.012639\n",
      "[410/00439] train_loss: 0.012781\n",
      "[410/00489] train_loss: 0.012057\n",
      "[410/00539] train_loss: 0.012759\n",
      "[410/00589] train_loss: 0.012520\n",
      "[410/00639] train_loss: 0.012017\n",
      "[410/00689] train_loss: 0.012268\n",
      "[410/00739] train_loss: 0.011886\n",
      "[410/00789] train_loss: 0.012498\n",
      "[410/00839] train_loss: 0.012181\n",
      "[410/00889] train_loss: 0.012422\n",
      "[410/00939] train_loss: 0.012492\n",
      "[410/00989] train_loss: 0.012859\n",
      "[410/01039] train_loss: 0.012407\n",
      "[410/01089] train_loss: 0.011810\n",
      "[410/01139] train_loss: 0.011892\n",
      "[410/01189] train_loss: 0.012037\n",
      "[411/00013] train_loss: 0.012411\n",
      "[411/00063] train_loss: 0.012897\n",
      "[411/00113] train_loss: 0.013215\n",
      "[411/00163] train_loss: 0.012996\n",
      "[411/00213] train_loss: 0.012947\n",
      "[411/00263] train_loss: 0.012682\n",
      "[411/00313] train_loss: 0.012492\n",
      "[411/00363] train_loss: 0.012130\n",
      "[411/00413] train_loss: 0.012934\n",
      "[411/00463] train_loss: 0.012632\n",
      "[411/00513] train_loss: 0.012504\n",
      "[411/00563] train_loss: 0.011971\n",
      "[411/00613] train_loss: 0.012726\n",
      "[411/00663] train_loss: 0.011980\n",
      "[411/00713] train_loss: 0.012408\n",
      "[411/00763] train_loss: 0.011884\n",
      "[411/00813] train_loss: 0.012463\n",
      "[411/00863] train_loss: 0.011671\n",
      "[411/00913] train_loss: 0.012487\n",
      "[411/00963] train_loss: 0.012821\n",
      "[411/01013] train_loss: 0.012265\n",
      "[411/01063] train_loss: 0.011969\n",
      "[411/01113] train_loss: 0.011518\n",
      "[411/01163] train_loss: 0.012161\n",
      "[411/01213] train_loss: 0.012307\n",
      "[412/00037] train_loss: 0.013039\n",
      "[412/00087] train_loss: 0.012786\n",
      "[412/00137] train_loss: 0.012619\n",
      "[412/00187] train_loss: 0.012466\n",
      "[412/00237] train_loss: 0.012696\n",
      "[412/00287] train_loss: 0.012560\n",
      "[412/00337] train_loss: 0.012370\n",
      "[412/00387] train_loss: 0.012351\n",
      "[412/00437] train_loss: 0.012160\n",
      "[412/00487] train_loss: 0.012595\n",
      "[412/00537] train_loss: 0.012831\n",
      "[412/00587] train_loss: 0.012950\n",
      "[412/00637] train_loss: 0.011549\n",
      "[412/00687] train_loss: 0.012161\n",
      "[412/00737] train_loss: 0.012151\n",
      "[412/00787] train_loss: 0.011889\n",
      "[412/00837] train_loss: 0.012168\n",
      "[412/00887] train_loss: 0.013064\n",
      "[412/00937] train_loss: 0.012090\n",
      "[412/00987] train_loss: 0.011890\n",
      "[412/01037] train_loss: 0.011999\n",
      "[412/01087] train_loss: 0.012255\n",
      "[412/01137] train_loss: 0.012427\n",
      "[412/01187] train_loss: 0.011960\n",
      "[413/00011] train_loss: 0.012511\n",
      "[413/00061] train_loss: 0.012308\n",
      "[413/00111] train_loss: 0.013280\n",
      "[413/00161] train_loss: 0.011845\n",
      "[413/00211] train_loss: 0.013041\n",
      "[413/00261] train_loss: 0.012846\n",
      "[413/00311] train_loss: 0.012819\n",
      "[413/00361] train_loss: 0.011557\n",
      "[413/00411] train_loss: 0.012363\n",
      "[413/00461] train_loss: 0.012286\n",
      "[413/00511] train_loss: 0.012235\n",
      "[413/00561] train_loss: 0.012308\n",
      "[413/00611] train_loss: 0.012846\n",
      "[413/00661] train_loss: 0.011886\n",
      "[413/00711] train_loss: 0.012382\n",
      "[413/00761] train_loss: 0.011893\n",
      "[413/00811] train_loss: 0.012100\n",
      "[413/00861] train_loss: 0.011834\n",
      "[413/00911] train_loss: 0.012818\n",
      "[413/00961] train_loss: 0.012080\n",
      "[413/01011] train_loss: 0.012310\n",
      "[413/01061] train_loss: 0.012019\n",
      "[413/01111] train_loss: 0.011853\n",
      "[413/01161] train_loss: 0.012672\n",
      "[413/01211] train_loss: 0.012296\n",
      "[414/00035] train_loss: 0.012671\n",
      "[414/00085] train_loss: 0.012715\n",
      "[414/00135] train_loss: 0.013853\n",
      "[414/00185] train_loss: 0.012678\n",
      "[414/00235] train_loss: 0.012381\n",
      "[414/00285] train_loss: 0.012407\n",
      "[414/00335] train_loss: 0.012323\n",
      "[414/00385] train_loss: 0.012936\n",
      "[414/00435] train_loss: 0.012248\n",
      "[414/00485] train_loss: 0.012848\n",
      "[414/00535] train_loss: 0.012417\n",
      "[414/00585] train_loss: 0.012357\n",
      "[414/00635] train_loss: 0.011530\n",
      "[414/00685] train_loss: 0.012237\n",
      "[414/00735] train_loss: 0.011788\n",
      "[414/00785] train_loss: 0.012277\n",
      "[414/00835] train_loss: 0.012207\n",
      "[414/00885] train_loss: 0.011780\n",
      "[414/00935] train_loss: 0.012373\n",
      "[414/00985] train_loss: 0.011875\n",
      "[414/01035] train_loss: 0.012206\n",
      "[414/01085] train_loss: 0.012240\n",
      "[414/01135] train_loss: 0.012253\n",
      "[414/01185] train_loss: 0.012035\n",
      "[415/00009] train_loss: 0.012892\n",
      "[415/00059] train_loss: 0.012917\n",
      "[415/00109] train_loss: 0.012774\n",
      "[415/00159] train_loss: 0.012745\n",
      "[415/00209] train_loss: 0.012784\n",
      "[415/00259] train_loss: 0.012477\n",
      "[415/00309] train_loss: 0.012089\n",
      "[415/00359] train_loss: 0.012733\n",
      "[415/00409] train_loss: 0.012109\n",
      "[415/00459] train_loss: 0.012200\n",
      "[415/00509] train_loss: 0.012960\n",
      "[415/00559] train_loss: 0.011741\n",
      "[415/00609] train_loss: 0.012275\n",
      "[415/00659] train_loss: 0.012481\n",
      "[415/00709] train_loss: 0.012106\n",
      "[415/00759] train_loss: 0.012116\n",
      "[415/00809] train_loss: 0.012549\n",
      "[415/00859] train_loss: 0.012373\n",
      "[415/00909] train_loss: 0.012146\n",
      "[415/00959] train_loss: 0.012927\n",
      "[415/01009] train_loss: 0.012510\n",
      "[415/01059] train_loss: 0.012697\n",
      "[415/01109] train_loss: 0.012216\n",
      "[415/01159] train_loss: 0.011962\n",
      "[415/01209] train_loss: 0.012942\n",
      "[416/00033] train_loss: 0.012336\n",
      "[416/00083] train_loss: 0.013037\n",
      "[416/00133] train_loss: 0.012904\n",
      "[416/00183] train_loss: 0.013100\n",
      "[416/00233] train_loss: 0.012285\n",
      "[416/00283] train_loss: 0.012469\n",
      "[416/00333] train_loss: 0.012157\n",
      "[416/00383] train_loss: 0.012720\n",
      "[416/00433] train_loss: 0.012695\n",
      "[416/00483] train_loss: 0.012017\n",
      "[416/00533] train_loss: 0.012514\n",
      "[416/00583] train_loss: 0.011668\n",
      "[416/00633] train_loss: 0.011951\n",
      "[416/00683] train_loss: 0.011664\n",
      "[416/00733] train_loss: 0.011853\n",
      "[416/00783] train_loss: 0.012552\n",
      "[416/00833] train_loss: 0.012807\n",
      "[416/00883] train_loss: 0.011686\n",
      "[416/00933] train_loss: 0.012923\n",
      "[416/00983] train_loss: 0.011824\n",
      "[416/01033] train_loss: 0.012147\n",
      "[416/01083] train_loss: 0.012283\n",
      "[416/01133] train_loss: 0.012015\n",
      "[416/01183] train_loss: 0.012002\n",
      "[417/00007] train_loss: 0.012470\n",
      "[417/00057] train_loss: 0.013232\n",
      "[417/00107] train_loss: 0.012305\n",
      "[417/00157] train_loss: 0.012476\n",
      "[417/00207] train_loss: 0.012779\n",
      "[417/00257] train_loss: 0.012710\n",
      "[417/00307] train_loss: 0.012521\n",
      "[417/00357] train_loss: 0.012684\n",
      "[417/00407] train_loss: 0.012532\n",
      "[417/00457] train_loss: 0.012740\n",
      "[417/00507] train_loss: 0.012206\n",
      "[417/00557] train_loss: 0.012546\n",
      "[417/00607] train_loss: 0.011065\n",
      "[417/00657] train_loss: 0.012838\n",
      "[417/00707] train_loss: 0.011836\n",
      "[417/00757] train_loss: 0.011885\n",
      "[417/00807] train_loss: 0.012154\n",
      "[417/00857] train_loss: 0.012364\n",
      "[417/00907] train_loss: 0.012697\n",
      "[417/00957] train_loss: 0.011771\n",
      "[417/01007] train_loss: 0.012501\n",
      "[417/01057] train_loss: 0.012260\n",
      "[417/01107] train_loss: 0.012237\n",
      "[417/01157] train_loss: 0.013114\n",
      "[417/01207] train_loss: 0.011937\n",
      "[418/00031] train_loss: 0.011760\n",
      "[418/00081] train_loss: 0.012421\n",
      "[418/00131] train_loss: 0.012722\n",
      "[418/00181] train_loss: 0.012872\n",
      "[418/00231] train_loss: 0.012827\n",
      "[418/00281] train_loss: 0.013288\n",
      "[418/00331] train_loss: 0.012386\n",
      "[418/00381] train_loss: 0.011437\n",
      "[418/00431] train_loss: 0.013168\n",
      "[418/00481] train_loss: 0.011766\n",
      "[418/00531] train_loss: 0.012530\n",
      "[418/00581] train_loss: 0.012044\n",
      "[418/00631] train_loss: 0.012353\n",
      "[418/00681] train_loss: 0.012750\n",
      "[418/00731] train_loss: 0.011814\n",
      "[418/00781] train_loss: 0.011872\n",
      "[418/00831] train_loss: 0.012024\n",
      "[418/00881] train_loss: 0.011829\n",
      "[418/00931] train_loss: 0.012111\n",
      "[418/00981] train_loss: 0.012105\n",
      "[418/01031] train_loss: 0.012252\n",
      "[418/01081] train_loss: 0.012558\n",
      "[418/01131] train_loss: 0.012144\n",
      "[418/01181] train_loss: 0.011342\n",
      "[419/00005] train_loss: 0.012349\n",
      "[419/00055] train_loss: 0.012811\n",
      "[419/00105] train_loss: 0.012640\n",
      "[419/00155] train_loss: 0.011712\n",
      "[419/00205] train_loss: 0.013179\n",
      "[419/00255] train_loss: 0.012819\n",
      "[419/00305] train_loss: 0.012110\n",
      "[419/00355] train_loss: 0.012007\n",
      "[419/00405] train_loss: 0.012150\n",
      "[419/00455] train_loss: 0.012493\n",
      "[419/00505] train_loss: 0.013022\n",
      "[419/00555] train_loss: 0.011904\n",
      "[419/00605] train_loss: 0.011951\n",
      "[419/00655] train_loss: 0.012017\n",
      "[419/00705] train_loss: 0.012694\n",
      "[419/00755] train_loss: 0.012533\n",
      "[419/00805] train_loss: 0.012655\n",
      "[419/00855] train_loss: 0.012464\n",
      "[419/00905] train_loss: 0.012130\n",
      "[419/00955] train_loss: 0.012396\n",
      "[419/01005] train_loss: 0.012091\n",
      "[419/01055] train_loss: 0.012007\n",
      "[419/01105] train_loss: 0.012076\n",
      "[419/01155] train_loss: 0.011664\n",
      "[419/01205] train_loss: 0.012068\n",
      "[420/00029] train_loss: 0.011959\n",
      "[420/00079] train_loss: 0.012327\n",
      "[420/00129] train_loss: 0.012549\n",
      "[420/00179] train_loss: 0.012425\n",
      "[420/00229] train_loss: 0.012883\n",
      "[420/00279] train_loss: 0.012892\n",
      "[420/00329] train_loss: 0.012209\n",
      "[420/00379] train_loss: 0.012378\n",
      "[420/00429] train_loss: 0.013239\n",
      "[420/00479] train_loss: 0.012718\n",
      "[420/00529] train_loss: 0.012038\n",
      "[420/00579] train_loss: 0.012184\n",
      "[420/00629] train_loss: 0.012413\n",
      "[420/00679] train_loss: 0.012575\n",
      "[420/00729] train_loss: 0.012216\n",
      "[420/00779] train_loss: 0.012856\n",
      "[420/00829] train_loss: 0.012618\n",
      "[420/00879] train_loss: 0.011986\n",
      "[420/00929] train_loss: 0.011607\n",
      "[420/00979] train_loss: 0.011710\n",
      "[420/01029] train_loss: 0.012403\n",
      "[420/01079] train_loss: 0.011829\n",
      "[420/01129] train_loss: 0.012126\n",
      "[420/01179] train_loss: 0.012121\n",
      "[421/00003] train_loss: 0.011996\n",
      "[421/00053] train_loss: 0.012980\n",
      "[421/00103] train_loss: 0.012840\n",
      "[421/00153] train_loss: 0.012587\n",
      "[421/00203] train_loss: 0.012085\n",
      "[421/00253] train_loss: 0.012768\n",
      "[421/00303] train_loss: 0.012512\n",
      "[421/00353] train_loss: 0.012649\n",
      "[421/00403] train_loss: 0.011956\n",
      "[421/00453] train_loss: 0.012750\n",
      "[421/00503] train_loss: 0.012039\n",
      "[421/00553] train_loss: 0.013056\n",
      "[421/00603] train_loss: 0.012140\n",
      "[421/00653] train_loss: 0.011406\n",
      "[421/00703] train_loss: 0.012586\n",
      "[421/00753] train_loss: 0.011800\n",
      "[421/00803] train_loss: 0.012777\n",
      "[421/00853] train_loss: 0.011813\n",
      "[421/00903] train_loss: 0.012337\n",
      "[421/00953] train_loss: 0.012431\n",
      "[421/01003] train_loss: 0.011740\n",
      "[421/01053] train_loss: 0.011978\n",
      "[421/01103] train_loss: 0.011768\n",
      "[421/01153] train_loss: 0.011664\n",
      "[421/01203] train_loss: 0.011733\n",
      "[422/00027] train_loss: 0.012849\n",
      "[422/00077] train_loss: 0.012366\n",
      "[422/00127] train_loss: 0.012215\n",
      "[422/00177] train_loss: 0.012435\n",
      "[422/00227] train_loss: 0.012841\n",
      "[422/00277] train_loss: 0.012426\n",
      "[422/00327] train_loss: 0.012098\n",
      "[422/00377] train_loss: 0.012061\n",
      "[422/00427] train_loss: 0.012161\n",
      "[422/00477] train_loss: 0.012182\n",
      "[422/00527] train_loss: 0.012815\n",
      "[422/00577] train_loss: 0.012455\n",
      "[422/00627] train_loss: 0.012249\n",
      "[422/00677] train_loss: 0.013161\n",
      "[422/00727] train_loss: 0.012137\n",
      "[422/00777] train_loss: 0.011690\n",
      "[422/00827] train_loss: 0.012654\n",
      "[422/00877] train_loss: 0.012338\n",
      "[422/00927] train_loss: 0.011995\n",
      "[422/00977] train_loss: 0.012120\n",
      "[422/01027] train_loss: 0.012809\n",
      "[422/01077] train_loss: 0.012239\n",
      "[422/01127] train_loss: 0.012079\n",
      "[422/01177] train_loss: 0.011832\n",
      "[423/00001] train_loss: 0.012160\n",
      "[423/00051] train_loss: 0.012938\n",
      "[423/00101] train_loss: 0.013160\n",
      "[423/00151] train_loss: 0.011965\n",
      "[423/00201] train_loss: 0.012196\n",
      "[423/00251] train_loss: 0.012385\n",
      "[423/00301] train_loss: 0.012526\n",
      "[423/00351] train_loss: 0.012335\n",
      "[423/00401] train_loss: 0.012262\n",
      "[423/00451] train_loss: 0.011720\n",
      "[423/00501] train_loss: 0.012781\n",
      "[423/00551] train_loss: 0.012305\n",
      "[423/00601] train_loss: 0.012246\n",
      "[423/00651] train_loss: 0.012143\n",
      "[423/00701] train_loss: 0.012142\n",
      "[423/00751] train_loss: 0.012098\n",
      "[423/00801] train_loss: 0.011917\n",
      "[423/00851] train_loss: 0.011693\n",
      "[423/00901] train_loss: 0.012714\n",
      "[423/00951] train_loss: 0.012424\n",
      "[423/01001] train_loss: 0.012559\n",
      "[423/01051] train_loss: 0.011941\n",
      "[423/01101] train_loss: 0.012729\n",
      "[423/01151] train_loss: 0.012497\n",
      "[423/01201] train_loss: 0.011287\n",
      "[424/00025] train_loss: 0.012174\n",
      "[424/00075] train_loss: 0.013927\n",
      "[424/00125] train_loss: 0.014012\n",
      "[424/00175] train_loss: 0.012553\n",
      "[424/00225] train_loss: 0.012256\n",
      "[424/00275] train_loss: 0.012906\n",
      "[424/00325] train_loss: 0.012352\n",
      "[424/00375] train_loss: 0.012831\n",
      "[424/00425] train_loss: 0.012127\n",
      "[424/00475] train_loss: 0.012041\n",
      "[424/00525] train_loss: 0.011805\n",
      "[424/00575] train_loss: 0.012103\n",
      "[424/00625] train_loss: 0.012412\n",
      "[424/00675] train_loss: 0.012184\n",
      "[424/00725] train_loss: 0.011963\n",
      "[424/00775] train_loss: 0.011970\n",
      "[424/00825] train_loss: 0.012842\n",
      "[424/00875] train_loss: 0.012754\n",
      "[424/00925] train_loss: 0.012119\n",
      "[424/00975] train_loss: 0.011847\n",
      "[424/01025] train_loss: 0.012126\n",
      "[424/01075] train_loss: 0.011774\n",
      "[424/01125] train_loss: 0.011835\n",
      "[424/01175] train_loss: 0.012131\n",
      "[424/01225] train_loss: 0.011281\n",
      "[425/00049] train_loss: 0.013605\n",
      "[425/00099] train_loss: 0.013023\n",
      "[425/00149] train_loss: 0.012487\n",
      "[425/00199] train_loss: 0.011738\n",
      "[425/00249] train_loss: 0.012501\n",
      "[425/00299] train_loss: 0.012543\n",
      "[425/00349] train_loss: 0.011726\n",
      "[425/00399] train_loss: 0.012288\n",
      "[425/00449] train_loss: 0.012174\n",
      "[425/00499] train_loss: 0.011738\n",
      "[425/00549] train_loss: 0.012874\n",
      "[425/00599] train_loss: 0.011699\n",
      "[425/00649] train_loss: 0.012378\n",
      "[425/00699] train_loss: 0.012520\n",
      "[425/00749] train_loss: 0.011915\n",
      "[425/00799] train_loss: 0.012266\n",
      "[425/00849] train_loss: 0.012690\n",
      "[425/00899] train_loss: 0.013030\n",
      "[425/00949] train_loss: 0.012130\n",
      "[425/00999] train_loss: 0.012907\n",
      "[425/01049] train_loss: 0.012140\n",
      "[425/01099] train_loss: 0.012045\n",
      "[425/01149] train_loss: 0.011959\n",
      "[425/01199] train_loss: 0.011614\n",
      "[426/00023] train_loss: 0.012297\n",
      "[426/00073] train_loss: 0.013470\n",
      "[426/00123] train_loss: 0.012338\n",
      "[426/00173] train_loss: 0.013371\n",
      "[426/00223] train_loss: 0.012163\n",
      "[426/00273] train_loss: 0.012530\n",
      "[426/00323] train_loss: 0.012907\n",
      "[426/00373] train_loss: 0.011873\n",
      "[426/00423] train_loss: 0.012518\n",
      "[426/00473] train_loss: 0.012889\n",
      "[426/00523] train_loss: 0.012474\n",
      "[426/00573] train_loss: 0.012924\n",
      "[426/00623] train_loss: 0.012571\n",
      "[426/00673] train_loss: 0.011570\n",
      "[426/00723] train_loss: 0.012314\n",
      "[426/00773] train_loss: 0.011535\n",
      "[426/00823] train_loss: 0.012271\n",
      "[426/00873] train_loss: 0.012840\n",
      "[426/00923] train_loss: 0.012282\n",
      "[426/00973] train_loss: 0.011619\n",
      "[426/01023] train_loss: 0.012504\n",
      "[426/01073] train_loss: 0.011512\n",
      "[426/01123] train_loss: 0.012313\n",
      "[426/01173] train_loss: 0.012619\n",
      "[426/01223] train_loss: 0.011706\n",
      "[427/00047] train_loss: 0.013186\n",
      "[427/00097] train_loss: 0.012544\n",
      "[427/00147] train_loss: 0.012124\n",
      "[427/00197] train_loss: 0.012573\n",
      "[427/00247] train_loss: 0.012210\n",
      "[427/00297] train_loss: 0.012092\n",
      "[427/00347] train_loss: 0.012275\n",
      "[427/00397] train_loss: 0.012139\n",
      "[427/00447] train_loss: 0.012272\n",
      "[427/00497] train_loss: 0.012430\n",
      "[427/00547] train_loss: 0.012313\n",
      "[427/00597] train_loss: 0.011814\n",
      "[427/00647] train_loss: 0.011683\n",
      "[427/00697] train_loss: 0.012048\n",
      "[427/00747] train_loss: 0.012894\n",
      "[427/00797] train_loss: 0.012723\n",
      "[427/00847] train_loss: 0.012665\n",
      "[427/00897] train_loss: 0.012365\n",
      "[427/00947] train_loss: 0.012499\n",
      "[427/00997] train_loss: 0.012093\n",
      "[427/01047] train_loss: 0.012338\n",
      "[427/01097] train_loss: 0.012590\n",
      "[427/01147] train_loss: 0.011995\n",
      "[427/01197] train_loss: 0.012547\n",
      "[428/00021] train_loss: 0.013356\n",
      "[428/00071] train_loss: 0.012914\n",
      "[428/00121] train_loss: 0.012426\n",
      "[428/00171] train_loss: 0.012166\n",
      "[428/00221] train_loss: 0.013136\n",
      "[428/00271] train_loss: 0.012963\n",
      "[428/00321] train_loss: 0.011965\n",
      "[428/00371] train_loss: 0.012026\n",
      "[428/00421] train_loss: 0.013009\n",
      "[428/00471] train_loss: 0.013373\n",
      "[428/00521] train_loss: 0.012283\n",
      "[428/00571] train_loss: 0.012326\n",
      "[428/00621] train_loss: 0.012263\n",
      "[428/00671] train_loss: 0.011930\n",
      "[428/00721] train_loss: 0.012022\n",
      "[428/00771] train_loss: 0.011778\n",
      "[428/00821] train_loss: 0.012374\n",
      "[428/00871] train_loss: 0.012225\n",
      "[428/00921] train_loss: 0.012110\n",
      "[428/00971] train_loss: 0.011978\n",
      "[428/01021] train_loss: 0.011992\n",
      "[428/01071] train_loss: 0.012319\n",
      "[428/01121] train_loss: 0.011336\n",
      "[428/01171] train_loss: 0.012217\n",
      "[428/01221] train_loss: 0.012168\n",
      "[429/00045] train_loss: 0.012720\n",
      "[429/00095] train_loss: 0.012722\n",
      "[429/00145] train_loss: 0.012291\n",
      "[429/00195] train_loss: 0.012024\n",
      "[429/00245] train_loss: 0.012211\n",
      "[429/00295] train_loss: 0.012692\n",
      "[429/00345] train_loss: 0.012329\n",
      "[429/00395] train_loss: 0.013059\n",
      "[429/00445] train_loss: 0.011995\n",
      "[429/00495] train_loss: 0.013732\n",
      "[429/00545] train_loss: 0.011983\n",
      "[429/00595] train_loss: 0.012724\n",
      "[429/00645] train_loss: 0.012067\n",
      "[429/00695] train_loss: 0.011719\n",
      "[429/00745] train_loss: 0.011810\n",
      "[429/00795] train_loss: 0.012532\n",
      "[429/00845] train_loss: 0.012522\n",
      "[429/00895] train_loss: 0.012205\n",
      "[429/00945] train_loss: 0.012480\n",
      "[429/00995] train_loss: 0.012666\n",
      "[429/01045] train_loss: 0.012055\n",
      "[429/01095] train_loss: 0.012341\n",
      "[429/01145] train_loss: 0.012091\n",
      "[429/01195] train_loss: 0.012472\n",
      "[430/00019] train_loss: 0.012660\n",
      "[430/00069] train_loss: 0.013296\n",
      "[430/00119] train_loss: 0.012700\n",
      "[430/00169] train_loss: 0.012217\n",
      "[430/00219] train_loss: 0.012375\n",
      "[430/00269] train_loss: 0.012330\n",
      "[430/00319] train_loss: 0.012137\n",
      "[430/00369] train_loss: 0.012905\n",
      "[430/00419] train_loss: 0.012711\n",
      "[430/00469] train_loss: 0.012747\n",
      "[430/00519] train_loss: 0.012083\n",
      "[430/00569] train_loss: 0.012568\n",
      "[430/00619] train_loss: 0.012456\n",
      "[430/00669] train_loss: 0.012716\n",
      "[430/00719] train_loss: 0.011648\n",
      "[430/00769] train_loss: 0.012087\n",
      "[430/00819] train_loss: 0.012394\n",
      "[430/00869] train_loss: 0.013266\n",
      "[430/00919] train_loss: 0.012066\n",
      "[430/00969] train_loss: 0.011806\n",
      "[430/01019] train_loss: 0.011784\n",
      "[430/01069] train_loss: 0.012016\n",
      "[430/01119] train_loss: 0.011962\n",
      "[430/01169] train_loss: 0.011470\n",
      "[430/01219] train_loss: 0.012303\n",
      "[431/00043] train_loss: 0.012980\n",
      "[431/00093] train_loss: 0.013165\n",
      "[431/00143] train_loss: 0.012249\n",
      "[431/00193] train_loss: 0.012423\n",
      "[431/00243] train_loss: 0.012209\n",
      "[431/00293] train_loss: 0.012595\n",
      "[431/00343] train_loss: 0.012051\n",
      "[431/00393] train_loss: 0.012516\n",
      "[431/00443] train_loss: 0.012027\n",
      "[431/00493] train_loss: 0.012175\n",
      "[431/00543] train_loss: 0.012043\n",
      "[431/00593] train_loss: 0.011988\n",
      "[431/00643] train_loss: 0.012026\n",
      "[431/00693] train_loss: 0.012418\n",
      "[431/00743] train_loss: 0.012018\n",
      "[431/00793] train_loss: 0.012176\n",
      "[431/00843] train_loss: 0.012673\n",
      "[431/00893] train_loss: 0.012131\n",
      "[431/00943] train_loss: 0.011808\n",
      "[431/00993] train_loss: 0.012464\n",
      "[431/01043] train_loss: 0.011962\n",
      "[431/01093] train_loss: 0.013048\n",
      "[431/01143] train_loss: 0.012430\n",
      "[431/01193] train_loss: 0.012126\n",
      "[432/00017] train_loss: 0.012391\n",
      "[432/00067] train_loss: 0.011893\n",
      "[432/00117] train_loss: 0.012415\n",
      "[432/00167] train_loss: 0.012008\n",
      "[432/00217] train_loss: 0.012783\n",
      "[432/00267] train_loss: 0.012562\n",
      "[432/00317] train_loss: 0.012583\n",
      "[432/00367] train_loss: 0.012444\n",
      "[432/00417] train_loss: 0.012424\n",
      "[432/00467] train_loss: 0.012196\n",
      "[432/00517] train_loss: 0.011573\n",
      "[432/00567] train_loss: 0.011893\n",
      "[432/00617] train_loss: 0.012279\n",
      "[432/00667] train_loss: 0.012690\n",
      "[432/00717] train_loss: 0.012620\n",
      "[432/00767] train_loss: 0.012581\n",
      "[432/00817] train_loss: 0.012416\n",
      "[432/00867] train_loss: 0.011782\n",
      "[432/00917] train_loss: 0.012733\n",
      "[432/00967] train_loss: 0.012944\n",
      "[432/01017] train_loss: 0.011746\n",
      "[432/01067] train_loss: 0.011906\n",
      "[432/01117] train_loss: 0.012345\n",
      "[432/01167] train_loss: 0.012585\n",
      "[432/01217] train_loss: 0.012537\n",
      "[433/00041] train_loss: 0.012066\n",
      "[433/00091] train_loss: 0.011833\n",
      "[433/00141] train_loss: 0.012676\n",
      "[433/00191] train_loss: 0.013162\n",
      "[433/00241] train_loss: 0.012733\n",
      "[433/00291] train_loss: 0.012597\n",
      "[433/00341] train_loss: 0.012557\n",
      "[433/00391] train_loss: 0.012007\n",
      "[433/00441] train_loss: 0.012422\n",
      "[433/00491] train_loss: 0.012462\n",
      "[433/00541] train_loss: 0.012872\n",
      "[433/00591] train_loss: 0.013188\n",
      "[433/00641] train_loss: 0.012377\n",
      "[433/00691] train_loss: 0.012080\n",
      "[433/00741] train_loss: 0.011642\n",
      "[433/00791] train_loss: 0.012826\n",
      "[433/00841] train_loss: 0.011941\n",
      "[433/00891] train_loss: 0.011968\n",
      "[433/00941] train_loss: 0.011931\n",
      "[433/00991] train_loss: 0.011755\n",
      "[433/01041] train_loss: 0.011816\n",
      "[433/01091] train_loss: 0.012257\n",
      "[433/01141] train_loss: 0.011649\n",
      "[433/01191] train_loss: 0.012319\n",
      "[434/00015] train_loss: 0.011996\n",
      "[434/00065] train_loss: 0.012485\n",
      "[434/00115] train_loss: 0.012690\n",
      "[434/00165] train_loss: 0.012644\n",
      "[434/00215] train_loss: 0.012273\n",
      "[434/00265] train_loss: 0.012916\n",
      "[434/00315] train_loss: 0.012634\n",
      "[434/00365] train_loss: 0.011884\n",
      "[434/00415] train_loss: 0.012090\n",
      "[434/00465] train_loss: 0.012489\n",
      "[434/00515] train_loss: 0.012122\n",
      "[434/00565] train_loss: 0.012921\n",
      "[434/00615] train_loss: 0.012232\n",
      "[434/00665] train_loss: 0.011959\n",
      "[434/00715] train_loss: 0.012600\n",
      "[434/00765] train_loss: 0.012315\n",
      "[434/00815] train_loss: 0.012464\n",
      "[434/00865] train_loss: 0.012538\n",
      "[434/00915] train_loss: 0.012065\n",
      "[434/00965] train_loss: 0.011820\n",
      "[434/01015] train_loss: 0.012234\n",
      "[434/01065] train_loss: 0.013005\n",
      "[434/01115] train_loss: 0.012054\n",
      "[434/01165] train_loss: 0.012352\n",
      "[434/01215] train_loss: 0.011317\n",
      "[435/00039] train_loss: 0.012644\n",
      "[435/00089] train_loss: 0.012878\n",
      "[435/00139] train_loss: 0.012423\n",
      "[435/00189] train_loss: 0.012367\n",
      "[435/00239] train_loss: 0.012699\n",
      "[435/00289] train_loss: 0.012531\n",
      "[435/00339] train_loss: 0.012299\n",
      "[435/00389] train_loss: 0.012542\n",
      "[435/00439] train_loss: 0.011963\n",
      "[435/00489] train_loss: 0.012330\n",
      "[435/00539] train_loss: 0.011619\n",
      "[435/00589] train_loss: 0.012094\n",
      "[435/00639] train_loss: 0.012173\n",
      "[435/00689] train_loss: 0.011707\n",
      "[435/00739] train_loss: 0.012904\n",
      "[435/00789] train_loss: 0.012120\n",
      "[435/00839] train_loss: 0.012195\n",
      "[435/00889] train_loss: 0.011813\n",
      "[435/00939] train_loss: 0.012216\n",
      "[435/00989] train_loss: 0.012435\n",
      "[435/01039] train_loss: 0.012028\n",
      "[435/01089] train_loss: 0.011815\n",
      "[435/01139] train_loss: 0.012525\n",
      "[435/01189] train_loss: 0.012028\n",
      "[436/00013] train_loss: 0.012553\n",
      "[436/00063] train_loss: 0.012575\n",
      "[436/00113] train_loss: 0.012524\n",
      "[436/00163] train_loss: 0.012649\n",
      "[436/00213] train_loss: 0.012468\n",
      "[436/00263] train_loss: 0.013076\n",
      "[436/00313] train_loss: 0.012724\n",
      "[436/00363] train_loss: 0.012113\n",
      "[436/00413] train_loss: 0.012384\n",
      "[436/00463] train_loss: 0.012383\n",
      "[436/00513] train_loss: 0.012641\n",
      "[436/00563] train_loss: 0.012587\n",
      "[436/00613] train_loss: 0.011681\n",
      "[436/00663] train_loss: 0.011318\n",
      "[436/00713] train_loss: 0.012999\n",
      "[436/00763] train_loss: 0.012616\n",
      "[436/00813] train_loss: 0.012046\n",
      "[436/00863] train_loss: 0.011899\n",
      "[436/00913] train_loss: 0.011952\n",
      "[436/00963] train_loss: 0.012313\n",
      "[436/01013] train_loss: 0.012229\n",
      "[436/01063] train_loss: 0.012151\n",
      "[436/01113] train_loss: 0.012457\n",
      "[436/01163] train_loss: 0.012207\n",
      "[436/01213] train_loss: 0.012469\n",
      "[437/00037] train_loss: 0.012694\n",
      "[437/00087] train_loss: 0.013117\n",
      "[437/00137] train_loss: 0.012516\n",
      "[437/00187] train_loss: 0.012365\n",
      "[437/00237] train_loss: 0.012465\n",
      "[437/00287] train_loss: 0.012535\n",
      "[437/00337] train_loss: 0.011458\n",
      "[437/00387] train_loss: 0.011816\n",
      "[437/00437] train_loss: 0.012317\n",
      "[437/00487] train_loss: 0.012305\n",
      "[437/00537] train_loss: 0.012339\n",
      "[437/00587] train_loss: 0.012089\n",
      "[437/00637] train_loss: 0.011862\n",
      "[437/00687] train_loss: 0.011913\n",
      "[437/00737] train_loss: 0.012414\n",
      "[437/00787] train_loss: 0.011482\n",
      "[437/00837] train_loss: 0.012379\n",
      "[437/00887] train_loss: 0.012063\n",
      "[437/00937] train_loss: 0.012159\n",
      "[437/00987] train_loss: 0.012756\n",
      "[437/01037] train_loss: 0.012584\n",
      "[437/01087] train_loss: 0.012775\n",
      "[437/01137] train_loss: 0.012727\n",
      "[437/01187] train_loss: 0.011447\n",
      "[438/00011] train_loss: 0.012093\n",
      "[438/00061] train_loss: 0.013830\n",
      "[438/00111] train_loss: 0.012765\n",
      "[438/00161] train_loss: 0.012945\n",
      "[438/00211] train_loss: 0.012000\n",
      "[438/00261] train_loss: 0.011855\n",
      "[438/00311] train_loss: 0.011977\n",
      "[438/00361] train_loss: 0.012878\n",
      "[438/00411] train_loss: 0.012128\n",
      "[438/00461] train_loss: 0.011980\n",
      "[438/00511] train_loss: 0.012634\n",
      "[438/00561] train_loss: 0.012291\n",
      "[438/00611] train_loss: 0.011586\n",
      "[438/00661] train_loss: 0.012417\n",
      "[438/00711] train_loss: 0.012308\n",
      "[438/00761] train_loss: 0.012222\n",
      "[438/00811] train_loss: 0.011802\n",
      "[438/00861] train_loss: 0.012160\n",
      "[438/00911] train_loss: 0.012219\n",
      "[438/00961] train_loss: 0.012052\n",
      "[438/01011] train_loss: 0.012126\n",
      "[438/01061] train_loss: 0.011603\n",
      "[438/01111] train_loss: 0.012591\n",
      "[438/01161] train_loss: 0.011813\n",
      "[438/01211] train_loss: 0.013055\n",
      "[439/00035] train_loss: 0.012241\n",
      "[439/00085] train_loss: 0.012983\n",
      "[439/00135] train_loss: 0.013146\n",
      "[439/00185] train_loss: 0.012466\n",
      "[439/00235] train_loss: 0.012068\n",
      "[439/00285] train_loss: 0.011812\n",
      "[439/00335] train_loss: 0.012085\n",
      "[439/00385] train_loss: 0.011898\n",
      "[439/00435] train_loss: 0.011973\n",
      "[439/00485] train_loss: 0.012403\n",
      "[439/00535] train_loss: 0.012859\n",
      "[439/00585] train_loss: 0.011786\n",
      "[439/00635] train_loss: 0.012672\n",
      "[439/00685] train_loss: 0.011759\n",
      "[439/00735] train_loss: 0.012016\n",
      "[439/00785] train_loss: 0.012036\n",
      "[439/00835] train_loss: 0.012436\n",
      "[439/00885] train_loss: 0.012145\n",
      "[439/00935] train_loss: 0.012227\n",
      "[439/00985] train_loss: 0.012459\n",
      "[439/01035] train_loss: 0.011795\n",
      "[439/01085] train_loss: 0.011855\n",
      "[439/01135] train_loss: 0.011210\n",
      "[439/01185] train_loss: 0.012635\n",
      "[440/00009] train_loss: 0.012487\n",
      "[440/00059] train_loss: 0.012584\n",
      "[440/00109] train_loss: 0.013094\n",
      "[440/00159] train_loss: 0.012492\n",
      "[440/00209] train_loss: 0.012249\n",
      "[440/00259] train_loss: 0.011872\n",
      "[440/00309] train_loss: 0.012423\n",
      "[440/00359] train_loss: 0.012186\n",
      "[440/00409] train_loss: 0.012750\n",
      "[440/00459] train_loss: 0.012281\n",
      "[440/00509] train_loss: 0.011978\n",
      "[440/00559] train_loss: 0.011873\n",
      "[440/00609] train_loss: 0.012073\n",
      "[440/00659] train_loss: 0.012544\n",
      "[440/00709] train_loss: 0.011951\n",
      "[440/00759] train_loss: 0.011551\n",
      "[440/00809] train_loss: 0.011711\n",
      "[440/00859] train_loss: 0.011909\n",
      "[440/00909] train_loss: 0.012685\n",
      "[440/00959] train_loss: 0.012098\n",
      "[440/01009] train_loss: 0.012006\n",
      "[440/01059] train_loss: 0.012642\n",
      "[440/01109] train_loss: 0.012309\n",
      "[440/01159] train_loss: 0.012161\n",
      "[440/01209] train_loss: 0.012152\n",
      "[441/00033] train_loss: 0.012992\n",
      "[441/00083] train_loss: 0.012916\n",
      "[441/00133] train_loss: 0.012275\n",
      "[441/00183] train_loss: 0.012187\n",
      "[441/00233] train_loss: 0.012598\n",
      "[441/00283] train_loss: 0.012737\n",
      "[441/00333] train_loss: 0.011942\n",
      "[441/00383] train_loss: 0.012383\n",
      "[441/00433] train_loss: 0.011810\n",
      "[441/00483] train_loss: 0.012886\n",
      "[441/00533] train_loss: 0.012002\n",
      "[441/00583] train_loss: 0.012032\n",
      "[441/00633] train_loss: 0.011565\n",
      "[441/00683] train_loss: 0.012601\n",
      "[441/00733] train_loss: 0.012094\n",
      "[441/00783] train_loss: 0.012981\n",
      "[441/00833] train_loss: 0.012409\n",
      "[441/00883] train_loss: 0.011751\n",
      "[441/00933] train_loss: 0.011800\n",
      "[441/00983] train_loss: 0.011948\n",
      "[441/01033] train_loss: 0.012899\n",
      "[441/01083] train_loss: 0.011872\n",
      "[441/01133] train_loss: 0.012148\n",
      "[441/01183] train_loss: 0.011465\n",
      "[442/00007] train_loss: 0.011937\n",
      "[442/00057] train_loss: 0.012370\n",
      "[442/00107] train_loss: 0.012843\n",
      "[442/00157] train_loss: 0.012695\n",
      "[442/00207] train_loss: 0.013164\n",
      "[442/00257] train_loss: 0.012134\n",
      "[442/00307] train_loss: 0.012472\n",
      "[442/00357] train_loss: 0.011708\n",
      "[442/00407] train_loss: 0.012839\n",
      "[442/00457] train_loss: 0.012536\n",
      "[442/00507] train_loss: 0.012279\n",
      "[442/00557] train_loss: 0.012408\n",
      "[442/00607] train_loss: 0.012422\n",
      "[442/00657] train_loss: 0.012400\n",
      "[442/00707] train_loss: 0.012016\n",
      "[442/00757] train_loss: 0.012152\n",
      "[442/00807] train_loss: 0.012070\n",
      "[442/00857] train_loss: 0.011673\n",
      "[442/00907] train_loss: 0.011676\n",
      "[442/00957] train_loss: 0.011845\n",
      "[442/01007] train_loss: 0.012367\n",
      "[442/01057] train_loss: 0.011880\n",
      "[442/01107] train_loss: 0.011776\n",
      "[442/01157] train_loss: 0.012403\n",
      "[442/01207] train_loss: 0.012558\n",
      "[443/00031] train_loss: 0.013063\n",
      "[443/00081] train_loss: 0.012329\n",
      "[443/00131] train_loss: 0.012749\n",
      "[443/00181] train_loss: 0.012332\n",
      "[443/00231] train_loss: 0.011939\n",
      "[443/00281] train_loss: 0.011938\n",
      "[443/00331] train_loss: 0.011585\n",
      "[443/00381] train_loss: 0.012029\n",
      "[443/00431] train_loss: 0.012235\n",
      "[443/00481] train_loss: 0.012698\n",
      "[443/00531] train_loss: 0.012957\n",
      "[443/00581] train_loss: 0.011329\n",
      "[443/00631] train_loss: 0.012562\n",
      "[443/00681] train_loss: 0.012640\n",
      "[443/00731] train_loss: 0.012592\n",
      "[443/00781] train_loss: 0.012843\n",
      "[443/00831] train_loss: 0.012043\n",
      "[443/00881] train_loss: 0.011440\n",
      "[443/00931] train_loss: 0.012090\n",
      "[443/00981] train_loss: 0.012042\n",
      "[443/01031] train_loss: 0.011878\n",
      "[443/01081] train_loss: 0.012113\n",
      "[443/01131] train_loss: 0.012389\n",
      "[443/01181] train_loss: 0.012808\n",
      "[444/00005] train_loss: 0.011492\n",
      "[444/00055] train_loss: 0.012828\n",
      "[444/00105] train_loss: 0.013019\n",
      "[444/00155] train_loss: 0.013201\n",
      "[444/00205] train_loss: 0.012556\n",
      "[444/00255] train_loss: 0.012519\n",
      "[444/00305] train_loss: 0.013194\n",
      "[444/00355] train_loss: 0.012122\n",
      "[444/00405] train_loss: 0.011790\n",
      "[444/00455] train_loss: 0.012302\n",
      "[444/00505] train_loss: 0.012112\n",
      "[444/00555] train_loss: 0.011949\n",
      "[444/00605] train_loss: 0.012149\n",
      "[444/00655] train_loss: 0.011451\n",
      "[444/00705] train_loss: 0.012355\n",
      "[444/00755] train_loss: 0.012229\n",
      "[444/00805] train_loss: 0.012292\n",
      "[444/00855] train_loss: 0.012205\n",
      "[444/00905] train_loss: 0.011984\n",
      "[444/00955] train_loss: 0.012533\n",
      "[444/01005] train_loss: 0.012480\n",
      "[444/01055] train_loss: 0.011581\n",
      "[444/01105] train_loss: 0.013037\n",
      "[444/01155] train_loss: 0.012696\n",
      "[444/01205] train_loss: 0.011733\n",
      "[445/00029] train_loss: 0.012235\n",
      "[445/00079] train_loss: 0.012863\n",
      "[445/00129] train_loss: 0.012764\n",
      "[445/00179] train_loss: 0.012048\n",
      "[445/00229] train_loss: 0.011345\n",
      "[445/00279] train_loss: 0.011997\n",
      "[445/00329] train_loss: 0.012709\n",
      "[445/00379] train_loss: 0.012664\n",
      "[445/00429] train_loss: 0.012540\n",
      "[445/00479] train_loss: 0.012574\n",
      "[445/00529] train_loss: 0.012229\n",
      "[445/00579] train_loss: 0.012502\n",
      "[445/00629] train_loss: 0.012079\n",
      "[445/00679] train_loss: 0.012613\n",
      "[445/00729] train_loss: 0.012141\n",
      "[445/00779] train_loss: 0.011490\n",
      "[445/00829] train_loss: 0.011762\n",
      "[445/00879] train_loss: 0.011292\n",
      "[445/00929] train_loss: 0.012120\n",
      "[445/00979] train_loss: 0.012456\n",
      "[445/01029] train_loss: 0.012331\n",
      "[445/01079] train_loss: 0.012182\n",
      "[445/01129] train_loss: 0.012374\n",
      "[445/01179] train_loss: 0.012821\n",
      "[446/00003] train_loss: 0.011731\n",
      "[446/00053] train_loss: 0.012061\n",
      "[446/00103] train_loss: 0.012804\n",
      "[446/00153] train_loss: 0.012204\n",
      "[446/00203] train_loss: 0.013038\n",
      "[446/00253] train_loss: 0.012885\n",
      "[446/00303] train_loss: 0.012336\n",
      "[446/00353] train_loss: 0.012279\n",
      "[446/00403] train_loss: 0.011387\n",
      "[446/00453] train_loss: 0.012348\n",
      "[446/00503] train_loss: 0.012505\n",
      "[446/00553] train_loss: 0.012119\n",
      "[446/00603] train_loss: 0.012254\n",
      "[446/00653] train_loss: 0.011934\n",
      "[446/00703] train_loss: 0.012446\n",
      "[446/00753] train_loss: 0.012286\n",
      "[446/00803] train_loss: 0.011887\n",
      "[446/00853] train_loss: 0.012033\n",
      "[446/00903] train_loss: 0.012364\n",
      "[446/00953] train_loss: 0.012135\n",
      "[446/01003] train_loss: 0.012033\n",
      "[446/01053] train_loss: 0.012387\n",
      "[446/01103] train_loss: 0.012090\n",
      "[446/01153] train_loss: 0.011945\n",
      "[446/01203] train_loss: 0.011816\n",
      "[447/00027] train_loss: 0.011679\n",
      "[447/00077] train_loss: 0.013267\n",
      "[447/00127] train_loss: 0.013039\n",
      "[447/00177] train_loss: 0.012904\n",
      "[447/00227] train_loss: 0.012031\n",
      "[447/00277] train_loss: 0.012382\n",
      "[447/00327] train_loss: 0.012920\n",
      "[447/00377] train_loss: 0.011968\n",
      "[447/00427] train_loss: 0.012548\n",
      "[447/00477] train_loss: 0.012862\n",
      "[447/00527] train_loss: 0.012407\n",
      "[447/00577] train_loss: 0.011991\n",
      "[447/00627] train_loss: 0.012751\n",
      "[447/00677] train_loss: 0.011630\n",
      "[447/00727] train_loss: 0.012963\n",
      "[447/00777] train_loss: 0.012138\n",
      "[447/00827] train_loss: 0.012444\n",
      "[447/00877] train_loss: 0.012355\n",
      "[447/00927] train_loss: 0.012549\n",
      "[447/00977] train_loss: 0.011975\n",
      "[447/01027] train_loss: 0.011880\n",
      "[447/01077] train_loss: 0.012366\n",
      "[447/01127] train_loss: 0.011355\n",
      "[447/01177] train_loss: 0.012470\n",
      "[448/00001] train_loss: 0.011545\n",
      "[448/00051] train_loss: 0.012699\n",
      "[448/00101] train_loss: 0.012890\n",
      "[448/00151] train_loss: 0.011930\n",
      "[448/00201] train_loss: 0.012303\n",
      "[448/00251] train_loss: 0.012335\n",
      "[448/00301] train_loss: 0.012739\n",
      "[448/00351] train_loss: 0.012118\n",
      "[448/00401] train_loss: 0.012196\n",
      "[448/00451] train_loss: 0.012415\n",
      "[448/00501] train_loss: 0.012385\n",
      "[448/00551] train_loss: 0.012458\n",
      "[448/00601] train_loss: 0.012682\n",
      "[448/00651] train_loss: 0.012140\n",
      "[448/00701] train_loss: 0.011855\n",
      "[448/00751] train_loss: 0.012821\n",
      "[448/00801] train_loss: 0.012054\n",
      "[448/00851] train_loss: 0.011805\n",
      "[448/00901] train_loss: 0.011659\n",
      "[448/00951] train_loss: 0.012532\n",
      "[448/01001] train_loss: 0.012106\n",
      "[448/01051] train_loss: 0.012022\n",
      "[448/01101] train_loss: 0.011685\n",
      "[448/01151] train_loss: 0.012234\n",
      "[448/01201] train_loss: 0.011895\n",
      "[449/00025] train_loss: 0.011673\n",
      "[449/00075] train_loss: 0.012274\n",
      "[449/00125] train_loss: 0.012821\n",
      "[449/00175] train_loss: 0.012828\n",
      "[449/00225] train_loss: 0.012817\n",
      "[449/00275] train_loss: 0.011919\n",
      "[449/00325] train_loss: 0.012255\n",
      "[449/00375] train_loss: 0.012347\n",
      "[449/00425] train_loss: 0.012790\n",
      "[449/00475] train_loss: 0.011949\n",
      "[449/00525] train_loss: 0.011661\n",
      "[449/00575] train_loss: 0.011657\n",
      "[449/00625] train_loss: 0.012880\n",
      "[449/00675] train_loss: 0.012094\n",
      "[449/00725] train_loss: 0.012536\n",
      "[449/00775] train_loss: 0.012001\n",
      "[449/00825] train_loss: 0.012664\n",
      "[449/00875] train_loss: 0.011829\n",
      "[449/00925] train_loss: 0.011847\n",
      "[449/00975] train_loss: 0.012064\n",
      "[449/01025] train_loss: 0.012167\n",
      "[449/01075] train_loss: 0.011726\n",
      "[449/01125] train_loss: 0.012190\n",
      "[449/01175] train_loss: 0.011719\n",
      "[449/01225] train_loss: 0.012357\n",
      "[450/00049] train_loss: 0.013410\n",
      "[450/00099] train_loss: 0.012996\n",
      "[450/00149] train_loss: 0.012460\n",
      "[450/00199] train_loss: 0.012414\n",
      "[450/00249] train_loss: 0.012076\n",
      "[450/00299] train_loss: 0.012241\n",
      "[450/00349] train_loss: 0.012447\n",
      "[450/00399] train_loss: 0.013479\n",
      "[450/00449] train_loss: 0.012967\n",
      "[450/00499] train_loss: 0.012374\n",
      "[450/00549] train_loss: 0.011685\n",
      "[450/00599] train_loss: 0.011644\n",
      "[450/00649] train_loss: 0.012359\n",
      "[450/00699] train_loss: 0.012178\n",
      "[450/00749] train_loss: 0.012127\n",
      "[450/00799] train_loss: 0.011312\n",
      "[450/00849] train_loss: 0.012367\n",
      "[450/00899] train_loss: 0.012069\n",
      "[450/00949] train_loss: 0.011946\n",
      "[450/00999] train_loss: 0.011934\n",
      "[450/01049] train_loss: 0.012218\n",
      "[450/01099] train_loss: 0.012025\n",
      "[450/01149] train_loss: 0.012292\n",
      "[450/01199] train_loss: 0.011577\n",
      "[451/00023] train_loss: 0.012256\n",
      "[451/00073] train_loss: 0.013026\n",
      "[451/00123] train_loss: 0.012638\n",
      "[451/00173] train_loss: 0.012531\n",
      "[451/00223] train_loss: 0.013037\n",
      "[451/00273] train_loss: 0.011967\n",
      "[451/00323] train_loss: 0.012297\n",
      "[451/00373] train_loss: 0.012082\n",
      "[451/00423] train_loss: 0.011970\n",
      "[451/00473] train_loss: 0.011679\n",
      "[451/00523] train_loss: 0.011883\n",
      "[451/00573] train_loss: 0.012807\n",
      "[451/00623] train_loss: 0.012538\n",
      "[451/00673] train_loss: 0.012420\n",
      "[451/00723] train_loss: 0.011429\n",
      "[451/00773] train_loss: 0.011950\n",
      "[451/00823] train_loss: 0.012306\n",
      "[451/00873] train_loss: 0.011799\n",
      "[451/00923] train_loss: 0.011915\n",
      "[451/00973] train_loss: 0.011480\n",
      "[451/01023] train_loss: 0.011647\n",
      "[451/01073] train_loss: 0.012239\n",
      "[451/01123] train_loss: 0.012629\n",
      "[451/01173] train_loss: 0.011622\n",
      "[451/01223] train_loss: 0.012422\n",
      "[452/00047] train_loss: 0.012975\n",
      "[452/00097] train_loss: 0.012903\n",
      "[452/00147] train_loss: 0.012070\n",
      "[452/00197] train_loss: 0.011770\n",
      "[452/00247] train_loss: 0.012073\n",
      "[452/00297] train_loss: 0.012629\n",
      "[452/00347] train_loss: 0.012877\n",
      "[452/00397] train_loss: 0.012157\n",
      "[452/00447] train_loss: 0.011978\n",
      "[452/00497] train_loss: 0.012694\n",
      "[452/00547] train_loss: 0.011791\n",
      "[452/00597] train_loss: 0.012118\n",
      "[452/00647] train_loss: 0.012575\n",
      "[452/00697] train_loss: 0.012910\n",
      "[452/00747] train_loss: 0.012558\n",
      "[452/00797] train_loss: 0.012289\n",
      "[452/00847] train_loss: 0.011909\n",
      "[452/00897] train_loss: 0.011778\n",
      "[452/00947] train_loss: 0.012311\n",
      "[452/00997] train_loss: 0.012231\n",
      "[452/01047] train_loss: 0.012010\n",
      "[452/01097] train_loss: 0.011694\n",
      "[452/01147] train_loss: 0.012651\n",
      "[452/01197] train_loss: 0.012402\n",
      "[453/00021] train_loss: 0.012706\n",
      "[453/00071] train_loss: 0.012778\n",
      "[453/00121] train_loss: 0.012801\n",
      "[453/00171] train_loss: 0.011908\n",
      "[453/00221] train_loss: 0.012559\n",
      "[453/00271] train_loss: 0.012295\n",
      "[453/00321] train_loss: 0.012106\n",
      "[453/00371] train_loss: 0.012259\n",
      "[453/00421] train_loss: 0.012321\n",
      "[453/00471] train_loss: 0.011940\n",
      "[453/00521] train_loss: 0.011669\n",
      "[453/00571] train_loss: 0.011953\n",
      "[453/00621] train_loss: 0.012687\n",
      "[453/00671] train_loss: 0.012488\n",
      "[453/00721] train_loss: 0.011753\n",
      "[453/00771] train_loss: 0.011731\n",
      "[453/00821] train_loss: 0.011635\n",
      "[453/00871] train_loss: 0.011857\n",
      "[453/00921] train_loss: 0.012032\n",
      "[453/00971] train_loss: 0.012541\n",
      "[453/01021] train_loss: 0.012445\n",
      "[453/01071] train_loss: 0.011864\n",
      "[453/01121] train_loss: 0.011405\n",
      "[453/01171] train_loss: 0.012705\n",
      "[453/01221] train_loss: 0.012794\n",
      "[454/00045] train_loss: 0.012552\n",
      "[454/00095] train_loss: 0.013203\n",
      "[454/00145] train_loss: 0.011558\n",
      "[454/00195] train_loss: 0.012583\n",
      "[454/00245] train_loss: 0.012435\n",
      "[454/00295] train_loss: 0.012347\n",
      "[454/00345] train_loss: 0.012438\n",
      "[454/00395] train_loss: 0.012293\n",
      "[454/00445] train_loss: 0.012860\n",
      "[454/00495] train_loss: 0.012522\n",
      "[454/00545] train_loss: 0.012409\n",
      "[454/00595] train_loss: 0.011855\n",
      "[454/00645] train_loss: 0.011746\n",
      "[454/00695] train_loss: 0.011699\n",
      "[454/00745] train_loss: 0.012355\n",
      "[454/00795] train_loss: 0.012025\n",
      "[454/00845] train_loss: 0.012073\n",
      "[454/00895] train_loss: 0.012350\n",
      "[454/00945] train_loss: 0.012150\n",
      "[454/00995] train_loss: 0.012002\n",
      "[454/01045] train_loss: 0.012132\n",
      "[454/01095] train_loss: 0.012156\n",
      "[454/01145] train_loss: 0.011820\n",
      "[454/01195] train_loss: 0.011863\n",
      "[455/00019] train_loss: 0.012023\n",
      "[455/00069] train_loss: 0.013694\n",
      "[455/00119] train_loss: 0.011940\n",
      "[455/00169] train_loss: 0.011983\n",
      "[455/00219] train_loss: 0.012717\n",
      "[455/00269] train_loss: 0.012664\n",
      "[455/00319] train_loss: 0.012288\n",
      "[455/00369] train_loss: 0.012711\n",
      "[455/00419] train_loss: 0.011954\n",
      "[455/00469] train_loss: 0.011543\n",
      "[455/00519] train_loss: 0.012144\n",
      "[455/00569] train_loss: 0.012550\n",
      "[455/00619] train_loss: 0.012173\n",
      "[455/00669] train_loss: 0.011989\n",
      "[455/00719] train_loss: 0.012449\n",
      "[455/00769] train_loss: 0.012022\n",
      "[455/00819] train_loss: 0.011802\n",
      "[455/00869] train_loss: 0.012405\n",
      "[455/00919] train_loss: 0.012253\n",
      "[455/00969] train_loss: 0.011901\n",
      "[455/01019] train_loss: 0.011916\n",
      "[455/01069] train_loss: 0.012199\n",
      "[455/01119] train_loss: 0.012769\n",
      "[455/01169] train_loss: 0.012342\n",
      "[455/01219] train_loss: 0.011938\n",
      "[456/00043] train_loss: 0.013134\n",
      "[456/00093] train_loss: 0.012773\n",
      "[456/00143] train_loss: 0.013198\n",
      "[456/00193] train_loss: 0.012019\n",
      "[456/00243] train_loss: 0.011819\n",
      "[456/00293] train_loss: 0.012380\n",
      "[456/00343] train_loss: 0.011878\n",
      "[456/00393] train_loss: 0.011846\n",
      "[456/00443] train_loss: 0.012768\n",
      "[456/00493] train_loss: 0.012059\n",
      "[456/00543] train_loss: 0.012115\n",
      "[456/00593] train_loss: 0.012121\n",
      "[456/00643] train_loss: 0.012270\n",
      "[456/00693] train_loss: 0.012671\n",
      "[456/00743] train_loss: 0.011968\n",
      "[456/00793] train_loss: 0.012183\n",
      "[456/00843] train_loss: 0.012665\n",
      "[456/00893] train_loss: 0.011044\n",
      "[456/00943] train_loss: 0.012000\n",
      "[456/00993] train_loss: 0.011634\n",
      "[456/01043] train_loss: 0.012501\n",
      "[456/01093] train_loss: 0.011721\n",
      "[456/01143] train_loss: 0.011689\n",
      "[456/01193] train_loss: 0.011849\n",
      "[457/00017] train_loss: 0.012910\n",
      "[457/00067] train_loss: 0.013218\n",
      "[457/00117] train_loss: 0.012459\n",
      "[457/00167] train_loss: 0.012799\n",
      "[457/00217] train_loss: 0.012458\n",
      "[457/00267] train_loss: 0.012650\n",
      "[457/00317] train_loss: 0.013170\n",
      "[457/00367] train_loss: 0.012519\n",
      "[457/00417] train_loss: 0.013139\n",
      "[457/00467] train_loss: 0.012362\n",
      "[457/00517] train_loss: 0.012485\n",
      "[457/00567] train_loss: 0.012910\n",
      "[457/00617] train_loss: 0.011656\n",
      "[457/00667] train_loss: 0.012324\n",
      "[457/00717] train_loss: 0.011782\n",
      "[457/00767] train_loss: 0.012101\n",
      "[457/00817] train_loss: 0.011633\n",
      "[457/00867] train_loss: 0.012212\n",
      "[457/00917] train_loss: 0.011773\n",
      "[457/00967] train_loss: 0.011555\n",
      "[457/01017] train_loss: 0.011895\n",
      "[457/01067] train_loss: 0.011831\n",
      "[457/01117] train_loss: 0.011702\n",
      "[457/01167] train_loss: 0.012284\n",
      "[457/01217] train_loss: 0.012186\n",
      "[458/00041] train_loss: 0.013117\n",
      "[458/00091] train_loss: 0.012803\n",
      "[458/00141] train_loss: 0.012259\n",
      "[458/00191] train_loss: 0.012309\n",
      "[458/00241] train_loss: 0.012470\n",
      "[458/00291] train_loss: 0.011774\n",
      "[458/00341] train_loss: 0.012453\n",
      "[458/00391] train_loss: 0.013054\n",
      "[458/00441] train_loss: 0.011654\n",
      "[458/00491] train_loss: 0.012858\n",
      "[458/00541] train_loss: 0.012411\n",
      "[458/00591] train_loss: 0.012282\n",
      "[458/00641] train_loss: 0.012296\n",
      "[458/00691] train_loss: 0.011392\n",
      "[458/00741] train_loss: 0.011885\n",
      "[458/00791] train_loss: 0.011848\n",
      "[458/00841] train_loss: 0.011555\n",
      "[458/00891] train_loss: 0.012042\n",
      "[458/00941] train_loss: 0.011941\n",
      "[458/00991] train_loss: 0.012897\n",
      "[458/01041] train_loss: 0.011856\n",
      "[458/01091] train_loss: 0.011828\n",
      "[458/01141] train_loss: 0.011503\n",
      "[458/01191] train_loss: 0.011816\n",
      "[459/00015] train_loss: 0.012310\n",
      "[459/00065] train_loss: 0.012911\n",
      "[459/00115] train_loss: 0.012388\n",
      "[459/00165] train_loss: 0.012260\n",
      "[459/00215] train_loss: 0.012680\n",
      "[459/00265] train_loss: 0.011950\n",
      "[459/00315] train_loss: 0.012441\n",
      "[459/00365] train_loss: 0.012248\n",
      "[459/00415] train_loss: 0.012200\n",
      "[459/00465] train_loss: 0.012526\n",
      "[459/00515] train_loss: 0.012096\n",
      "[459/00565] train_loss: 0.012027\n",
      "[459/00615] train_loss: 0.011636\n",
      "[459/00665] train_loss: 0.012071\n",
      "[459/00715] train_loss: 0.011663\n",
      "[459/00765] train_loss: 0.011903\n",
      "[459/00815] train_loss: 0.011981\n",
      "[459/00865] train_loss: 0.012354\n",
      "[459/00915] train_loss: 0.011927\n",
      "[459/00965] train_loss: 0.012243\n",
      "[459/01015] train_loss: 0.012261\n",
      "[459/01065] train_loss: 0.012486\n",
      "[459/01115] train_loss: 0.011442\n",
      "[459/01165] train_loss: 0.012213\n",
      "[459/01215] train_loss: 0.011577\n",
      "[460/00039] train_loss: 0.012274\n",
      "[460/00089] train_loss: 0.012410\n",
      "[460/00139] train_loss: 0.013380\n",
      "[460/00189] train_loss: 0.012088\n",
      "[460/00239] train_loss: 0.013430\n",
      "[460/00289] train_loss: 0.012435\n",
      "[460/00339] train_loss: 0.012886\n",
      "[460/00389] train_loss: 0.012492\n",
      "[460/00439] train_loss: 0.012446\n",
      "[460/00489] train_loss: 0.011902\n",
      "[460/00539] train_loss: 0.012164\n",
      "[460/00589] train_loss: 0.012185\n",
      "[460/00639] train_loss: 0.011790\n",
      "[460/00689] train_loss: 0.011664\n",
      "[460/00739] train_loss: 0.012476\n",
      "[460/00789] train_loss: 0.013221\n",
      "[460/00839] train_loss: 0.011771\n",
      "[460/00889] train_loss: 0.012099\n",
      "[460/00939] train_loss: 0.012066\n",
      "[460/00989] train_loss: 0.012572\n",
      "[460/01039] train_loss: 0.012320\n",
      "[460/01089] train_loss: 0.011898\n",
      "[460/01139] train_loss: 0.012069\n",
      "[460/01189] train_loss: 0.011619\n",
      "[461/00013] train_loss: 0.012076\n",
      "[461/00063] train_loss: 0.012880\n",
      "[461/00113] train_loss: 0.012772\n",
      "[461/00163] train_loss: 0.012496\n",
      "[461/00213] train_loss: 0.012926\n",
      "[461/00263] train_loss: 0.012596\n",
      "[461/00313] train_loss: 0.011606\n",
      "[461/00363] train_loss: 0.012371\n",
      "[461/00413] train_loss: 0.012566\n",
      "[461/00463] train_loss: 0.012247\n",
      "[461/00513] train_loss: 0.012005\n",
      "[461/00563] train_loss: 0.012074\n",
      "[461/00613] train_loss: 0.012331\n",
      "[461/00663] train_loss: 0.011206\n",
      "[461/00713] train_loss: 0.011705\n",
      "[461/00763] train_loss: 0.012735\n",
      "[461/00813] train_loss: 0.012075\n",
      "[461/00863] train_loss: 0.011632\n",
      "[461/00913] train_loss: 0.012358\n",
      "[461/00963] train_loss: 0.012100\n",
      "[461/01013] train_loss: 0.013018\n",
      "[461/01063] train_loss: 0.012295\n",
      "[461/01113] train_loss: 0.011608\n",
      "[461/01163] train_loss: 0.012405\n",
      "[461/01213] train_loss: 0.011889\n",
      "[462/00037] train_loss: 0.012390\n",
      "[462/00087] train_loss: 0.013438\n",
      "[462/00137] train_loss: 0.013179\n",
      "[462/00187] train_loss: 0.012589\n",
      "[462/00237] train_loss: 0.012505\n",
      "[462/00287] train_loss: 0.011342\n",
      "[462/00337] train_loss: 0.012481\n",
      "[462/00387] train_loss: 0.012691\n",
      "[462/00437] train_loss: 0.012561\n",
      "[462/00487] train_loss: 0.012777\n",
      "[462/00537] train_loss: 0.012202\n",
      "[462/00587] train_loss: 0.012196\n",
      "[462/00637] train_loss: 0.012604\n",
      "[462/00687] train_loss: 0.012238\n",
      "[462/00737] train_loss: 0.012019\n",
      "[462/00787] train_loss: 0.011990\n",
      "[462/00837] train_loss: 0.011724\n",
      "[462/00887] train_loss: 0.012389\n",
      "[462/00937] train_loss: 0.012138\n",
      "[462/00987] train_loss: 0.012427\n",
      "[462/01037] train_loss: 0.012426\n",
      "[462/01087] train_loss: 0.011783\n",
      "[462/01137] train_loss: 0.012255\n",
      "[462/01187] train_loss: 0.011881\n",
      "[463/00011] train_loss: 0.011952\n",
      "[463/00061] train_loss: 0.012396\n",
      "[463/00111] train_loss: 0.013036\n",
      "[463/00161] train_loss: 0.012468\n",
      "[463/00211] train_loss: 0.012259\n",
      "[463/00261] train_loss: 0.012024\n",
      "[463/00311] train_loss: 0.012817\n",
      "[463/00361] train_loss: 0.011748\n",
      "[463/00411] train_loss: 0.011910\n",
      "[463/00461] train_loss: 0.012836\n",
      "[463/00511] train_loss: 0.012351\n",
      "[463/00561] train_loss: 0.012200\n",
      "[463/00611] train_loss: 0.012510\n",
      "[463/00661] train_loss: 0.011691\n",
      "[463/00711] train_loss: 0.012251\n",
      "[463/00761] train_loss: 0.012492\n",
      "[463/00811] train_loss: 0.012135\n",
      "[463/00861] train_loss: 0.011654\n",
      "[463/00911] train_loss: 0.012248\n",
      "[463/00961] train_loss: 0.012360\n",
      "[463/01011] train_loss: 0.011848\n",
      "[463/01061] train_loss: 0.011502\n",
      "[463/01111] train_loss: 0.012238\n",
      "[463/01161] train_loss: 0.012055\n",
      "[463/01211] train_loss: 0.011855\n",
      "[464/00035] train_loss: 0.012690\n",
      "[464/00085] train_loss: 0.012474\n",
      "[464/00135] train_loss: 0.012520\n",
      "[464/00185] train_loss: 0.011555\n",
      "[464/00235] train_loss: 0.012799\n",
      "[464/00285] train_loss: 0.012800\n",
      "[464/00335] train_loss: 0.012659\n",
      "[464/00385] train_loss: 0.012251\n",
      "[464/00435] train_loss: 0.012018\n",
      "[464/00485] train_loss: 0.012399\n",
      "[464/00535] train_loss: 0.011980\n",
      "[464/00585] train_loss: 0.012351\n",
      "[464/00635] train_loss: 0.012336\n",
      "[464/00685] train_loss: 0.012207\n",
      "[464/00735] train_loss: 0.012314\n",
      "[464/00785] train_loss: 0.012467\n",
      "[464/00835] train_loss: 0.011913\n",
      "[464/00885] train_loss: 0.011170\n",
      "[464/00935] train_loss: 0.011639\n",
      "[464/00985] train_loss: 0.012381\n",
      "[464/01035] train_loss: 0.012299\n",
      "[464/01085] train_loss: 0.012234\n",
      "[464/01135] train_loss: 0.012104\n",
      "[464/01185] train_loss: 0.012350\n",
      "[465/00009] train_loss: 0.011888\n",
      "[465/00059] train_loss: 0.012964\n",
      "[465/00109] train_loss: 0.012876\n",
      "[465/00159] train_loss: 0.011999\n",
      "[465/00209] train_loss: 0.012274\n",
      "[465/00259] train_loss: 0.012346\n",
      "[465/00309] train_loss: 0.012210\n",
      "[465/00359] train_loss: 0.011917\n",
      "[465/00409] train_loss: 0.012294\n",
      "[465/00459] train_loss: 0.012039\n",
      "[465/00509] train_loss: 0.012190\n",
      "[465/00559] train_loss: 0.011719\n",
      "[465/00609] train_loss: 0.011741\n",
      "[465/00659] train_loss: 0.011681\n",
      "[465/00709] train_loss: 0.012129\n",
      "[465/00759] train_loss: 0.011612\n",
      "[465/00809] train_loss: 0.012101\n",
      "[465/00859] train_loss: 0.012626\n",
      "[465/00909] train_loss: 0.011703\n",
      "[465/00959] train_loss: 0.011805\n",
      "[465/01009] train_loss: 0.012273\n",
      "[465/01059] train_loss: 0.012127\n",
      "[465/01109] train_loss: 0.012287\n",
      "[465/01159] train_loss: 0.012064\n",
      "[465/01209] train_loss: 0.011721\n",
      "[466/00033] train_loss: 0.012817\n",
      "[466/00083] train_loss: 0.012108\n",
      "[466/00133] train_loss: 0.012588\n",
      "[466/00183] train_loss: 0.012641\n",
      "[466/00233] train_loss: 0.012472\n",
      "[466/00283] train_loss: 0.012020\n",
      "[466/00333] train_loss: 0.012094\n",
      "[466/00383] train_loss: 0.012063\n",
      "[466/00433] train_loss: 0.012364\n",
      "[466/00483] train_loss: 0.012098\n",
      "[466/00533] train_loss: 0.012012\n",
      "[466/00583] train_loss: 0.011937\n",
      "[466/00633] train_loss: 0.012306\n",
      "[466/00683] train_loss: 0.011720\n",
      "[466/00733] train_loss: 0.012293\n",
      "[466/00783] train_loss: 0.012125\n",
      "[466/00833] train_loss: 0.013235\n",
      "[466/00883] train_loss: 0.011253\n",
      "[466/00933] train_loss: 0.012157\n",
      "[466/00983] train_loss: 0.012048\n",
      "[466/01033] train_loss: 0.012963\n",
      "[466/01083] train_loss: 0.012271\n",
      "[466/01133] train_loss: 0.012205\n",
      "[466/01183] train_loss: 0.012414\n",
      "[467/00007] train_loss: 0.012549\n",
      "[467/00057] train_loss: 0.012072\n",
      "[467/00107] train_loss: 0.012908\n",
      "[467/00157] train_loss: 0.012464\n",
      "[467/00207] train_loss: 0.012496\n",
      "[467/00257] train_loss: 0.012029\n",
      "[467/00307] train_loss: 0.011780\n",
      "[467/00357] train_loss: 0.012825\n",
      "[467/00407] train_loss: 0.012124\n",
      "[467/00457] train_loss: 0.012444\n",
      "[467/00507] train_loss: 0.012182\n",
      "[467/00557] train_loss: 0.012549\n",
      "[467/00607] train_loss: 0.011496\n",
      "[467/00657] train_loss: 0.011042\n",
      "[467/00707] train_loss: 0.012186\n",
      "[467/00757] train_loss: 0.012537\n",
      "[467/00807] train_loss: 0.012435\n",
      "[467/00857] train_loss: 0.011911\n",
      "[467/00907] train_loss: 0.011708\n",
      "[467/00957] train_loss: 0.011899\n",
      "[467/01007] train_loss: 0.012114\n",
      "[467/01057] train_loss: 0.011657\n",
      "[467/01107] train_loss: 0.011934\n",
      "[467/01157] train_loss: 0.012630\n",
      "[467/01207] train_loss: 0.012210\n",
      "[468/00031] train_loss: 0.011975\n",
      "[468/00081] train_loss: 0.013492\n",
      "[468/00131] train_loss: 0.011835\n",
      "[468/00181] train_loss: 0.012488\n",
      "[468/00231] train_loss: 0.012726\n",
      "[468/00281] train_loss: 0.011568\n",
      "[468/00331] train_loss: 0.012019\n",
      "[468/00381] train_loss: 0.012572\n",
      "[468/00431] train_loss: 0.012302\n",
      "[468/00481] train_loss: 0.012629\n",
      "[468/00531] train_loss: 0.013229\n",
      "[468/00581] train_loss: 0.012618\n",
      "[468/00631] train_loss: 0.011697\n",
      "[468/00681] train_loss: 0.011708\n",
      "[468/00731] train_loss: 0.011470\n",
      "[468/00781] train_loss: 0.011799\n",
      "[468/00831] train_loss: 0.012175\n",
      "[468/00881] train_loss: 0.012085\n",
      "[468/00931] train_loss: 0.012044\n",
      "[468/00981] train_loss: 0.012313\n",
      "[468/01031] train_loss: 0.012122\n",
      "[468/01081] train_loss: 0.011718\n",
      "[468/01131] train_loss: 0.012022\n",
      "[468/01181] train_loss: 0.012265\n",
      "[469/00005] train_loss: 0.012174\n",
      "[469/00055] train_loss: 0.013244\n",
      "[469/00105] train_loss: 0.012451\n",
      "[469/00155] train_loss: 0.012598\n",
      "[469/00205] train_loss: 0.012154\n",
      "[469/00255] train_loss: 0.012973\n",
      "[469/00305] train_loss: 0.012748\n",
      "[469/00355] train_loss: 0.011865\n",
      "[469/00405] train_loss: 0.012023\n",
      "[469/00455] train_loss: 0.012614\n",
      "[469/00505] train_loss: 0.011536\n",
      "[469/00555] train_loss: 0.012060\n",
      "[469/00605] train_loss: 0.012385\n",
      "[469/00655] train_loss: 0.012500\n",
      "[469/00705] train_loss: 0.012554\n",
      "[469/00755] train_loss: 0.011570\n",
      "[469/00805] train_loss: 0.012334\n",
      "[469/00855] train_loss: 0.012375\n",
      "[469/00905] train_loss: 0.011697\n",
      "[469/00955] train_loss: 0.011942\n",
      "[469/01005] train_loss: 0.011607\n",
      "[469/01055] train_loss: 0.012227\n",
      "[469/01105] train_loss: 0.011710\n",
      "[469/01155] train_loss: 0.011056\n",
      "[469/01205] train_loss: 0.011454\n",
      "[470/00029] train_loss: 0.012539\n",
      "[470/00079] train_loss: 0.012632\n",
      "[470/00129] train_loss: 0.012143\n",
      "[470/00179] train_loss: 0.012594\n",
      "[470/00229] train_loss: 0.012244\n",
      "[470/00279] train_loss: 0.012488\n",
      "[470/00329] train_loss: 0.011885\n",
      "[470/00379] train_loss: 0.012251\n",
      "[470/00429] train_loss: 0.012224\n",
      "[470/00479] train_loss: 0.012159\n",
      "[470/00529] train_loss: 0.012205\n",
      "[470/00579] train_loss: 0.012715\n",
      "[470/00629] train_loss: 0.012577\n",
      "[470/00679] train_loss: 0.012032\n",
      "[470/00729] train_loss: 0.011815\n",
      "[470/00779] train_loss: 0.011990\n",
      "[470/00829] train_loss: 0.012254\n",
      "[470/00879] train_loss: 0.011699\n",
      "[470/00929] train_loss: 0.011960\n",
      "[470/00979] train_loss: 0.012329\n",
      "[470/01029] train_loss: 0.012509\n",
      "[470/01079] train_loss: 0.012059\n",
      "[470/01129] train_loss: 0.012477\n",
      "[470/01179] train_loss: 0.012186\n",
      "[471/00003] train_loss: 0.012250\n",
      "[471/00053] train_loss: 0.012236\n",
      "[471/00103] train_loss: 0.012637\n",
      "[471/00153] train_loss: 0.012529\n",
      "[471/00203] train_loss: 0.012196\n",
      "[471/00253] train_loss: 0.012679\n",
      "[471/00303] train_loss: 0.011761\n",
      "[471/00353] train_loss: 0.012776\n",
      "[471/00403] train_loss: 0.012106\n",
      "[471/00453] train_loss: 0.011188\n",
      "[471/00503] train_loss: 0.011914\n",
      "[471/00553] train_loss: 0.011898\n",
      "[471/00603] train_loss: 0.012043\n",
      "[471/00653] train_loss: 0.011914\n",
      "[471/00703] train_loss: 0.011754\n",
      "[471/00753] train_loss: 0.012456\n",
      "[471/00803] train_loss: 0.011984\n",
      "[471/00853] train_loss: 0.012264\n",
      "[471/00903] train_loss: 0.011996\n",
      "[471/00953] train_loss: 0.011585\n",
      "[471/01003] train_loss: 0.011286\n",
      "[471/01053] train_loss: 0.011679\n",
      "[471/01103] train_loss: 0.013180\n",
      "[471/01153] train_loss: 0.012356\n",
      "[471/01203] train_loss: 0.011695\n",
      "[472/00027] train_loss: 0.012010\n",
      "[472/00077] train_loss: 0.012743\n",
      "[472/00127] train_loss: 0.012630\n",
      "[472/00177] train_loss: 0.012444\n",
      "[472/00227] train_loss: 0.012127\n",
      "[472/00277] train_loss: 0.011914\n",
      "[472/00327] train_loss: 0.012841\n",
      "[472/00377] train_loss: 0.012696\n",
      "[472/00427] train_loss: 0.012572\n",
      "[472/00477] train_loss: 0.011974\n",
      "[472/00527] train_loss: 0.011909\n",
      "[472/00577] train_loss: 0.011934\n",
      "[472/00627] train_loss: 0.012170\n",
      "[472/00677] train_loss: 0.012770\n",
      "[472/00727] train_loss: 0.012175\n",
      "[472/00777] train_loss: 0.012200\n",
      "[472/00827] train_loss: 0.012028\n",
      "[472/00877] train_loss: 0.012366\n",
      "[472/00927] train_loss: 0.011720\n",
      "[472/00977] train_loss: 0.012459\n",
      "[472/01027] train_loss: 0.012216\n",
      "[472/01077] train_loss: 0.011331\n",
      "[472/01127] train_loss: 0.012024\n",
      "[472/01177] train_loss: 0.012427\n",
      "[473/00001] train_loss: 0.012531\n",
      "[473/00051] train_loss: 0.012776\n",
      "[473/00101] train_loss: 0.011886\n",
      "[473/00151] train_loss: 0.011960\n",
      "[473/00201] train_loss: 0.013313\n",
      "[473/00251] train_loss: 0.012395\n",
      "[473/00301] train_loss: 0.012433\n",
      "[473/00351] train_loss: 0.012825\n",
      "[473/00401] train_loss: 0.012363\n",
      "[473/00451] train_loss: 0.012979\n",
      "[473/00501] train_loss: 0.012201\n",
      "[473/00551] train_loss: 0.011992\n",
      "[473/00601] train_loss: 0.012453\n",
      "[473/00651] train_loss: 0.012027\n",
      "[473/00701] train_loss: 0.012272\n",
      "[473/00751] train_loss: 0.011254\n",
      "[473/00801] train_loss: 0.012029\n",
      "[473/00851] train_loss: 0.012223\n",
      "[473/00901] train_loss: 0.011925\n",
      "[473/00951] train_loss: 0.012002\n",
      "[473/01001] train_loss: 0.011288\n",
      "[473/01051] train_loss: 0.012271\n",
      "[473/01101] train_loss: 0.011838\n",
      "[473/01151] train_loss: 0.011272\n",
      "[473/01201] train_loss: 0.011861\n",
      "[474/00025] train_loss: 0.011944\n",
      "[474/00075] train_loss: 0.012602\n",
      "[474/00125] train_loss: 0.012262\n",
      "[474/00175] train_loss: 0.012043\n",
      "[474/00225] train_loss: 0.011819\n",
      "[474/00275] train_loss: 0.012700\n",
      "[474/00325] train_loss: 0.013073\n",
      "[474/00375] train_loss: 0.011540\n",
      "[474/00425] train_loss: 0.012471\n",
      "[474/00475] train_loss: 0.012345\n",
      "[474/00525] train_loss: 0.011556\n",
      "[474/00575] train_loss: 0.012041\n",
      "[474/00625] train_loss: 0.012436\n",
      "[474/00675] train_loss: 0.011423\n",
      "[474/00725] train_loss: 0.012448\n",
      "[474/00775] train_loss: 0.013070\n",
      "[474/00825] train_loss: 0.013078\n",
      "[474/00875] train_loss: 0.012121\n",
      "[474/00925] train_loss: 0.011859\n",
      "[474/00975] train_loss: 0.012275\n",
      "[474/01025] train_loss: 0.012141\n",
      "[474/01075] train_loss: 0.012941\n",
      "[474/01125] train_loss: 0.011666\n",
      "[474/01175] train_loss: 0.011420\n",
      "[474/01225] train_loss: 0.012125\n",
      "[475/00049] train_loss: 0.013065\n",
      "[475/00099] train_loss: 0.013191\n",
      "[475/00149] train_loss: 0.012054\n",
      "[475/00199] train_loss: 0.012326\n",
      "[475/00249] train_loss: 0.012385\n",
      "[475/00299] train_loss: 0.012086\n",
      "[475/00349] train_loss: 0.011565\n",
      "[475/00399] train_loss: 0.011634\n",
      "[475/00449] train_loss: 0.012406\n",
      "[475/00499] train_loss: 0.012445\n",
      "[475/00549] train_loss: 0.012139\n",
      "[475/00599] train_loss: 0.012716\n",
      "[475/00649] train_loss: 0.011341\n",
      "[475/00699] train_loss: 0.012429\n",
      "[475/00749] train_loss: 0.011729\n",
      "[475/00799] train_loss: 0.011981\n",
      "[475/00849] train_loss: 0.012014\n",
      "[475/00899] train_loss: 0.011752\n",
      "[475/00949] train_loss: 0.012161\n",
      "[475/00999] train_loss: 0.011897\n",
      "[475/01049] train_loss: 0.012389\n",
      "[475/01099] train_loss: 0.012033\n",
      "[475/01149] train_loss: 0.012436\n",
      "[475/01199] train_loss: 0.012244\n",
      "[476/00023] train_loss: 0.012115\n",
      "[476/00073] train_loss: 0.011978\n",
      "[476/00123] train_loss: 0.013125\n",
      "[476/00173] train_loss: 0.012833\n",
      "[476/00223] train_loss: 0.011686\n",
      "[476/00273] train_loss: 0.012629\n",
      "[476/00323] train_loss: 0.012558\n",
      "[476/00373] train_loss: 0.012376\n",
      "[476/00423] train_loss: 0.012258\n",
      "[476/00473] train_loss: 0.012450\n",
      "[476/00523] train_loss: 0.012498\n",
      "[476/00573] train_loss: 0.011986\n",
      "[476/00623] train_loss: 0.012172\n",
      "[476/00673] train_loss: 0.012013\n",
      "[476/00723] train_loss: 0.012213\n",
      "[476/00773] train_loss: 0.012408\n",
      "[476/00823] train_loss: 0.011490\n",
      "[476/00873] train_loss: 0.012443\n",
      "[476/00923] train_loss: 0.011984\n",
      "[476/00973] train_loss: 0.011830\n",
      "[476/01023] train_loss: 0.011987\n",
      "[476/01073] train_loss: 0.012063\n",
      "[476/01123] train_loss: 0.011676\n",
      "[476/01173] train_loss: 0.012202\n",
      "[476/01223] train_loss: 0.011979\n",
      "[477/00047] train_loss: 0.013202\n",
      "[477/00097] train_loss: 0.012337\n",
      "[477/00147] train_loss: 0.012647\n",
      "[477/00197] train_loss: 0.012021\n",
      "[477/00247] train_loss: 0.012390\n",
      "[477/00297] train_loss: 0.013251\n",
      "[477/00347] train_loss: 0.011733\n",
      "[477/00397] train_loss: 0.012091\n",
      "[477/00447] train_loss: 0.012359\n",
      "[477/00497] train_loss: 0.012192\n",
      "[477/00547] train_loss: 0.011621\n",
      "[477/00597] train_loss: 0.012144\n",
      "[477/00647] train_loss: 0.011479\n",
      "[477/00697] train_loss: 0.012699\n",
      "[477/00747] train_loss: 0.011671\n",
      "[477/00797] train_loss: 0.012245\n",
      "[477/00847] train_loss: 0.011697\n",
      "[477/00897] train_loss: 0.012042\n",
      "[477/00947] train_loss: 0.011820\n",
      "[477/00997] train_loss: 0.012192\n",
      "[477/01047] train_loss: 0.011431\n",
      "[477/01097] train_loss: 0.011724\n",
      "[477/01147] train_loss: 0.012141\n",
      "[477/01197] train_loss: 0.012414\n",
      "[478/00021] train_loss: 0.012611\n",
      "[478/00071] train_loss: 0.012571\n",
      "[478/00121] train_loss: 0.012414\n",
      "[478/00171] train_loss: 0.012428\n",
      "[478/00221] train_loss: 0.012352\n",
      "[478/00271] train_loss: 0.012479\n",
      "[478/00321] train_loss: 0.012103\n",
      "[478/00371] train_loss: 0.011625\n",
      "[478/00421] train_loss: 0.012636\n",
      "[478/00471] train_loss: 0.012804\n",
      "[478/00521] train_loss: 0.012078\n",
      "[478/00571] train_loss: 0.012310\n",
      "[478/00621] train_loss: 0.012561\n",
      "[478/00671] train_loss: 0.011205\n",
      "[478/00721] train_loss: 0.012563\n",
      "[478/00771] train_loss: 0.012084\n",
      "[478/00821] train_loss: 0.011966\n",
      "[478/00871] train_loss: 0.011700\n",
      "[478/00921] train_loss: 0.012135\n",
      "[478/00971] train_loss: 0.012362\n",
      "[478/01021] train_loss: 0.012566\n",
      "[478/01071] train_loss: 0.011786\n",
      "[478/01121] train_loss: 0.012005\n",
      "[478/01171] train_loss: 0.011809\n",
      "[478/01221] train_loss: 0.011637\n",
      "[479/00045] train_loss: 0.011794\n",
      "[479/00095] train_loss: 0.012707\n",
      "[479/00145] train_loss: 0.012521\n",
      "[479/00195] train_loss: 0.012617\n",
      "[479/00245] train_loss: 0.012336\n",
      "[479/00295] train_loss: 0.011359\n",
      "[479/00345] train_loss: 0.012431\n",
      "[479/00395] train_loss: 0.011508\n",
      "[479/00445] train_loss: 0.012293\n",
      "[479/00495] train_loss: 0.012891\n",
      "[479/00545] train_loss: 0.012707\n",
      "[479/00595] train_loss: 0.011464\n",
      "[479/00645] train_loss: 0.012004\n",
      "[479/00695] train_loss: 0.011762\n",
      "[479/00745] train_loss: 0.012320\n",
      "[479/00795] train_loss: 0.012240\n",
      "[479/00845] train_loss: 0.011786\n",
      "[479/00895] train_loss: 0.012311\n",
      "[479/00945] train_loss: 0.012273\n",
      "[479/00995] train_loss: 0.011918\n",
      "[479/01045] train_loss: 0.011733\n",
      "[479/01095] train_loss: 0.011745\n",
      "[479/01145] train_loss: 0.012159\n",
      "[479/01195] train_loss: 0.012402\n",
      "[480/00019] train_loss: 0.011938\n",
      "[480/00069] train_loss: 0.012610\n",
      "[480/00119] train_loss: 0.012646\n",
      "[480/00169] train_loss: 0.011500\n",
      "[480/00219] train_loss: 0.012068\n",
      "[480/00269] train_loss: 0.012187\n",
      "[480/00319] train_loss: 0.012621\n",
      "[480/00369] train_loss: 0.012753\n",
      "[480/00419] train_loss: 0.011857\n",
      "[480/00469] train_loss: 0.012167\n",
      "[480/00519] train_loss: 0.011633\n",
      "[480/00569] train_loss: 0.011350\n",
      "[480/00619] train_loss: 0.012641\n",
      "[480/00669] train_loss: 0.011897\n",
      "[480/00719] train_loss: 0.011938\n",
      "[480/00769] train_loss: 0.012160\n",
      "[480/00819] train_loss: 0.012874\n",
      "[480/00869] train_loss: 0.012057\n",
      "[480/00919] train_loss: 0.012119\n",
      "[480/00969] train_loss: 0.011596\n",
      "[480/01019] train_loss: 0.011458\n",
      "[480/01069] train_loss: 0.012064\n",
      "[480/01119] train_loss: 0.012252\n",
      "[480/01169] train_loss: 0.011939\n",
      "[480/01219] train_loss: 0.011546\n",
      "[481/00043] train_loss: 0.012343\n",
      "[481/00093] train_loss: 0.012588\n",
      "[481/00143] train_loss: 0.011882\n",
      "[481/00193] train_loss: 0.012114\n",
      "[481/00243] train_loss: 0.012778\n",
      "[481/00293] train_loss: 0.012859\n",
      "[481/00343] train_loss: 0.011533\n",
      "[481/00393] train_loss: 0.011733\n",
      "[481/00443] train_loss: 0.011615\n",
      "[481/00493] train_loss: 0.012748\n",
      "[481/00543] train_loss: 0.011938\n",
      "[481/00593] train_loss: 0.012132\n",
      "[481/00643] train_loss: 0.012375\n",
      "[481/00693] train_loss: 0.012573\n",
      "[481/00743] train_loss: 0.011696\n",
      "[481/00793] train_loss: 0.012900\n",
      "[481/00843] train_loss: 0.011783\n",
      "[481/00893] train_loss: 0.011859\n",
      "[481/00943] train_loss: 0.011879\n",
      "[481/00993] train_loss: 0.012750\n",
      "[481/01043] train_loss: 0.012251\n",
      "[481/01093] train_loss: 0.011780\n",
      "[481/01143] train_loss: 0.012328\n",
      "[481/01193] train_loss: 0.011471\n",
      "[482/00017] train_loss: 0.012369\n",
      "[482/00067] train_loss: 0.012716\n",
      "[482/00117] train_loss: 0.012456\n",
      "[482/00167] train_loss: 0.012608\n",
      "[482/00217] train_loss: 0.012218\n",
      "[482/00267] train_loss: 0.012825\n",
      "[482/00317] train_loss: 0.012605\n",
      "[482/00367] train_loss: 0.012103\n",
      "[482/00417] train_loss: 0.012513\n",
      "[482/00467] train_loss: 0.012147\n",
      "[482/00517] train_loss: 0.012339\n",
      "[482/00567] train_loss: 0.011972\n",
      "[482/00617] train_loss: 0.012096\n",
      "[482/00667] train_loss: 0.012218\n",
      "[482/00717] train_loss: 0.012000\n",
      "[482/00767] train_loss: 0.012073\n",
      "[482/00817] train_loss: 0.011968\n",
      "[482/00867] train_loss: 0.011644\n",
      "[482/00917] train_loss: 0.011671\n",
      "[482/00967] train_loss: 0.011968\n",
      "[482/01017] train_loss: 0.012219\n",
      "[482/01067] train_loss: 0.011938\n",
      "[482/01117] train_loss: 0.011601\n",
      "[482/01167] train_loss: 0.011638\n",
      "[482/01217] train_loss: 0.011874\n",
      "[483/00041] train_loss: 0.011958\n",
      "[483/00091] train_loss: 0.012260\n",
      "[483/00141] train_loss: 0.012456\n",
      "[483/00191] train_loss: 0.012464\n",
      "[483/00241] train_loss: 0.012664\n",
      "[483/00291] train_loss: 0.012413\n",
      "[483/00341] train_loss: 0.012838\n",
      "[483/00391] train_loss: 0.012172\n",
      "[483/00441] train_loss: 0.011748\n",
      "[483/00491] train_loss: 0.012058\n",
      "[483/00541] train_loss: 0.012278\n",
      "[483/00591] train_loss: 0.012144\n",
      "[483/00641] train_loss: 0.012922\n",
      "[483/00691] train_loss: 0.011545\n",
      "[483/00741] train_loss: 0.012078\n",
      "[483/00791] train_loss: 0.011620\n",
      "[483/00841] train_loss: 0.011727\n",
      "[483/00891] train_loss: 0.012158\n",
      "[483/00941] train_loss: 0.011562\n",
      "[483/00991] train_loss: 0.011483\n",
      "[483/01041] train_loss: 0.012395\n",
      "[483/01091] train_loss: 0.012525\n",
      "[483/01141] train_loss: 0.011067\n",
      "[483/01191] train_loss: 0.012686\n",
      "[484/00015] train_loss: 0.012086\n",
      "[484/00065] train_loss: 0.012829\n",
      "[484/00115] train_loss: 0.012568\n",
      "[484/00165] train_loss: 0.011918\n",
      "[484/00215] train_loss: 0.012542\n",
      "[484/00265] train_loss: 0.012636\n",
      "[484/00315] train_loss: 0.012400\n",
      "[484/00365] train_loss: 0.011466\n",
      "[484/00415] train_loss: 0.012151\n",
      "[484/00465] train_loss: 0.012450\n",
      "[484/00515] train_loss: 0.012789\n",
      "[484/00565] train_loss: 0.012503\n",
      "[484/00615] train_loss: 0.012276\n",
      "[484/00665] train_loss: 0.012527\n",
      "[484/00715] train_loss: 0.011185\n",
      "[484/00765] train_loss: 0.011978\n",
      "[484/00815] train_loss: 0.011987\n",
      "[484/00865] train_loss: 0.011667\n",
      "[484/00915] train_loss: 0.011963\n",
      "[484/00965] train_loss: 0.011538\n",
      "[484/01015] train_loss: 0.011770\n",
      "[484/01065] train_loss: 0.012087\n",
      "[484/01115] train_loss: 0.012208\n",
      "[484/01165] train_loss: 0.011872\n",
      "[484/01215] train_loss: 0.012262\n",
      "[485/00039] train_loss: 0.012950\n",
      "[485/00089] train_loss: 0.012539\n",
      "[485/00139] train_loss: 0.012574\n",
      "[485/00189] train_loss: 0.012016\n",
      "[485/00239] train_loss: 0.012360\n",
      "[485/00289] train_loss: 0.012180\n",
      "[485/00339] train_loss: 0.011921\n",
      "[485/00389] train_loss: 0.012715\n",
      "[485/00439] train_loss: 0.012277\n",
      "[485/00489] train_loss: 0.012774\n",
      "[485/00539] train_loss: 0.012800\n",
      "[485/00589] train_loss: 0.012048\n",
      "[485/00639] train_loss: 0.012173\n",
      "[485/00689] train_loss: 0.012194\n",
      "[485/00739] train_loss: 0.013166\n",
      "[485/00789] train_loss: 0.011732\n",
      "[485/00839] train_loss: 0.012195\n",
      "[485/00889] train_loss: 0.011513\n",
      "[485/00939] train_loss: 0.012306\n",
      "[485/00989] train_loss: 0.011785\n",
      "[485/01039] train_loss: 0.011919\n",
      "[485/01089] train_loss: 0.012043\n",
      "[485/01139] train_loss: 0.011348\n",
      "[485/01189] train_loss: 0.011915\n",
      "[486/00013] train_loss: 0.012074\n",
      "[486/00063] train_loss: 0.013283\n",
      "[486/00113] train_loss: 0.013656\n",
      "[486/00163] train_loss: 0.012075\n",
      "[486/00213] train_loss: 0.012293\n",
      "[486/00263] train_loss: 0.011570\n",
      "[486/00313] train_loss: 0.011903\n",
      "[486/00363] train_loss: 0.012080\n",
      "[486/00413] train_loss: 0.011362\n",
      "[486/00463] train_loss: 0.012116\n",
      "[486/00513] train_loss: 0.012394\n",
      "[486/00563] train_loss: 0.012068\n",
      "[486/00613] train_loss: 0.012062\n",
      "[486/00663] train_loss: 0.011973\n",
      "[486/00713] train_loss: 0.013181\n",
      "[486/00763] train_loss: 0.012311\n",
      "[486/00813] train_loss: 0.012043\n",
      "[486/00863] train_loss: 0.011810\n",
      "[486/00913] train_loss: 0.011694\n",
      "[486/00963] train_loss: 0.012042\n",
      "[486/01013] train_loss: 0.012038\n",
      "[486/01063] train_loss: 0.013091\n",
      "[486/01113] train_loss: 0.012580\n",
      "[486/01163] train_loss: 0.011345\n",
      "[486/01213] train_loss: 0.011138\n",
      "[487/00037] train_loss: 0.012010\n",
      "[487/00087] train_loss: 0.012885\n",
      "[487/00137] train_loss: 0.011992\n",
      "[487/00187] train_loss: 0.012367\n",
      "[487/00237] train_loss: 0.012328\n",
      "[487/00287] train_loss: 0.012567\n",
      "[487/00337] train_loss: 0.011774\n",
      "[487/00387] train_loss: 0.012133\n",
      "[487/00437] train_loss: 0.011710\n",
      "[487/00487] train_loss: 0.011936\n",
      "[487/00537] train_loss: 0.011920\n",
      "[487/00587] train_loss: 0.011850\n",
      "[487/00637] train_loss: 0.012755\n",
      "[487/00687] train_loss: 0.012183\n",
      "[487/00737] train_loss: 0.011821\n",
      "[487/00787] train_loss: 0.012488\n",
      "[487/00837] train_loss: 0.011529\n",
      "[487/00887] train_loss: 0.012291\n",
      "[487/00937] train_loss: 0.012407\n",
      "[487/00987] train_loss: 0.012832\n",
      "[487/01037] train_loss: 0.011399\n",
      "[487/01087] train_loss: 0.011742\n",
      "[487/01137] train_loss: 0.012386\n",
      "[487/01187] train_loss: 0.011483\n",
      "[488/00011] train_loss: 0.012113\n",
      "[488/00061] train_loss: 0.012408\n",
      "[488/00111] train_loss: 0.012550\n",
      "[488/00161] train_loss: 0.012682\n",
      "[488/00211] train_loss: 0.011652\n",
      "[488/00261] train_loss: 0.012329\n",
      "[488/00311] train_loss: 0.012649\n",
      "[488/00361] train_loss: 0.011561\n",
      "[488/00411] train_loss: 0.012064\n",
      "[488/00461] train_loss: 0.011734\n",
      "[488/00511] train_loss: 0.011759\n",
      "[488/00561] train_loss: 0.011831\n",
      "[488/00611] train_loss: 0.012376\n",
      "[488/00661] train_loss: 0.011992\n",
      "[488/00711] train_loss: 0.012415\n",
      "[488/00761] train_loss: 0.012153\n",
      "[488/00811] train_loss: 0.012043\n",
      "[488/00861] train_loss: 0.012134\n",
      "[488/00911] train_loss: 0.011819\n",
      "[488/00961] train_loss: 0.012391\n",
      "[488/01011] train_loss: 0.012353\n",
      "[488/01061] train_loss: 0.012692\n",
      "[488/01111] train_loss: 0.011136\n",
      "[488/01161] train_loss: 0.012061\n",
      "[488/01211] train_loss: 0.011270\n",
      "[489/00035] train_loss: 0.011928\n",
      "[489/00085] train_loss: 0.012896\n",
      "[489/00135] train_loss: 0.012444\n",
      "[489/00185] train_loss: 0.012126\n",
      "[489/00235] train_loss: 0.012354\n",
      "[489/00285] train_loss: 0.012999\n",
      "[489/00335] train_loss: 0.012751\n",
      "[489/00385] train_loss: 0.011670\n",
      "[489/00435] train_loss: 0.012330\n",
      "[489/00485] train_loss: 0.011769\n",
      "[489/00535] train_loss: 0.012213\n",
      "[489/00585] train_loss: 0.011598\n",
      "[489/00635] train_loss: 0.012697\n",
      "[489/00685] train_loss: 0.011990\n",
      "[489/00735] train_loss: 0.011656\n",
      "[489/00785] train_loss: 0.012630\n",
      "[489/00835] train_loss: 0.011865\n",
      "[489/00885] train_loss: 0.011299\n",
      "[489/00935] train_loss: 0.011850\n",
      "[489/00985] train_loss: 0.012069\n",
      "[489/01035] train_loss: 0.011675\n",
      "[489/01085] train_loss: 0.012105\n",
      "[489/01135] train_loss: 0.012178\n",
      "[489/01185] train_loss: 0.011708\n",
      "[490/00009] train_loss: 0.011905\n",
      "[490/00059] train_loss: 0.012622\n",
      "[490/00109] train_loss: 0.011848\n",
      "[490/00159] train_loss: 0.013772\n",
      "[490/00209] train_loss: 0.012070\n",
      "[490/00259] train_loss: 0.012552\n",
      "[490/00309] train_loss: 0.012678\n",
      "[490/00359] train_loss: 0.011761\n",
      "[490/00409] train_loss: 0.012719\n",
      "[490/00459] train_loss: 0.012359\n",
      "[490/00509] train_loss: 0.012145\n",
      "[490/00559] train_loss: 0.011558\n",
      "[490/00609] train_loss: 0.012180\n",
      "[490/00659] train_loss: 0.012675\n",
      "[490/00709] train_loss: 0.011687\n",
      "[490/00759] train_loss: 0.012121\n",
      "[490/00809] train_loss: 0.012475\n",
      "[490/00859] train_loss: 0.011993\n",
      "[490/00909] train_loss: 0.011131\n",
      "[490/00959] train_loss: 0.011601\n",
      "[490/01009] train_loss: 0.011587\n",
      "[490/01059] train_loss: 0.012637\n",
      "[490/01109] train_loss: 0.011659\n",
      "[490/01159] train_loss: 0.011535\n",
      "[490/01209] train_loss: 0.011362\n",
      "[491/00033] train_loss: 0.011737\n",
      "[491/00083] train_loss: 0.012316\n",
      "[491/00133] train_loss: 0.012367\n",
      "[491/00183] train_loss: 0.012184\n",
      "[491/00233] train_loss: 0.011961\n",
      "[491/00283] train_loss: 0.012258\n",
      "[491/00333] train_loss: 0.012062\n",
      "[491/00383] train_loss: 0.012801\n",
      "[491/00433] train_loss: 0.012163\n",
      "[491/00483] train_loss: 0.012727\n",
      "[491/00533] train_loss: 0.012020\n",
      "[491/00583] train_loss: 0.011163\n",
      "[491/00633] train_loss: 0.012243\n",
      "[491/00683] train_loss: 0.011688\n",
      "[491/00733] train_loss: 0.011443\n",
      "[491/00783] train_loss: 0.012640\n",
      "[491/00833] train_loss: 0.012592\n",
      "[491/00883] train_loss: 0.012264\n",
      "[491/00933] train_loss: 0.012337\n",
      "[491/00983] train_loss: 0.011851\n",
      "[491/01033] train_loss: 0.012006\n",
      "[491/01083] train_loss: 0.011628\n",
      "[491/01133] train_loss: 0.012519\n",
      "[491/01183] train_loss: 0.012528\n",
      "[492/00007] train_loss: 0.012100\n",
      "[492/00057] train_loss: 0.012912\n",
      "[492/00107] train_loss: 0.012701\n",
      "[492/00157] train_loss: 0.013616\n",
      "[492/00207] train_loss: 0.011983\n",
      "[492/00257] train_loss: 0.012740\n",
      "[492/00307] train_loss: 0.012420\n",
      "[492/00357] train_loss: 0.011604\n",
      "[492/00407] train_loss: 0.012171\n",
      "[492/00457] train_loss: 0.012472\n",
      "[492/00507] train_loss: 0.011766\n",
      "[492/00557] train_loss: 0.013402\n",
      "[492/00607] train_loss: 0.011818\n",
      "[492/00657] train_loss: 0.011722\n",
      "[492/00707] train_loss: 0.011476\n",
      "[492/00757] train_loss: 0.011938\n",
      "[492/00807] train_loss: 0.011670\n",
      "[492/00857] train_loss: 0.011868\n",
      "[492/00907] train_loss: 0.011576\n",
      "[492/00957] train_loss: 0.011484\n",
      "[492/01007] train_loss: 0.012533\n",
      "[492/01057] train_loss: 0.011580\n",
      "[492/01107] train_loss: 0.012140\n",
      "[492/01157] train_loss: 0.011613\n",
      "[492/01207] train_loss: 0.011594\n",
      "[493/00031] train_loss: 0.012665\n",
      "[493/00081] train_loss: 0.012947\n",
      "[493/00131] train_loss: 0.012032\n",
      "[493/00181] train_loss: 0.011552\n",
      "[493/00231] train_loss: 0.012573\n",
      "[493/00281] train_loss: 0.012687\n",
      "[493/00331] train_loss: 0.013015\n",
      "[493/00381] train_loss: 0.011931\n",
      "[493/00431] train_loss: 0.012855\n",
      "[493/00481] train_loss: 0.012506\n",
      "[493/00531] train_loss: 0.012058\n",
      "[493/00581] train_loss: 0.011823\n",
      "[493/00631] train_loss: 0.012128\n",
      "[493/00681] train_loss: 0.011730\n",
      "[493/00731] train_loss: 0.011267\n",
      "[493/00781] train_loss: 0.011866\n",
      "[493/00831] train_loss: 0.012071\n",
      "[493/00881] train_loss: 0.011810\n",
      "[493/00931] train_loss: 0.011748\n",
      "[493/00981] train_loss: 0.011678\n",
      "[493/01031] train_loss: 0.011715\n",
      "[493/01081] train_loss: 0.013192\n",
      "[493/01131] train_loss: 0.011660\n",
      "[493/01181] train_loss: 0.011802\n",
      "[494/00005] train_loss: 0.012105\n",
      "[494/00055] train_loss: 0.013019\n",
      "[494/00105] train_loss: 0.012103\n",
      "[494/00155] train_loss: 0.012422\n",
      "[494/00205] train_loss: 0.012716\n",
      "[494/00255] train_loss: 0.012005\n",
      "[494/00305] train_loss: 0.011922\n",
      "[494/00355] train_loss: 0.011724\n",
      "[494/00405] train_loss: 0.011417\n",
      "[494/00455] train_loss: 0.011761\n",
      "[494/00505] train_loss: 0.011950\n",
      "[494/00555] train_loss: 0.012492\n",
      "[494/00605] train_loss: 0.012301\n",
      "[494/00655] train_loss: 0.011978\n",
      "[494/00705] train_loss: 0.011881\n",
      "[494/00755] train_loss: 0.011695\n",
      "[494/00805] train_loss: 0.011900\n",
      "[494/00855] train_loss: 0.011398\n",
      "[494/00905] train_loss: 0.011482\n",
      "[494/00955] train_loss: 0.012165\n",
      "[494/01005] train_loss: 0.012602\n",
      "[494/01055] train_loss: 0.011725\n",
      "[494/01105] train_loss: 0.011978\n",
      "[494/01155] train_loss: 0.012604\n",
      "[494/01205] train_loss: 0.012216\n",
      "[495/00029] train_loss: 0.012464\n",
      "[495/00079] train_loss: 0.012706\n",
      "[495/00129] train_loss: 0.012461\n",
      "[495/00179] train_loss: 0.012181\n",
      "[495/00229] train_loss: 0.012197\n",
      "[495/00279] train_loss: 0.012731\n",
      "[495/00329] train_loss: 0.012048\n",
      "[495/00379] train_loss: 0.011681\n",
      "[495/00429] train_loss: 0.012064\n",
      "[495/00479] train_loss: 0.012210\n",
      "[495/00529] train_loss: 0.012169\n",
      "[495/00579] train_loss: 0.011494\n",
      "[495/00629] train_loss: 0.012168\n",
      "[495/00679] train_loss: 0.011708\n",
      "[495/00729] train_loss: 0.012546\n",
      "[495/00779] train_loss: 0.012922\n",
      "[495/00829] train_loss: 0.011726\n",
      "[495/00879] train_loss: 0.012273\n",
      "[495/00929] train_loss: 0.012346\n",
      "[495/00979] train_loss: 0.011756\n",
      "[495/01029] train_loss: 0.011555\n",
      "[495/01079] train_loss: 0.011991\n",
      "[495/01129] train_loss: 0.011776\n",
      "[495/01179] train_loss: 0.012801\n",
      "[496/00003] train_loss: 0.012126\n",
      "[496/00053] train_loss: 0.012173\n",
      "[496/00103] train_loss: 0.012453\n",
      "[496/00153] train_loss: 0.012533\n",
      "[496/00203] train_loss: 0.012523\n",
      "[496/00253] train_loss: 0.012245\n",
      "[496/00303] train_loss: 0.012655\n",
      "[496/00353] train_loss: 0.012909\n",
      "[496/00403] train_loss: 0.011619\n",
      "[496/00453] train_loss: 0.012626\n",
      "[496/00503] train_loss: 0.011723\n",
      "[496/00553] train_loss: 0.012241\n",
      "[496/00603] train_loss: 0.011610\n",
      "[496/00653] train_loss: 0.012435\n",
      "[496/00703] train_loss: 0.011707\n",
      "[496/00753] train_loss: 0.011972\n",
      "[496/00803] train_loss: 0.012000\n",
      "[496/00853] train_loss: 0.012092\n",
      "[496/00903] train_loss: 0.012171\n",
      "[496/00953] train_loss: 0.011728\n",
      "[496/01003] train_loss: 0.011424\n",
      "[496/01053] train_loss: 0.012330\n",
      "[496/01103] train_loss: 0.011741\n",
      "[496/01153] train_loss: 0.012390\n",
      "[496/01203] train_loss: 0.011106\n",
      "[497/00027] train_loss: 0.012769\n",
      "[497/00077] train_loss: 0.012258\n",
      "[497/00127] train_loss: 0.011807\n",
      "[497/00177] train_loss: 0.012426\n",
      "[497/00227] train_loss: 0.012427\n",
      "[497/00277] train_loss: 0.012528\n",
      "[497/00327] train_loss: 0.012173\n",
      "[497/00377] train_loss: 0.012609\n",
      "[497/00427] train_loss: 0.012441\n",
      "[497/00477] train_loss: 0.012015\n",
      "[497/00527] train_loss: 0.011936\n",
      "[497/00577] train_loss: 0.012259\n",
      "[497/00627] train_loss: 0.012517\n",
      "[497/00677] train_loss: 0.011291\n",
      "[497/00727] train_loss: 0.012430\n",
      "[497/00777] train_loss: 0.012790\n",
      "[497/00827] train_loss: 0.012673\n",
      "[497/00877] train_loss: 0.011303\n",
      "[497/00927] train_loss: 0.011628\n",
      "[497/00977] train_loss: 0.011846\n",
      "[497/01027] train_loss: 0.012127\n",
      "[497/01077] train_loss: 0.012122\n",
      "[497/01127] train_loss: 0.011857\n",
      "[497/01177] train_loss: 0.011666\n",
      "[498/00001] train_loss: 0.012409\n",
      "[498/00051] train_loss: 0.012270\n",
      "[498/00101] train_loss: 0.012296\n",
      "[498/00151] train_loss: 0.012503\n",
      "[498/00201] train_loss: 0.011750\n",
      "[498/00251] train_loss: 0.013063\n",
      "[498/00301] train_loss: 0.011819\n",
      "[498/00351] train_loss: 0.012865\n",
      "[498/00401] train_loss: 0.012008\n",
      "[498/00451] train_loss: 0.011272\n",
      "[498/00501] train_loss: 0.012244\n",
      "[498/00551] train_loss: 0.011377\n",
      "[498/00601] train_loss: 0.012282\n",
      "[498/00651] train_loss: 0.011306\n",
      "[498/00701] train_loss: 0.012345\n",
      "[498/00751] train_loss: 0.012332\n",
      "[498/00801] train_loss: 0.012278\n",
      "[498/00851] train_loss: 0.011799\n",
      "[498/00901] train_loss: 0.011744\n",
      "[498/00951] train_loss: 0.012166\n",
      "[498/01001] train_loss: 0.011889\n",
      "[498/01051] train_loss: 0.011584\n",
      "[498/01101] train_loss: 0.011780\n",
      "[498/01151] train_loss: 0.011795\n",
      "[498/01201] train_loss: 0.012057\n",
      "[499/00025] train_loss: 0.012641\n",
      "[499/00075] train_loss: 0.011677\n",
      "[499/00125] train_loss: 0.012160\n",
      "[499/00175] train_loss: 0.012669\n",
      "[499/00225] train_loss: 0.012505\n",
      "[499/00275] train_loss: 0.012019\n",
      "[499/00325] train_loss: 0.011950\n",
      "[499/00375] train_loss: 0.012816\n",
      "[499/00425] train_loss: 0.012313\n",
      "[499/00475] train_loss: 0.011746\n",
      "[499/00525] train_loss: 0.012391\n",
      "[499/00575] train_loss: 0.012174\n",
      "[499/00625] train_loss: 0.012444\n",
      "[499/00675] train_loss: 0.012580\n",
      "[499/00725] train_loss: 0.011567\n",
      "[499/00775] train_loss: 0.011533\n",
      "[499/00825] train_loss: 0.011583\n",
      "[499/00875] train_loss: 0.012301\n",
      "[499/00925] train_loss: 0.012354\n",
      "[499/00975] train_loss: 0.012456\n",
      "[499/01025] train_loss: 0.011965\n",
      "[499/01075] train_loss: 0.012160\n",
      "[499/01125] train_loss: 0.011545\n",
      "[499/01175] train_loss: 0.012307\n",
      "[499/01225] train_loss: 0.012442\n"
     ]
    }
   ],
   "source": [
    "from exercise_3.training import train_deepsdf\n",
    "\n",
    "generalization_config = {\n",
    "    'experiment_name': '3_2_deepsdf_generalization',\n",
    "    'device': 'cuda:0',  # run this on a gpu for a reasonable training time\n",
    "    'is_overfit': False,\n",
    "    'num_sample_points': 4096, # you can adjust this such that the model fits on your gpu\n",
    "    'latent_code_length': 256,\n",
    "    'batch_size': 1,\n",
    "    'resume_ckpt': None,\n",
    "    'learning_rate_model': 0.0005,\n",
    "    'learning_rate_code': 0.001,\n",
    "    'lambda_code_regularization': 0.0001,\n",
    "    'max_epochs': 500,  # not necessary to run for 2000 epochs if you're short on time, at 500 epochs you should start to see reasonable results\n",
    "    'print_every_n': 50,\n",
    "    'visualize_every_n': 5000,\n",
    "}\n",
    "\n",
    "train_deepsdf.main(generalization_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### (f) Inference using the trained model on observed SDF values\n",
    "\n",
    "Fill in the inference script `exercise_3/inference/infer_deepsdf.py`. Note that it's not simply a forward pass, but an optimization of the latent code such that we have lowest error on observed SDF values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from exercise_3.inference.infer_deepsdf import InferenceHandlerDeepSDF\n",
    "\n",
    "device = torch.device('cuda:0')  # change this to cpu if you're not using a gpu\n",
    "\n",
    "inference_handler = InferenceHandlerDeepSDF(256, \"exercise_3/runs/3_2_deepsdf_generalization\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "First, we try inference on a shape from validation set, for which we have a complete observation of sdf values. This is an easier problem as compared to shape completion,\n",
    "since we have all the information already in the input.\n",
    "\n",
    "Let's visualize the observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observations with negative SDF (inside)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a12b32d80fa340cb90a4c4d174d0f36e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observations with positive SDF (outside)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c91dd3d9c044129a8345af2814110cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# get observed data\n",
    "points, sdf = ShapeImplicit.get_all_sdf_samples(\"b351e06f5826444c19fb4103277a6b93\")\n",
    "\n",
    "inside_points = points[sdf[:, 0] < 0, :].numpy()\n",
    "outside_points = points[sdf[:, 0] > 0, :].numpy()\n",
    "\n",
    "# visualize observed points; you'll observe that the observations are very complete\n",
    "print('Observations with negative SDF (inside)')\n",
    "visualize_pointcloud(inside_points, 0.025, flip_axes=True)\n",
    "print('Observations with positive SDF (outside)')\n",
    "visualize_pointcloud(outside_points, 0.025, flip_axes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Reconstruction on these observations with the trained model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "randint(): argument 'size' (position 2) must be tuple of ints, not int",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Input \u001B[1;32mIn [11]\u001B[0m, in \u001B[0;36m<cell line: 2>\u001B[1;34m()\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# reconstruct\u001B[39;00m\n\u001B[1;32m----> 2\u001B[0m vertices, faces \u001B[38;5;241m=\u001B[39m \u001B[43minference_handler\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreconstruct\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpoints\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msdf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m800\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;66;03m# visualize\u001B[39;00m\n\u001B[0;32m      4\u001B[0m visualize_mesh(vertices, faces, flip_axes\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
      "File \u001B[1;32mD:\\TUM\\ML3D\\E3\\exercise_3\\inference\\infer_deepsdf.py:69\u001B[0m, in \u001B[0;36mInferenceHandlerDeepSDF.reconstruct\u001B[1;34m(self, points, sdf, num_optimization_iters)\u001B[0m\n\u001B[0;32m     66\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[0;32m     68\u001B[0m \u001B[38;5;66;03m# TODO: sample a random batch from the observations, batch size = self.num_samples\u001B[39;00m\n\u001B[1;32m---> 69\u001B[0m batch_indices \u001B[38;5;241m=\u001B[39m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrandint\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mlen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mpoints\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnum_samples\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     71\u001B[0m batch_points \u001B[38;5;241m=\u001B[39m points[batch_indices, :]\n\u001B[0;32m     72\u001B[0m batch_sdf \u001B[38;5;241m=\u001B[39m sdf[batch_indices, :]\n",
      "\u001B[1;31mTypeError\u001B[0m: randint(): argument 'size' (position 2) must be tuple of ints, not int"
     ]
    }
   ],
   "source": [
    "# reconstruct\n",
    "vertices, faces = inference_handler.reconstruct(points, sdf, 800)\n",
    "# visualize\n",
    "visualize_mesh(vertices, faces, flip_axes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Next, we can try the shape completion task, i.e., inference on a shape from validation set, for which we do not have a complete observation of sdf values. The observed points are visualized below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# get observed data\n",
    "points, sdf = ShapeImplicit.get_all_sdf_samples(\"b351e06f5826444c19fb4103277a6b93_incomplete\")\n",
    "\n",
    "inside_points = points[sdf[:, 0] < 0, :].numpy()\n",
    "outside_points = points[sdf[:, 0] > 0, :].numpy()\n",
    "\n",
    "# visualize observed points; you'll observe that the observations are incomplete\n",
    "# making this is a shape completion task\n",
    "print('Observations with negative SDF (inside)')\n",
    "visualize_pointcloud(inside_points, 0.025, flip_axes=True)\n",
    "print('Observations with positive SDF (outside)')\n",
    "visualize_pointcloud(outside_points, 0.025, flip_axes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Shape completion using the trained model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# reconstruct\n",
    "vertices, faces = inference_handler.reconstruct(points, sdf, 800)\n",
    "# visualize\n",
    "visualize_mesh(vertices, faces, flip_axes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### (g) Latent space interpolation\n",
    "\n",
    "The latent space learned by DeepSDF is interpolatable, meaning that decoding latent codes from this space produced meaningful shapes. Given two latent codes, a linearly interpolatable latent space will decode\n",
    "each of the intermediate codes to some valid shape. Let's see if this holds for our trained model.\n",
    "\n",
    "We'll pick two shapes from the train set as visualized below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from exercise_3.data.shape_implicit import ShapeImplicit\n",
    "from exercise_3.util.visualization import visualize_mesh\n",
    "\n",
    "mesh = ShapeImplicit.get_mesh(\"494fe53da65650b8c358765b76c296\")\n",
    "print('GT Shape A')\n",
    "visualize_mesh(mesh.vertices, mesh.faces, flip_axes=True)\n",
    "\n",
    "mesh = ShapeImplicit.get_mesh(\"5ca1ef55ff5f68501921e7a85cf9da35\")\n",
    "print('GT Shape B')\n",
    "visualize_mesh(mesh.vertices, mesh.faces, flip_axes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Implement the missing parts in `exercise_3/inference/infer_deepsdf.py` such that it interpolates two given latent vectors, and run the code fragement below once done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from exercise_3.inference.infer_deepsdf import InferenceHandlerDeepSDF\n",
    "\n",
    "inference_handler = InferenceHandlerDeepSDF(256, \"exercise_3/runs/3_2_deepsdf_generalization\", torch.device('cuda:0'))\n",
    "# interpolate; also exports interpolated meshes to disk\n",
    "inference_handler.interpolate('494fe53da65650b8c358765b76c296', '5ca1ef55ff5f68501921e7a85cf9da35', 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Visualize the interpolation below. If everything works out correctly, you should see a smooth transformation between the shapes, with all intermediate shapes being valid sofas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from exercise_3.util.mesh_collection_to_gif import  meshes_to_gif\n",
    "from exercise_3.util.misc import show_gif\n",
    "\n",
    "# create list of meshes (just exported) to be visualized\n",
    "mesh_paths = sorted([x for x in Path(\"exercise_3/runs/3_2_deepsdf_generalization/interpolation\").iterdir() if int(x.name.split('.')[0].split(\"_\")[1]) == 0], key=lambda x: int(x.name.split('.')[0].split(\"_\")[0]))\n",
    "mesh_paths = mesh_paths + mesh_paths[::-1]\n",
    "\n",
    "# create a visualization of the interpolation process\n",
    "meshes_to_gif(mesh_paths, \"exercise_3/runs/3_2_deepsdf_generalization/latent_interp.gif\", 20)\n",
    "show_gif(\"exercise_3/runs/3_2_deepsdf_generalization/latent_interp.gif\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Submission\n",
    "\n",
    "This is the end of exercise 3 🙂. Please create a zip containing all files we provided, everything you modified, your visualization images/gif (no need to submit generated OBJs), including your checkpoints. Name it with your matriculation number(s) as described in exercise 1. Make sure this notebook can be run without problems. Then, submit via Moodle.\n",
    "\n",
    "**Note**: The maximum submission file size limit for Moodle is 100M. You do not need to submit your overfitting checkpoints; however, the generalization checkpoint will be >200M. The easiest way to still be able to submit that one is to split it with zip like this: `zip -s 100M model_best.ckpt.zip model_best.ckpt` which creates a `.zip` and a `.z01`. You can then submit both files alongside another zip containing all your code and outputs.\n",
    "\n",
    "**Submission Deadline**: 16.06.2022, 23:55"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## References\n",
    "\n",
    "[1] Dai, Angela, Charles Ruizhongtai Qi, and Matthias Nießner. \"Shape completion using 3d-encoder-predictor cnns and shape synthesis.\" Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2017.\n",
    "\n",
    "[2] Park, Jeong Joon, et al. \"Deepsdf: Learning continuous signed distance functions for shape representation.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2019."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}