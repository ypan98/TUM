{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Project 3: Neural Temporal Point Process (10 points)\n",
    "\n",
    "This project will be to implement a simple autoregressive neural TPP. This project is seperated into 4 sub-tasks:\n",
    "\n",
    "1. Implement the utility functions to handle batches of variable length event sequences.\n",
    "2. Implement an RNN-based encoder for the event history $H_i$ to be represented with a fixed-dimensional vector $c_i \\in \\mathbb{R}^d$ (often called “context embedding” or “history embedding”).\n",
    "3. Implement a conditional distribution in pytorch to parameterize the PDF $f^{*}(\\tau)$ of the TPP.\n",
    "4. Compute the Log-Likelihood of the event sequence $\\mathbf{t}$ for training.\n",
    "\n",
    "## Your task\n",
    "Complete the missing code. Make sure that all the functions follow the provided specification, i.e. the output of the function exactly matches the description in the docstring. \n",
    "\n",
    "Do not add or modify any code outside of the following comment blocks\n",
    "```\n",
    "##########################################################\n",
    "# YOUR CODE HERE\n",
    ".....\n",
    "##########################################################\n",
    "```\n",
    "\n",
    "The following things are **NOT** allowed:\n",
    "- Using additional `import` statements\n",
    "- Copying / reusing code from other sources (e.g. code by other students)\n",
    "\n",
    "If you plagiarize even for a single project task, you won't be eligible for the bonus this semester."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Utility functions (2 pt.):\n",
    "\n",
    "Remember that each realization of a TPP can be represented by a strictly increasing sequence of arrival times $(t_1 , \\cdots , t_{N} )$ where $t_i \\in [0, T]$. However, we will instead consider the inter-event times $(\\tau_1 , \\cdots , \\tau_{N+1} )$ computed as $\\tau_i = t_i − t_{i−1}$ (assuming $t_0 = 0$ and $t_{N+1} = T$). \n",
    "\n",
    "\n",
    "To train the Neural TPP we will further have to work with batches of inter-event sequences in parallel. Here, we will have to implement a padding procedure to batch the sequences, as the sequences are of different lengths. \n",
    "\n",
    "1. Implement the function `get_tau` in `tpp.utils` to compute the inter-event times for a tensor of arrival times. You are free to implement it from scratch or use any pytorch function.\n",
    "\n",
    "2. Implement `get_sequence_batch` in `tpp.utils` to batch a list of temporal point process instances represented by their interevent times given by `tpp.utils.get_tau`. This will include zero-padding the sequences. In order to remember which element of the padded sequence is \"actual\" data you will have to return a boolean mask. Again you are free to implement it yourself or use any pytorch function.\n",
    "\n",
    "A visual summary of this subtask is represented in the following figure:\n",
    "![image](data/preprocess_times.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import utility functions\n",
    "from tpp.utils import get_tau, get_sequence_batch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Encoder (3 pt.):\n",
    "\n",
    "\n",
    "We will encode each inter-event time as $(\\tau_i, log(\\tau_i))$ to attain a two-dimensional representation $x_i \\in \\mathbb{R}^2$. Thus, our history $H_i$ can be represented by a sequence of vectors $(x_1,\\cdots,x_{N+1})$. \n",
    "\n",
    "Next we will obtain the history embedding $c_i \\in \\mathbb{R}^d$ with a simple RNN. We initialize the first context vector to all zeros $c_1 = 0$. We define the other context vectors $c_i$ recursively using the RNN update equation\n",
    "$c_{i+1} = tanh(W^{input} x_i + W^{update} c_i + b)$.\n",
    "\n",
    "1. Implement the method `NeuralTPP.encode` to encode the batch of interevent times as $(\\tau_i, log(\\tau_i))$ and attain $x_i$.\n",
    "\n",
    "2. Set-up the single layer RNN self.embedding_rnn with $d$ = hidden_dim in `NeuralTPP.__init__`.\n",
    "\n",
    "3. Apply the encoding and RNN to the inter-event times to attain the history embeddings $(c_1,\\cdots, c_N)$ in `NeuralTPP.embed_history`. Note, that the context starts with $c_1$ and ends with $c_N$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Conditional density (3 pt.)\n",
    "\n",
    "We model our conditional density of the positive inter-event times with a Log-Normal distribution:\n",
    "\n",
    "$f^{*}(\\tau_i) = Lognormal(\\tau_i| \\mu_i, \\sigma_i),$\n",
    "\n",
    "where $\\mu_i = v^T_{mean}c_i+b_{mean}$ and $\\sigma_i = exp(v^T_{std}c_i+b_{std})$. Note, that both the mean and standard deviation are parameterized by an affine transform, that can be batched and implemented by a single MLP to attain $\\mu$ and $log(\\sigma)$ simultaneously.\n",
    "\n",
    "1. Intitialize the single Layer MLP that maps from $c_i$ to $\\mu$ and $log(\\sigma)$ as `self.linear` in `NeuralTPP.__init__`.\n",
    "\n",
    "\n",
    "2. Implement the method `get_distribution_parameters` that applies `self.linear` and returns the batched $\\mu$ and $\\sigma$ for all events.\n",
    "\n",
    "\n",
    "3. Initialize the LogNormal distribution in `get_distributions` for the given batched $\\mu$ and $\\sigma$ of all events. For an introduction to shapes and batching for pytorch distributions please refer to: https://bochang.me/blog/posts/pytorch-distributions/"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Log-Likelihood (2 pts.)\n",
    "\n",
    "The log-likelihood for an event sequence $\\mathbf{t}$ of length $N$ is given by:\n",
    "\n",
    "$log p(\\mathbf{t}) = \\left[ \\sum^N_{i=1} log f^{*}(\\tau_i)\\right] + log S(\\tau_{N+1}| c_{N+1}),$\n",
    "\n",
    "where $S$ is the survival function.\n",
    "\n",
    "1. Implement the first half of the log-likelihood in `NeuralTPP.get_log_densities` for the batched event sequences.\n",
    "\n",
    "2. The second half of the log-likelihood, i.e., the evaluation of the survival function is to be implemented in `NeuralTPP.get_log_survival_prob`. Note that Pytorch distributions don’t implement the logarithm of the survival function, but you can easily compute it as log(1 - cdf(t)) using the cdf method."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tpp.model import NeuralTPP"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data and prepare variable length sequences for batched processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load toy data\n",
    "data = torch.load(\"data/hawkes.pkl\")\n",
    "\n",
    "arrival_times = data[\"arrival_times\"]\n",
    "t_end = data[\"t_end\"]\n",
    "\n",
    "# compute interevent times and batch sequences\n",
    "tau = [get_tau(t, t_end) for t in arrival_times]\n",
    "times, mask = get_sequence_batch(tau)\n",
    "\n",
    "# normalize inter event times [0,1]\n",
    "times = times/t_end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train your model on the data\n",
    "\n",
    "The expected behaviour of a correctly implemented neural TPP would be to overfit the training set, leading to a very negative NLL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralTPP(hidden_dim=32)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "losses = []\n",
    "epochs = 5000\n",
    "\n",
    "with tqdm.tqdm(range(1, epochs), unit=\"epoch\") as tepoch:\n",
    "    for epoch in tepoch:\n",
    "        optimizer.zero_grad()\n",
    "        loss = model.log_likelihood(times, mask)\n",
    "        loss = -loss.mean()\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        tepoch.set_postfix(NLL=loss.item())\n",
    "\n",
    "plt.plot(range(1, epochs), losses)\n",
    "plt.ylabel(\"NLL\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diffusion_tpp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "457a493feb279d2a8f7e805e1fb95d405b20bc23f0c027dbdc5dd17843557a95"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}