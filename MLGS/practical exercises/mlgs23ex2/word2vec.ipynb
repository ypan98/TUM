{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "%matplotlib inline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 2: Word2Vec (10 pts)\n",
    "The goal of this project is to obtain the vector representations for words from text.\n",
    "\n",
    "The main idea is that words appearing in similar contexts have similar meanings. Because of that, word vectors of similar words should be close together. Models that use word vectors can utilize these properties, e.g., in sentiment analysis a model will learn that \"good\" and \"great\" are positive words, but will also generalize to other words that it has not seen (e.g. \"amazing\") because they should be close together in the vector space.\n",
    "\n",
    "Vectors can keep other language properties as well, like analogies. The question \"a is to b as c is to ...?\", where the answer is d, can be answered by looking into word vector space and calculating $\\mathbf{u}_b - \\mathbf{u}_a + \\mathbf{u}_c$, and finding the word vector that is the closest to the result.\n",
    "\n",
    "## Your task\n",
    "Complete the missing code in this notebook. Make sure that all the functions follow the provided specification, i.e. the output of the function exactly matches the description in the docstring. \n",
    "\n",
    "We are given a text that contains $N$ unique words $\\{ x_1, ..., x_N \\}$. We will focus on the Skip-Gram model in which the goal is to predict the context window $S = \\{ x_{i-l}, ..., x_{i-1}, x_{i+1}, ..., x_{i+l} \\}$ from current word $x_i$, where $l$ is the window size. \n",
    "\n",
    "We get a word embedding $\\mathbf{u}_i$ by multiplying the matrix $\\mathbf{U}$ with a one-hot representation $\\mathbf{x}_i$ of a word $x_i$. Then, to get output probabilities for context window, we multiply this embedding with another matrix $\\mathbf{V}$ and apply softmax. The objective is to minimize the loss: $-\\mathop{\\mathbb{E}}[P(S|x_i;\\mathbf{U}, \\mathbf{V})]$.\n",
    "\n",
    "You are given a dataset with positive and negative reviews. Your task is to:\n",
    "+ Construct input-output pairs corresponding to the current word and a word in the context window\n",
    "+ Implement forward and backward propagation with parameter updates for Skip-Gram model\n",
    "+ Train the model\n",
    "+ Test it on word analogies and sentiment analysis task\n",
    "\n",
    "## General remarks\n",
    "\n",
    "Only functionality in the python files will be graded. Carefully read the method docstrings to understand the task, parameters and what output is expected.\n",
    "Fill in the missing code at the markers in the files `data.py`, `model.py`, `train.py`, `analogies.py`\n",
    "```python\n",
    "###########################\n",
    "# YOUR CODE HERE\n",
    "###########################\n",
    "```\n",
    "Do not add or modify any code at other places in the notebook and python code files except where otherwise explicitly stated.\n",
    "After you fill in all the missing code, restart the kernel and re-run all the cells in the notebook.\n",
    "\n",
    "The following things are **NOT** allowed:\n",
    "- Using additional `import` statements\n",
    "- Copying / reusing code from other sources (e.g. code by other students)\n",
    "\n",
    "If you plagiarise even for a single project task, you won't be eligible for the bonus this semester."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load data (1.5 pts)\n",
    "\n",
    "We'll be working with a subset of reviews for restaurants in Las Vegas. The reviews that we'll be working with are either 1-star or 5-star. First, we need to process tokens (words) into integer values. Second, as the embedding model is trained with pairs of tokens, we need to compute pairs of co-occuring tokens.\n",
    "\n",
    "You need to implemenet this functionality in `data.py`:\n",
    "- `compute_token_to_index` **(0.5 pts)**: Map tokens (words) in sequences to numerical values (integers)\n",
    "- `get_token_pairs_from_window` **(1 pts)**: For each token in a sequence, compute all tokens that appear in its context, i.e. tokens that are within a given window size around that word\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of pairs: 207462\n"
     ]
    }
   ],
   "source": [
    "from data import load_data, build_vocabulary, compute_token_to_index, get_token_pairs_from_window\n",
    "\n",
    "reviews_1star, reviews_5star = load_data('task03_data.npy')\n",
    "corpus = reviews_1star + reviews_5star\n",
    "corpus, vocabulary, counts = build_vocabulary(corpus)\n",
    "token_to_idx, idx_to_token, idx_to_count = compute_token_to_index(vocabulary, counts)\n",
    "data = np.array(sum((list(get_token_pairs_from_window(sequence, 3, token_to_idx)) \n",
    "                        for sequence in corpus), start = [])) # N, 2\n",
    "# Should output\n",
    "# Total number of pairs: 207462\n",
    "print('Total number of pairs:', data.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "VOCABULARY_SIZE = len(vocabulary)\n",
    "EMBEDDING_DIM = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of positive reviews: 1000\n",
      "Number of negative reviews: 2000\n",
      "Number of unique words: 201\n"
     ]
    }
   ],
   "source": [
    "print('Number of positive reviews:', len(reviews_1star))\n",
    "print('Number of negative reviews:', len(reviews_5star))\n",
    "print('Number of unique words:', VOCABULARY_SIZE)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We calculate a weighting score to counter the imbalance between the rare and frequent words. Rare words will be sampled more frequently. See https://arxiv.org/pdf/1310.4546.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.8206203e-06 4.8206203e-06 4.8206203e-06]\n"
     ]
    }
   ],
   "source": [
    "# Compute sampling probabilities\n",
    "probabilities = np.array([1 - np.sqrt(1e-3 / idx_to_count[token_idx]) for token_idx in data[:, 0]])\n",
    "probabilities /= np.sum(probabilities)\n",
    "# Should output: \n",
    "# [4.8206203e-06 4.8206203e-06 4.8206203e-06]\n",
    "print(probabilities[:3])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Model Definition (6.5 pts)\n",
    "\n",
    "Now you need to implement the word embedding model. In particular, you need to implement the following functionality in the `Embedding` class in `model.py`:\n",
    "- `one_hot` **(0.5 pts)**: Computes a one-hot encoding for the integer representations of tokens\n",
    "- `softmax`**(1 pts)**: Applies the softmax normalization to model outputs. (Hint: Watch out for numerical stability!)\n",
    "- `loss` **(0.5 pts)**: Computes the cross-entropy loss for a prediction (=probability distribution over the vocabulary) given the ground truth observed context word\n",
    "- `forward` **(2 pts)**: Computes the forward pass of the model. You also need to cache intermediate values as they are needed for backpropagation.\n",
    "- `backward` **(2.5 pts)**: Computes the gradients with respect to both model weights $V$ and $U$. Use the activation values cached in the `forward` method\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Training (1 pts)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train our model using stochastic gradient descent. At every step we sample a mini-batch from data and update the weights.\n",
    "\n",
    "The following function samples words from data and creates mini-batches. It subsamples frequent words based on previously calculated probabilities.\n",
    "\n",
    "You need to implement the optimizer that iteratively updates the model weights after each training step. We use an optimizer with momentum. In particular, you need to implement the following functionality in `train.py`:\n",
    "- `step` **(1 pts)**: Applies an update to the model weights given the gradients of the current step.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import Embedding\n",
    "from train import Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(123)\n",
    "def get_batch(data, size, prob):\n",
    "    x = rng.choice(data, size, p=prob)\n",
    "    return x[:,0], x[:,1]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the model can take some time so plan accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1000, Avg. training loss: 3.7195\n",
      "Iteration: 2000, Avg. training loss: 3.5692\n",
      "Iteration: 3000, Avg. training loss: 3.5514\n",
      "Iteration: 4000, Avg. training loss: 3.5378\n",
      "Iteration: 5000, Avg. training loss: 3.5259\n",
      "Iteration: 6000, Avg. training loss: 3.5178\n",
      "Iteration: 7000, Avg. training loss: 3.5143\n",
      "Iteration: 8000, Avg. training loss: 3.5015\n",
      "Iteration: 9000, Avg. training loss: 3.4980\n",
      "Iteration: 10000, Avg. training loss: 3.4880\n",
      "Iteration: 11000, Avg. training loss: 3.4951\n",
      "Iteration: 12000, Avg. training loss: 3.4872\n",
      "Iteration: 13000, Avg. training loss: 3.4807\n",
      "Iteration: 14000, Avg. training loss: 3.4832\n",
      "Iteration: 15000, Avg. training loss: 3.4823\n"
     ]
    }
   ],
   "source": [
    "model = Embedding(VOCABULARY_SIZE, EMBEDDING_DIM)\n",
    "optim = Optimizer(model, learning_rate=1.0, momentum=0.5)\n",
    "\n",
    "losses = []\n",
    "\n",
    "MAX_ITERATIONS = 15000\n",
    "PRINT_EVERY = 1000\n",
    "BATCH_SIZE = 1000\n",
    "\n",
    "for i in range(MAX_ITERATIONS):\n",
    "    x, y = get_batch(data, BATCH_SIZE, probabilities)\n",
    "    \n",
    "    loss = model.forward(x, y)\n",
    "    grad = model.backward()\n",
    "    optim.step(grad)\n",
    "    \n",
    "    assert not np.isnan(loss)\n",
    "    \n",
    "    losses.append(loss)\n",
    "\n",
    "    if (i + 1) % PRINT_EVERY == 0:\n",
    "        print(f'Iteration: {i + 1}, Avg. training loss: {np.mean(losses[-PRINT_EVERY:]):.4f}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The embedding matrix is given by $\\mathbf{U}^T$, where the $i$th row is the vector for $i$th word in the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "emb_matrix = model.U.T"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Analogies (1 pts)\n",
    "\n",
    "As mentioned before, vectors can keep some language properties like analogies. Given a relation a:b and a query c, we can find d such that c:d follows the same relation. We hope to find d by using vector operations. In this case, finding the real word vector $\\mathbf{u}_d$ closest to $\\mathbf{u}_b - \\mathbf{u}_a + \\mathbf{u}_c$ gives us d. \n",
    "\n",
    "**Note that the quality of the analysis results is not expected to be excellent.**\n",
    "\n",
    "You need to implement the following functionality in `analogies.py`:\n",
    "- `get_analogies` (**1 pts**): Given a triplet of tokens (a, b, d), compute the top k tokens with an embedding closest to $u_a - u_b + u_d$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from analogies import get_analogies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`is` is to `was` as [are, from, sauce, friendly, cheese] is to `were`\n",
      "`lunch` is to `day` as [while, great, dinner, first, so] is to `night`\n",
      "`i` is to `my` as [you, we, if, not, taste] is to `your`\n"
     ]
    }
   ],
   "source": [
    "triplets = [['is', 'was', 'were'], ['lunch', 'day', 'night'], ['i', 'my', 'your']]\n",
    "\n",
    "for triplet in triplets:\n",
    "    a, b, d = triplet\n",
    "    candidates = get_analogies(emb_matrix, triplet, token_to_idx, idx_to_token, num_candidates=5)\n",
    "    print(f'`{a}` is to `{b}` as [{\", \".join(candidates)}] is to `{d}`')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
